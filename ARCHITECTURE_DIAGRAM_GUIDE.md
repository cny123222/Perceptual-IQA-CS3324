# æ¶æ„å›¾ç»˜åˆ¶æŒ‡å— - è¯¦ç»†ç‰ˆ

**ç›®çš„**: ä¸ºè®ºæ–‡ç»˜åˆ¶æ¶æ„å›¾æä¾›è¯¦ç»†çš„æŠ€æœ¯è¯´æ˜  
**æ—¥æœŸ**: 2025-12-23

---

## ğŸ“ é—®é¢˜1: æ¶æ„è¯¦ç»†è¯´æ˜ - ç”¨äºç»˜åˆ¶æ¶æ„å›¾

### ğŸ” æ•´ä½“å¯¹æ¯”ï¼šåŸå§‹ vs æˆ‘ä»¬çš„æ”¹è¿›

```
åŸå§‹HyperIQAæ¶æ„:
Input Image (224Ã—224) 
    â†“
ResNet50 Backbone (CNN)
    â”œâ”€ Layer1 (256-D)  â”€â”€â†’ LDA1 â”€â”€â”
    â”œâ”€ Layer2 (512-D)  â”€â”€â†’ LDA2 â”€â”€â”¤
    â”œâ”€ Layer3 (1024-D) â”€â”€â†’ LDA3 â”€â”€â”¼â”€â”€â†’ Concat â†’ target_in_vec (112-D)
    â””â”€ Layer4 (2048-D) â”€â”€â†’ LDA4 â”€â”€â”˜
         â”‚
         â””â”€â”€â†’ hyper_in_feat (2048-D, 7Ã—7) â†’ HyperNet â†’ åŠ¨æ€æƒé‡
                                                            â†“
                                                    TargetNet (target_in_vec + åŠ¨æ€æƒé‡)
                                                            â†“
                                                      Quality Score

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æˆ‘ä»¬çš„æ”¹è¿›æ¶æ„ (Swin-HyperIQA):
Input Image (224Ã—224)
    â†“
Swin Transformer Backbone (Vision Transformer) [æ”¹è¿›1: Backboneæ›¿æ¢]
    â”œâ”€ Stage 0 (128-D, 56Ã—56) â”€â”€â”
    â”œâ”€ Stage 1 (256-D, 28Ã—28) â”€â”€â”¤
    â”œâ”€ Stage 2 (512-D, 14Ã—14) â”€â”€â”¼â”€â”€â†’ Multi-scale Fusion [æ”¹è¿›2: çœŸæ­£çš„å¤šå°ºåº¦]
    â””â”€ Stage 3 (1024-D, 7Ã—7)  â”€â”€â”˜        â†“
         â”‚                          Adaptive Pooling (all to 7Ã—7)
         â”‚                                 â†“
         â”‚                          Concatenate (1920-D, 7Ã—7) [Base model]
         â”‚                                 â†“
         â”‚                          Channel Attention [æ”¹è¿›3: æ³¨æ„åŠ›èåˆ]
         â”‚                                 â†“
         â”‚                          hyper_in_feat (1920-D, 7Ã—7)
         â”‚                                 â†“
         â”‚                            Conv1x1 Blocks
         â”‚                                 â†“
         â”‚                          Compressed feat (112-D)
         â”‚                                 â†“
         â””â”€â”€â†’ target_in_vec (112-D) â”€â”€â†’ HyperNet â†’ åŠ¨æ€æƒé‡ [æ”¹è¿›4: Dropoutæ­£åˆ™åŒ–]
                                                      â†“
                                              TargetNet (target_in_vec + åŠ¨æ€æƒé‡)
                                                      â†“
                                                Quality Score
```

---

## ğŸ¨ è¯¦ç»†æ¨¡å—è¯´æ˜ - ç”¨äºç”»å›¾

### æ¨¡å—1: Input & Patch Embedding

**åŸå§‹HyperIQA**:
```
Input: [B, 3, 224, 224]
  â†“ Conv 7Ã—7, stride=2
  â†“ BN + ReLU
  â†“ MaxPool 3Ã—3, stride=2
Output: [B, 64, 56, 56]
```

**æˆ‘ä»¬çš„æ”¹è¿›**:
```
Input: [B, 3, 224, 224]
  â†“ Patch Partition (4Ã—4 patches)
  â†“ Linear Embedding
Output: [B, 96, 56, 56] (Tiny/Small) æˆ– [B, 128, 56, 56] (Base)
```

**ç”»å›¾å»ºè®®**: 
- ç”¨**æ©™è‰²**è¡¨ç¤ºåŸå§‹çš„Convå±‚
- ç”¨**è“è‰²**è¡¨ç¤ºæˆ‘ä»¬çš„Patch Embedding
- æ ‡æ³¨"4Ã—4 patch, stride=4"

---

### æ¨¡å—2: ç‰¹å¾æå– Backbone (æ ¸å¿ƒæ”¹è¿›!)

#### 2.1 åŸå§‹ ResNet50

```
ç»“æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: [B, 64, 56, 56]                  â”‚
â”‚                                          â”‚
â”‚  Layer1 (64 â†’ 256):                      â”‚
â”‚  â”œâ”€ Bottleneck Ã— 3                       â”‚
â”‚  â””â”€ Output: [B, 256, 56, 56]             â”‚
â”‚        â†“                                 â”‚
â”‚  Layer2 (256 â†’ 512):                     â”‚
â”‚  â”œâ”€ Bottleneck Ã— 4                       â”‚
â”‚  â””â”€ Output: [B, 512, 28, 28]             â”‚
â”‚        â†“                                 â”‚
â”‚  Layer3 (512 â†’ 1024):                    â”‚
â”‚  â”œâ”€ Bottleneck Ã— 6                       â”‚
â”‚  â””â”€ Output: [B, 1024, 14, 14]            â”‚
â”‚        â†“                                 â”‚
â”‚  Layer4 (1024 â†’ 2048):                   â”‚
â”‚  â”œâ”€ Bottleneck Ã— 3                       â”‚
â”‚  â””â”€ Output: [B, 2048, 7, 7] â—€â”€â”€ åªæœ‰è¿™ä¸ªè¿›HyperNet!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®é—®é¢˜:
âŒ åªç”¨äº†Layer4çš„2048-Dç‰¹å¾
âŒ å…¶ä»–å±‚ç‰¹å¾è¢«LDAå‹ç¼©æˆ112-Dæ ‡é‡ (æŸå¤±ç©ºé—´ä¿¡æ¯)
âŒ CNNçš„å±€éƒ¨æ„Ÿå—é‡é™åˆ¶äº†å…¨å±€ç†è§£
```

**ç”»å›¾å»ºè®®**:
- ç”¨**ç°è‰²æ–¹å—**è¡¨ç¤ºBottleneck blocks
- Layer1-3ç”¨**è™šçº¿ç®­å¤´**æŒ‡å‘LDA (è¡¨ç¤ºä¿¡æ¯å‹ç¼©)
- Layer4ç”¨**å®çº¿ç²—ç®­å¤´**æŒ‡å‘HyperNet (å¼ºè°ƒè¿™æ˜¯ä¸»è¦è·¯å¾„)
- æ ‡æ³¨"ä»…Layer4 (2048-D) â†’ HyperNet"

---

#### 2.2 æˆ‘ä»¬çš„ Swin Transformer (â­ æ ¸å¿ƒæ”¹è¿›)

```
ç»“æ„ (Baseæ¨¡å‹):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: [B, 128, 56, 56]                                     â”‚
â”‚                                                              â”‚
â”‚  Stage 0: Swin Transformer Blocks Ã— 2                        â”‚
â”‚  â”œâ”€ Window-MSA (7Ã—7 windows)                                 â”‚
â”‚  â”œâ”€ Shifted Window-MSA                                       â”‚
â”‚  â”œâ”€ MLP (FFN with GELU)                                      â”‚
â”‚  â””â”€ Output: [B, 128, 56, 56] â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚        â†“ Patch Merging (2Ã—2â†’1)       â”‚                       â”‚
â”‚                                       â”‚                       â”‚
â”‚  Stage 1: Swin Transformer Blocks Ã— 2â”‚                       â”‚
â”‚  â””â”€ Output: [B, 256, 28, 28] â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”                   â”‚
â”‚        â†“ Patch Merging                â”‚   â”‚                   â”‚
â”‚                                       â”‚   â”‚                   â”‚
â”‚  Stage 2: Swin Transformer Blocks Ã— 18â”‚   â”‚                   â”‚
â”‚  â””â”€ Output: [B, 512, 14, 14] â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”               â”‚
â”‚        â†“ Patch Merging                â”‚   â”‚   â”‚               â”‚
â”‚                                       â”‚   â”‚   â”‚               â”‚
â”‚  Stage 3: Swin Transformer Blocks Ã— 2â”‚   â”‚   â”‚               â”‚
â”‚  â””â”€ Output: [B, 1024, 7, 7] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”           â”‚
â”‚                                       â”‚   â”‚   â”‚   â”‚           â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•         â”‚
â”‚  å¤šå°ºåº¦èåˆ (Multi-scale Fusion):     â”‚   â”‚   â”‚   â”‚           â”‚
â”‚                                       â†“   â†“   â†“   â†“           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Adaptive Pooling to 7Ã—7:                        â”‚         â”‚
â”‚  â”‚ â”œâ”€ feat0: [B, 128, 7, 7]                        â”‚         â”‚
â”‚  â”‚ â”œâ”€ feat1: [B, 256, 7, 7]                        â”‚         â”‚
â”‚  â”‚ â”œâ”€ feat2: [B, 512, 7, 7]                        â”‚         â”‚
â”‚  â”‚ â””â”€ feat3: [B, 1024, 7, 7]                       â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                      â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Channel Attention (å¯é€‰):                       â”‚         â”‚
â”‚  â”‚ 1. GAP on feat3 â†’ [B, 1024]                     â”‚         â”‚
â”‚  â”‚ 2. FC: 1024â†’256â†’4 (Softmax)                     â”‚         â”‚
â”‚  â”‚ 3. ç”Ÿæˆ4ä¸ªæ³¨æ„åŠ›æƒé‡: [B, 4]                    â”‚         â”‚
â”‚  â”‚ 4. å¯¹4ä¸ªfeatåŠ æƒåæ‹¼æ¥                          â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                      â†“                                        â”‚
â”‚  Concatenate: [B, 1920, 7, 7] â—€â”€â”€ æ‰€æœ‰stageéƒ½è¿›HyperNet! âœ…  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®ä¼˜åŠ¿:
âœ… ä½¿ç”¨æ‰€æœ‰4ä¸ªstageçš„å®Œæ•´ç©ºé—´ç‰¹å¾ (1920-D)
âœ… Shifted Window Attention å®ç°å…¨å±€å»ºæ¨¡
âœ… å±‚çº§å¼ç‰¹å¾èåˆ (ä½å±‚çº¹ç† + é«˜å±‚è¯­ä¹‰)
âœ… å¯é€‰çš„Channel AttentionåŠ¨æ€åŠ æƒ
```

**ç”»å›¾å»ºè®®**:
- ç”¨**è“ç»¿è‰²æ–¹å—**è¡¨ç¤ºSwin Transformer Blocks
- å†…éƒ¨ç”»å°å›¾æ ‡: `W-MSA â†’ SW-MSA â†’ MLP`
- ç”¨**4æ¡å½©è‰²ç®­å¤´**ä»4ä¸ªstageæŒ‡å‘èåˆæ¨¡å—:
  - æµ…è“ (Stage 0, ä½å±‚)
  - è“è‰² (Stage 1, ä¸­å±‚)
  - æ·±è“ (Stage 2, ä¸­é«˜å±‚)
  - ç´«è‰² (Stage 3, é«˜å±‚)
- **Attentionæ¨¡å—**ç”¨**é‡‘è‰²**é«˜äº®æ¡†
- æ ‡æ³¨"All stages â†’ 1920-D multi-scale features"

---

### æ¨¡å—3: Multi-Scale Fusion (â­ æ”¹è¿›2)

**åŸå§‹HyperIQAçš„LDAæ¨¡å—**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Local Distortion Aware (LDA):             â”‚
â”‚                                           â”‚
â”‚ Layer1 [B,256,56,56]                      â”‚
â”‚   â†“ Conv1Ã—1 (256â†’16)                      â”‚
â”‚   â†“ AvgPool 7Ã—7                           â”‚
â”‚   â†“ Flatten â†’ FC                          â”‚
â”‚   â†’ lda1 [B, 28]                          â”‚
â”‚                                           â”‚
â”‚ Layer2/3/4 ç±»ä¼¼...                        â”‚
â”‚                                           â”‚
â”‚ Concat â†’ target_in_vec [B, 112] âœ—        â”‚
â”‚                                           â”‚
â”‚ é—®é¢˜:                                     â”‚
â”‚ âŒ ç©ºé—´ä¿¡æ¯è¢«æ± åŒ–å’Œå‹ç¼©ä¸¢å¤±               â”‚
â”‚ âŒ å¤šå°ºåº¦ç‰¹å¾ä»…ç”¨äºTargetNet              â”‚
â”‚ âŒ HyperNetåªç”¨å•å°ºåº¦ (Layer4)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æˆ‘ä»¬çš„Multi-Scale Fusion**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Scale Feature Fusion:                          â”‚
â”‚                                                       â”‚
â”‚ Stage 0: [B, 128, 56, 56]                            â”‚
â”‚     â†“ AdaptiveAvgPool2d(7,7)                         â”‚
â”‚     â†’ feat0: [B, 128, 7, 7] â”€â”€â”                      â”‚
â”‚                                â”‚                      â”‚
â”‚ Stage 1: [B, 256, 28, 28]     â”‚                      â”‚
â”‚     â†“ AdaptiveAvgPool2d(7,7)  â”‚                      â”‚
â”‚     â†’ feat1: [B, 256, 7, 7] â”€â”€â”¼â”€â”€â”                   â”‚
â”‚                                â”‚  â”‚                   â”‚
â”‚ Stage 2: [B, 512, 14, 14]     â”‚  â”‚                   â”‚
â”‚     â†“ AdaptiveAvgPool2d(7,7)  â”‚  â”‚                   â”‚
â”‚     â†’ feat2: [B, 512, 7, 7] â”€â”€â”¼â”€â”€â”¼â”€â”€â”                â”‚
â”‚                                â”‚  â”‚  â”‚                â”‚
â”‚ Stage 3: [B, 1024, 7, 7]      â”‚  â”‚  â”‚                â”‚
â”‚     â†’ feat3: [B, 1024, 7, 7] â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”             â”‚
â”‚                                â”‚  â”‚  â”‚  â”‚             â”‚
â”‚                                â†“  â†“  â†“  â†“             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ é€‰é¡¹1: ç®€å•æ‹¼æ¥ (--no_attention_fusion) â”‚         â”‚
â”‚  â”‚ Concat(dim=1) â†’ [B, 1920, 7, 7]         â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                      OR                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ é€‰é¡¹2: æ³¨æ„åŠ›åŠ æƒ (--attention_fusion)  â”‚         â”‚
â”‚  â”‚                                          â”‚         â”‚
â”‚  â”‚ 1. Global feature from feat3:           â”‚         â”‚
â”‚  â”‚    GAP(feat3) â†’ [B, 1024]               â”‚         â”‚
â”‚  â”‚                                          â”‚         â”‚
â”‚  â”‚ 2. Attention network:                   â”‚         â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚         â”‚
â”‚  â”‚    â”‚ FC: 1024 â†’ 256       â”‚             â”‚         â”‚
â”‚  â”‚    â”‚ ReLU                 â”‚             â”‚         â”‚
â”‚  â”‚    â”‚ Dropout(0.5)         â”‚             â”‚         â”‚
â”‚  â”‚    â”‚ FC: 256 â†’ 4          â”‚             â”‚         â”‚
â”‚  â”‚    â”‚ Softmax              â”‚             â”‚         â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚         â”‚
â”‚  â”‚         â†“                                â”‚         â”‚
â”‚  â”‚    weights: [B, 4] (sum to 1)           â”‚         â”‚
â”‚  â”‚                                          â”‚         â”‚
â”‚  â”‚ 3. Weighted fusion:                     â”‚         â”‚
â”‚  â”‚    feat_i' = feat_i Ã— weight_i          â”‚         â”‚
â”‚  â”‚    Concat(feat0', ..., feat3')          â”‚         â”‚
â”‚  â”‚         â†“                                â”‚         â”‚
â”‚  â”‚    [B, 1920, 7, 7] âœ…                    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                       â”‚
â”‚ ä¼˜åŠ¿:                                                 â”‚
â”‚ âœ… ä¿ç•™æ‰€æœ‰stageçš„ç©ºé—´ä¿¡æ¯ (7Ã—7)                      â”‚
â”‚ âœ… å¤šå°ºåº¦ç‰¹å¾ç›´æ¥ç”¨äºHyperNet (1920-D)                â”‚
â”‚ âœ… Channel AttentionåŠ¨æ€è°ƒæ•´å„å°ºåº¦é‡è¦æ€§              â”‚
â”‚ âœ… ç«¯åˆ°ç«¯å¯å­¦ä¹ çš„èåˆç­–ç•¥                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç”»å›¾å»ºè®®**:
- ç”¨**4ä¸ªä¸åŒå¤§å°çš„æ–¹å—**è¡¨ç¤º4ä¸ªstage (56Ã—56, 28Ã—28, 14Ã—14, 7Ã—7)
- ç”¨**è™šçº¿ç®­å¤´**è¡¨ç¤ºAdaptivePoolingåˆ°7Ã—7
- **Attentionæ¨¡å—**ç”¨å•ç‹¬çš„**é‡‘è‰²çŸ©å½¢æ¡†**ï¼Œå†…éƒ¨ç”»:
  - `GAP â†’ FC(1024â†’256) â†’ ReLU â†’ Dropout â†’ FC(256â†’4) â†’ Softmax`
- ç”¨**æ¸å˜è‰²æ¡**è¡¨ç¤ºattention weights (0~1)
- æœ€åç”¨**å®½ç®­å¤´**æŒ‡å‘æ‹¼æ¥åçš„1920-Dç‰¹å¾

---

### æ¨¡å—4: HyperNet (åŠ¨æ€æƒé‡ç”Ÿæˆ)

**åŸå§‹ vs æ”¹è¿›å¯¹æ¯”**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŸå§‹HyperNet:                                              â”‚
â”‚                                                            â”‚
â”‚ Input: hyper_in_feat [B, 2048, 7, 7] (ä»…Layer4)            â”‚
â”‚   â†“                                                        â”‚
â”‚   Conv 2048â†’1024â†’512â†’112 (3ä¸ª1Ã—1 Conv + ReLU)             â”‚
â”‚   â†“                                                        â”‚
â”‚   Compressed: [B, 112, 7, 7]                               â”‚
â”‚   â†“                                                        â”‚
â”‚   ç”ŸæˆTargetNetçš„åŠ¨æ€æƒé‡:                                 â”‚
â”‚   â”œâ”€ fc1_weight: Conv3Ã—3 â†’ [B, 112Ã—16, 7, 7]              â”‚
â”‚   â”œâ”€ fc1_bias:   FC â†’ [B, 16]                             â”‚
â”‚   â”œâ”€ fc2_weight, fc2_bias (16â†’8)                          â”‚
â”‚   â”œâ”€ fc3_weight, fc3_bias (8â†’4)                           â”‚
â”‚   â”œâ”€ fc4_weight, fc4_bias (4â†’2)                           â”‚
â”‚   â””â”€ fc5_weight, fc5_bias (2â†’1) â†’ è´¨é‡åˆ†æ•°                â”‚
â”‚                                                            â”‚
â”‚ é—®é¢˜:                                                      â”‚
â”‚ âŒ è¾“å…¥ç‰¹å¾å•ä¸€ (ä»…2048-D)                                 â”‚
â”‚ âŒ æ— æ­£åˆ™åŒ– (å®¹æ˜“è¿‡æ‹Ÿåˆ)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

VS

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ”¹è¿›çš„HyperNet:                                            â”‚
â”‚                                                            â”‚
â”‚ Input: hyper_in_feat [B, 1920, 7, 7] (å¤šå°ºåº¦èåˆ) âœ…       â”‚
â”‚   â†“                                                        â”‚
â”‚   Conv 1920â†’512â†’256â†’112 (3ä¸ª1Ã—1 Conv + ReLU)              â”‚
â”‚   â†“                                                        â”‚
â”‚   Dropout(0.4) â—€â”€â”€ æ–°å¢æ­£åˆ™åŒ–! âœ…                          â”‚
â”‚   â†“                                                        â”‚
â”‚   Compressed: [B, 112, 7, 7]                               â”‚
â”‚   â†“                                                        â”‚
â”‚   ç”ŸæˆTargetNetçš„åŠ¨æ€æƒé‡ (ç»“æ„ç›¸åŒ):                      â”‚
â”‚   â”œâ”€ fc1_weight/bias                                      â”‚
â”‚   â”œâ”€ fc2_weight/bias                                      â”‚
â”‚   â”œâ”€ fc3_weight/bias                                      â”‚
â”‚   â”œâ”€ fc4_weight/bias                                      â”‚
â”‚   â””â”€ fc5_weight/bias                                      â”‚
â”‚                                                            â”‚
â”‚ æ”¹è¿›:                                                      â”‚
â”‚ âœ… è¾“å…¥ç‰¹å¾æ›´ä¸°å¯Œ (1920-D å¤šå°ºåº¦)                          â”‚
â”‚ âœ… Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ (0.4æ¯”ä¾‹)                             â”‚
â”‚ âœ… æ›´å¼ºçš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç”»å›¾å»ºè®®**:
- ç”¨**ç´«è‰²æ–¹å—**è¡¨ç¤ºHyperNet
- å†…éƒ¨ç”»5å±‚å·ç§¯çš„å †å  (ç”¨æ¸å˜è‰²è¡¨ç¤ºé€šé“æ•°å‡å°‘: 1920â†’512â†’256â†’112)
- **Dropoutå±‚**ç”¨**é»„è‰²è™šçº¿æ¡†**æ ‡æ³¨"Dropout 0.4 (Training only)"
- å³ä¾§ç”»5ä¸ªå°ç®­å¤´æŒ‡å‘åŠ¨æ€æƒé‡: `w1,b1` ~ `w5,b5`
- æ ‡æ³¨"Dynamic Weight Generation"

---

### æ¨¡å—5: TargetNet (è´¨é‡é¢„æµ‹)

**ç»“æ„ (åŸå§‹å’Œæ”¹è¿›åŸºæœ¬ç›¸åŒ)**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TargetNet: ä½¿ç”¨åŠ¨æ€æƒé‡çš„å‰å‘ç½‘ç»œ                       â”‚
â”‚                                                         â”‚
â”‚ Input: target_in_vec [B, 112, 1, 1]                     â”‚
â”‚   â†“                                                     â”‚
â”‚   FC1 (åŠ¨æ€æƒé‡): 112 â†’ 16                              â”‚
â”‚   â”œâ”€ weight: w1 [B, 16, 112, 1, 1]                     â”‚
â”‚   â”œâ”€ bias:   b1 [B, 16]                                â”‚
â”‚   â””â”€ ReLU + Dropout(0.5) â—€â”€â”€ æ”¹è¿›: æ–°å¢Dropout âœ…       â”‚
â”‚   â†“                                                     â”‚
â”‚   FC2 (åŠ¨æ€æƒé‡): 16 â†’ 8                                â”‚
â”‚   â””â”€ ReLU + Dropout(0.5) â—€â”€â”€ æ”¹è¿›: æ–°å¢Dropout âœ…       â”‚
â”‚   â†“                                                     â”‚
â”‚   FC3 (åŠ¨æ€æƒé‡): 8 â†’ 4                                 â”‚
â”‚   â””â”€ ReLU + Dropout(0.5) â—€â”€â”€ æ”¹è¿›: æ–°å¢Dropout âœ…       â”‚
â”‚   â†“                                                     â”‚
â”‚   FC4 (åŠ¨æ€æƒé‡): 4 â†’ 2                                 â”‚
â”‚   â””â”€ ReLU + Dropout(0.5) â—€â”€â”€ æ”¹è¿›: æ–°å¢Dropout âœ…       â”‚
â”‚   â†“                                                     â”‚
â”‚   FC5 (åŠ¨æ€æƒé‡): 2 â†’ 1                                 â”‚
â”‚   â†“                                                     â”‚
â”‚ Output: Quality Score [B, 1]                            â”‚
â”‚                                                         â”‚
â”‚ æ”¹è¿›:                                                   â”‚
â”‚ âœ… æ¯å±‚ååŠ Dropout(0.5)é˜²æ­¢è¿‡æ‹Ÿåˆ                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç”»å›¾å»ºè®®**:
- ç”¨**ç»¿è‰²æ–¹å—**è¡¨ç¤ºTargetNet
- ç”»5å±‚FCçš„é€’å‡ç»“æ„: `112â†’16â†’8â†’4â†’2â†’1`
- æ¯å±‚ä¹‹é—´æ ‡æ³¨"ReLU + Dropout 0.5"
- ç”¨**è™šçº¿**è¿æ¥HyperNetç”Ÿæˆçš„æƒé‡åˆ°å¯¹åº”çš„FCå±‚
- æœ€åè¾“å‡ºç”¨**çº¢è‰²åœ†åœˆ**è¡¨ç¤ºè´¨é‡åˆ†æ•°

---

## ğŸ¨ å®Œæ•´æ¶æ„å›¾å¸ƒå±€å»ºè®®

### å»ºè®®çš„å›¾å½¢å¸ƒå±€ (ä»å·¦åˆ°å³):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚   Input     â”‚ â†’  â”‚  Swin Backbone   â”‚ â†’  â”‚ Multi-Scale  â”‚ â†’  â”‚  HyperNet    â”‚ â†’  â”‚Targetâ”‚ â†’ Score
â”‚   Image     â”‚    â”‚  (4 Stages)      â”‚    â”‚   Fusion     â”‚    â”‚  (Dynamic)   â”‚    â”‚ Net  â”‚
â”‚  224Ã—224    â”‚    â”‚ 128â†’256â†’512â†’1024 â”‚    â”‚ +Attention   â”‚    â”‚   Weights    â”‚    â”‚ FCÃ—5 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                       
                            â”‚ Stage 0-3 features
                            â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  target_in_vec   â”‚ (å¯¹æ¯”: åŸå§‹ç”¨LDAå‹ç¼©)
                    â”‚     (112-D)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### é¢œè‰²æ–¹æ¡ˆå»ºè®®:

| æ¨¡å— | é¢œè‰² | è¯´æ˜ |
|------|------|------|
| **Input** | æµ…ç° | è¾“å…¥å›¾åƒ |
| **Swin Backbone** | è“è‰²ç³»æ¸å˜ | Stage 0æµ…è“â†’Stage 3æ·±è“ |
| **Multi-Scale Fusion** | æ©™è‰² | ç‰¹å¾èåˆæ¨¡å— |
| **Channel Attention** | é‡‘è‰² | æ³¨æ„åŠ›æœºåˆ¶(é«˜äº®) |
| **HyperNet** | ç´«è‰² | åŠ¨æ€æƒé‡ç”Ÿæˆ |
| **TargetNet** | ç»¿è‰² | è´¨é‡é¢„æµ‹ç½‘ç»œ |
| **Output** | çº¢è‰² | æœ€ç»ˆè´¨é‡åˆ†æ•° |
| **Dropout** | é»„è‰²è™šçº¿æ¡† | æ­£åˆ™åŒ– |
| **æ”¹è¿›æ ‡è®°** | âœ¨ æ˜Ÿå· | æ ‡æ³¨æˆ‘ä»¬çš„æ”¹è¿›ç‚¹ |

---

## ğŸ“Š å…³é”®æ•°å­—æ ‡æ³¨ (Baseæ¨¡å‹)

åœ¨å›¾ä¸Šæ ‡æ³¨ä»¥ä¸‹å…³é”®æ•°å­—:

1. **Backboneè¾“å‡º**:
   - Stage 0: `128-D, 56Ã—56`
   - Stage 1: `256-D, 28Ã—28`
   - Stage 2: `512-D, 14Ã—14`
   - Stage 3: `1024-D, 7Ã—7`

2. **å¤šå°ºåº¦èåˆ**:
   - ç»Ÿä¸€å: 4Ã—`7Ã—7` = `196 spatial locations per scale`
   - æ€»é€šé“: `128+256+512+1024 = 1920-D`

3. **æ³¨æ„åŠ›æƒé‡**:
   - `1024â†’256â†’4` (Softmax, sum to 1)

4. **HyperNetå‹ç¼©**:
   - `1920â†’512â†’256â†’112` (with Dropout 0.4)

5. **TargetNetåŠ¨æ€FC**:
   - `112â†’16â†’8â†’4â†’2â†’1` (with Dropout 0.5)

6. **å‚æ•°é‡å¯¹æ¯”**:
   - åŸå§‹: `~25M parameters`
   - æ”¹è¿›: `~88M parameters` (Base)

---

## ğŸ¯ å…³é”®æ”¹è¿›ç‚¹æ ‡æ³¨ (ç”¨æ˜Ÿå·æˆ–é«˜äº®)

åœ¨æ¶æ„å›¾ä¸Šç”¨**âœ¨æ˜Ÿå·**æˆ–**é«˜äº®æ¡†**æ ‡æ³¨ä»¥ä¸‹4ä¸ªæ”¹è¿›:

1. **â­ æ”¹è¿›1**: `ResNet50 â†’ Swin Transformer Base`
   - æ ‡æ³¨: "+2.68% SRCC (87% contribution)"

2. **â­ æ”¹è¿›2**: `å•å°ºåº¦ (2048-D) â†’ å¤šå°ºåº¦èåˆ (1920-D)`
   - æ ‡æ³¨: "+0.15% SRCC (5% contribution)"

3. **â­ æ”¹è¿›3**: `ç®€å•æ‹¼æ¥ â†’ Channel Attention`
   - æ ‡æ³¨: "+0.25% SRCC (8% contribution)"

4. **â­ æ”¹è¿›4**: `æ— æ­£åˆ™åŒ– â†’ Dropout (0.4 HyperNet, 0.5 TargetNet)`
   - æ ‡æ³¨: "é˜²æ­¢è¿‡æ‹Ÿåˆ"

**æ€»æå‡**: `+3.08% SRCC (0.907 â†’ 0.9378)`

---

## ğŸ–¼ï¸ å»ºè®®çš„å­å›¾

é™¤äº†ä¸»æ¶æ„å›¾ï¼Œå»ºè®®ç”»3ä¸ªå­å›¾:

### å­å›¾1: Swin Transformer Blockç»†èŠ‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: [B, C, H, W]               â”‚
â”‚    â†“                               â”‚
â”‚  Layer Norm                        â”‚
â”‚    â†“                               â”‚
â”‚  Window Multi-Head Self-Attention  â”‚
â”‚  (7Ã—7 windows, local attention)    â”‚
â”‚    â†“                               â”‚
â”‚  Residual Connection               â”‚
â”‚    â†“                               â”‚
â”‚  Layer Norm                        â”‚
â”‚    â†“                               â”‚
â”‚  Shifted Window MSA                â”‚
â”‚  (shifted 7Ã—7 windows, global)     â”‚
â”‚    â†“                               â”‚
â”‚  Residual Connection               â”‚
â”‚    â†“                               â”‚
â”‚  Layer Norm                        â”‚
â”‚    â†“                               â”‚
â”‚  MLP (FFN with GELU)               â”‚
â”‚    â†“                               â”‚
â”‚  Residual Connection               â”‚
â”‚    â†“                               â”‚
â”‚  Output: [B, C, H, W]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å­å›¾2: Channel Attentionæœºåˆ¶

```
                      feat3 [B, 1024, 7, 7]
                             â†“
                    Global Average Pooling
                             â†“
                       [B, 1024, 1, 1]
                             â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Attention Network                â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
        â”‚   â”‚ FC: 1024 â†’ 256       â”‚         â”‚
        â”‚   â”‚ ReLU                 â”‚         â”‚
        â”‚   â”‚ Dropout(0.5)         â”‚         â”‚
        â”‚   â”‚ FC: 256 â†’ 4          â”‚         â”‚
        â”‚   â”‚ Softmax (Î£=1)        â”‚         â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                  Attention Weights [B, 4]
                   Î±0   Î±1   Î±2   Î±3
                   â†“    â†“    â†“    â†“
              feat0Â·Î±0  feat1Â·Î±1  feat2Â·Î±2  feat3Â·Î±3
                   â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
                          â†“
              Weighted Concatenation
                          â†“
                  [B, 1920, 7, 7]
```

### å­å›¾3: åŠ¨æ€æƒé‡ç”Ÿæˆæœºåˆ¶

```
HyperNet Feature [B, 112, 7, 7]
        â†“
  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
  â”‚           â”‚
  â†“           â†“
Conv3Ã—3      GAP + FC
  â†“           â†“
Weights     Biases
  â†“           â†“
fc_w        fc_b
[B,O,I,1,1] [B,O]
        â†“
   TargetNet Layer:
   out = conv(input, weight=fc_w, bias=fc_b)
   (åŠ¨æ€å·ç§¯/FC)
```

---

## ğŸ“ ç»˜å›¾å·¥å…·å»ºè®®

1. **ä¸“ä¸šå·¥å…·**:
   - **draw.io** (å…è´¹ï¼Œæ˜“ç”¨): https://app.diagrams.net/
   - **Figma** (åä½œ): https://figma.com
   - **Microsoft Visio** (ä¸“ä¸š)
   - **Adobe Illustrator** (é«˜è´¨é‡)

2. **AIè¾…åŠ©å·¥å…·**:
   - **Midjourney/DALL-E**: ç”¨æ–‡å­—æè¿°ç”Ÿæˆè‰å›¾
   - **Lucidchart**: æ™ºèƒ½æµç¨‹å›¾

3. **LaTeX (TikZ)**:
   ```latex
   \usepackage{tikz}
   \usetikzlibrary{shapes,arrows,positioning}
   ```
   ä¼˜ç‚¹: çŸ¢é‡å›¾ï¼Œå¯åµŒå…¥è®ºæ–‡ï¼Œä¸“ä¸š

4. **Python (matplotlib)**:
   - å¯ç¼–ç¨‹ç”Ÿæˆï¼Œé€‚åˆè‡ªåŠ¨åŒ–

---

## ğŸ“ æ¶æ„å›¾Captionå»ºè®®

**ä¸­æ–‡**:
> å›¾1: Swin-HyperIQAæ¶æ„å›¾ã€‚æˆ‘ä»¬ç”¨Swin Transformeræ›¿æ¢ResNet50ä½œä¸ºbackbone (æ”¹è¿›1)ï¼Œå¼•å…¥çœŸæ­£çš„å¤šå°ºåº¦ç‰¹å¾èåˆ (æ”¹è¿›2)ï¼Œå¹¶ä½¿ç”¨Channel AttentionåŠ¨æ€åŠ æƒ (æ”¹è¿›3)ã€‚HyperNetå’ŒTargetNetä¸­åŠ å…¥Dropoutæ­£åˆ™åŒ– (æ”¹è¿›4)ã€‚ç›¸æ¯”åŸå§‹HyperIQAï¼Œåœ¨KonIQ-10kä¸Šæå‡3.08% SRCCã€‚

**è‹±æ–‡**:
> Figure 1: Architecture of Swin-HyperIQA. We replace ResNet50 with Swin Transformer as the backbone (Improvement 1), introduce true multi-scale feature fusion (Improvement 2), and employ Channel Attention for dynamic weighting (Improvement 3). Dropout regularization is added to HyperNet and TargetNet (Improvement 4). Compared to the original HyperIQA, our method achieves +3.08% SRCC improvement on KonIQ-10k.

---

**æœ€åæ›´æ–°**: 2025-12-23  
**çŠ¶æ€**: âœ… å®Œæ•´æ¶æ„è¯´æ˜ï¼Œå¯ç›´æ¥ç”¨äºç»˜å›¾

