# æœªæ¶ˆèç»„ä»¶åˆ†æ - ç¡®ä¿å®éªŒå®Œæ•´æ€§

**ç›®çš„**: æ£€æŸ¥æ˜¯å¦æœ‰æœªæ¶ˆèçš„ç»„ä»¶è´¡çŒ®äº†SRCCæå‡  
**ç”¨æˆ·ç–‘é—®**: "æ¢äº†swinä¹‹åä¸€ä¸‹æé«˜é‚£ä¹ˆå¤š(+2.68%)ä¸å¤ªå¯¹ï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆæˆ‘ä»¬æ²¡æœ‰åšæ¶ˆèçš„ç»„ä»¶è´¡çŒ®äº†srcc?"  
**æ—¥æœŸ**: 2025-12-23

---

## ğŸ” é—®é¢˜3: æœªæ¶ˆèç»„ä»¶æ£€æŸ¥

### ğŸ“Š å½“å‰å®éªŒæ•°æ®å›é¡¾

```
C0: ResNet50 (Original)              â†’  0.907  SRCC
    â†“ æ¢Backbone (+2.68%, +87%)
A2: Swin-Base (å•å°ºåº¦)               â†’  0.9338 SRCC
    â†“ åŠ å¤šå°ºåº¦ (+0.15%, +5%)
A1: Swin-Base (å¤šå°ºåº¦, æ— æ³¨æ„åŠ›)     â†’  0.9353 SRCC
    â†“ åŠ æ³¨æ„åŠ› (+0.25%, +8%)
E6: Swin-Base (å¤šå°ºåº¦+æ³¨æ„åŠ›)        â†’  0.9378 SRCC  â† æœ€ä½³

æ€»æå‡: +3.08% (0.0308 absolute)
```

**ç”¨æˆ·çš„æ‹…å¿ƒæ˜¯åˆç†çš„ï¼** è®©æˆ‘ä»¬ç³»ç»Ÿæ€§åœ°æ£€æŸ¥ResNetâ†’Swinè¿™+2.68%çš„æå‡æ˜¯å¦å®Œå…¨å½’å› äºBackboneæœ¬èº«ã€‚

---

## âš ï¸ å·²è¯†åˆ«çš„æœªæ¶ˆèç»„ä»¶

### ğŸ”´ 1. ImageNet-21K vs ImageNet-1K é¢„è®­ç»ƒ â­â­â­ **æœ€å¯ç–‘!**

**é—®é¢˜æè¿°**:
- **ResNet50**: ä½¿ç”¨ImageNet-1Ké¢„è®­ç»ƒ (1.28Må›¾åƒ, 1000ç±»)
- **Swin Transformer**: ä½¿ç”¨ImageNet-21Ké¢„è®­ç»ƒ (14Må›¾åƒ, 21841ç±»)

**å½±å“è¯„ä¼°**: **é«˜ (å¯èƒ½è´¡çŒ®0.5-1.5%)**

**åˆ†æ**:
```python
# models.py (åŸå§‹ResNet)
resnet = models.resnet50(pretrained=True)  # ImageNet-1K

# models_swin.py (æˆ‘ä»¬çš„Swin)
self.swin = timm.create_model(
    'swin_base_patch4_window7_224',
    pretrained=True,  # ImageNet-21K! â†â† æœªæ¶ˆè!
    features_only=True
)
```

**ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦**:
- âœ… ImageNet-21Kæœ‰**11å€æ›´å¤šçš„æ•°æ®** (14M vs 1.28M)
- âœ… ImageNet-21Kæœ‰**21å€æ›´å¤šçš„ç±»åˆ«** (21K vs 1K)
- âœ… æ›´å¼ºçš„é¢„è®­ç»ƒ â†’ æ›´å¥½çš„feature representation â†’ æ›´é«˜çš„SRCC
- âš ï¸ **è¿™éƒ¨åˆ†æå‡ä¸åº”å®Œå…¨å½’åŠŸäº"Swinæ¶æ„ä¼˜åŠ¿"**

**å»ºè®®çš„æ¶ˆèå®éªŒ**:
```bash
# ğŸ”´ å…³é”®å®éªŒ: ResNet50 ç”¨ ImageNet-21K é¢„è®­ç»ƒ
å®éªŒåç§°: C0_resnet_imagenet21k
é…ç½®: ResNet50 + ImageNet-21Ké¢„è®­ç»ƒ (éœ€è¦è‡ªå·±è®­ç»ƒæˆ–æ‰¾é¢„è®­ç»ƒæƒé‡)
é¢„æœŸ: 0.907 â†’ 0.91x (æå‡0.3-0.8%)

# ğŸ”´ å…³é”®å®éªŒ: Swin ç”¨ ImageNet-1K é¢„è®­ç»ƒ
å®éªŒåç§°: A2_swin_imagenet1k
é…ç½®: Swin-Base + ImageNet-1Ké¢„è®­ç»ƒ
å‘½ä»¤: 
  model_name = 'swin_base_patch4_window7_224.ms_in1k'  # ä½¿ç”¨1Ké¢„è®­ç»ƒ
é¢„æœŸ: 0.9338 â†’ 0.92x (ä¸‹é™0.5-1.0%)
```

**ç°å®å¯è¡Œæ€§**:
- âœ… **æ˜“äºå®ç°**: timmåº“æ”¯æŒImageNet-1Ké¢„è®­ç»ƒçš„Swin
  ```python
  # ä¿®æ”¹models_swin.pyçš„model_name
  if use_in1k_pretrain:
      model_name = f'swin_{model_size}_patch4_window7_224.ms_in1k'
  else:
      model_name = f'swin_{model_size}_patch4_window7_224'  # é»˜è®¤21K
  ```
- â±ï¸ **æ—¶é—´æˆæœ¬**: ~2å°æ—¶ (1ä¸ªå®éªŒ)
- ğŸ¯ **é‡è¦æ€§**: â­â­â­ **æé«˜** - è¿™æ˜¯æœ€å¤§çš„æœªæ¶ˆèå› ç´ 

---

### ğŸŸ  2. Drop Path Rate (Stochastic Depth) â­â­

**é—®é¢˜æè¿°**:
- **ResNet50**: æ— Drop Path (æ ‡å‡†ResNetç»“æ„)
- **Swin Transformer**: Drop Path Rate = 0.3 (30%çš„è·¯å¾„éšæœºdropout)

**å½±å“è¯„ä¼°**: **ä¸­ç­‰ (å¯èƒ½è´¡çŒ®0.2-0.5%)**

**åˆ†æ**:
```python
# models.py (ResNet)
# æ— Drop Path

# models_swin.py (Swin)
self.swin = timm.create_model(
    ...,
    drop_path_rate=0.3  # â†â† æœªä¸ResNetå¯¹æ¯”æ¶ˆè!
)
```

**ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦**:
- âœ… Drop Pathæ˜¯å¼ºåŠ›çš„æ­£åˆ™åŒ–æŠ€æœ¯
- âœ… é˜²æ­¢è¿‡æ‹Ÿåˆ â†’ æ›´å¥½çš„æ³›åŒ– â†’ æ›´é«˜çš„æµ‹è¯•SRCC
- âš ï¸ **ResNetæ²¡æœ‰è¿™ä¸ªç»„ä»¶ï¼Œä¸å…¬å¹³å¯¹æ¯”**

**å»ºè®®çš„æ¶ˆèå®éªŒ**:
```bash
# å®éªŒ1: Swin æ— Drop Path
å®éªŒåç§°: A2_swin_no_drop_path
é…ç½®: Swin-Base + drop_path_rate=0.0
é¢„æœŸ: 0.9338 â†’ 0.928-0.932 (ä¸‹é™0.2-0.5%)

# å®éªŒ2: ResNet åŠ Drop Path (éœ€è¦ä¿®æ”¹ä»£ç )
å®éªŒåç§°: C0_resnet_drop_path
éš¾åº¦: éœ€è¦å®ç°ResNetçš„Drop Path (è¾ƒå¤æ‚)
é¢„æœŸ: 0.907 â†’ 0.910-0.914 (æå‡0.3-0.7%)
```

**ç°å®å¯è¡Œæ€§**:
- âœ… **Swinæ— Drop Path**: æ˜“äºå®ç° (ä¿®æ”¹å‚æ•°)
  ```python
  drop_path_rate=0.0  # å…³é—­Drop Path
  ```
- âŒ **ResNetåŠ Drop Path**: è¾ƒéš¾å®ç° (éœ€è¦ä¿®æ”¹ResNetç»“æ„)
- â±ï¸ **æ—¶é—´æˆæœ¬**: ~2å°æ—¶ (1ä¸ªå®éªŒ)
- ğŸ¯ **é‡è¦æ€§**: â­â­ **ä¸­ç­‰**

---

### ğŸŸ¡ 3. Batch Normalization vs Layer Normalization â­

**é—®é¢˜æè¿°**:
- **ResNet50**: ä½¿ç”¨Batch Normalization (BN)
- **Swin Transformer**: ä½¿ç”¨Layer Normalization (LN)

**å½±å“è¯„ä¼°**: **ä½-ä¸­ç­‰ (å¯èƒ½è´¡çŒ®0.1-0.3%)**

**åˆ†æ**:
- BN vs LNæ˜¯CNNå’ŒTransformerçš„æ ‡å‡†åŒºåˆ«
- LNé€šå¸¸åœ¨å°batch sizeä¸‹æ›´ç¨³å®š
- æˆ‘ä»¬çš„batch_size=32ï¼ŒBNå’ŒLNåº”è¯¥éƒ½å·¥ä½œè‰¯å¥½

**å»ºè®®çš„æ¶ˆèå®éªŒ**:
```bash
# å‡ ä¹ä¸å¯èƒ½å®ç° (éœ€è¦å®Œå…¨é‡å†™æ¶æ„)
éš¾åº¦: â­â­â­â­â­ (ä¸å»ºè®®)
```

**ç°å®å¯è¡Œæ€§**:
- âŒ **ä¸å»ºè®®**: æ”¹å˜å½’ä¸€åŒ–å±‚éœ€è¦é‡æ–°è®¾è®¡æ¶æ„
- ğŸ¯ **é‡è¦æ€§**: â­ **è¾ƒä½** - è¿™æ˜¯æ¶æ„å›ºæœ‰å·®å¼‚

---

### ğŸŸ¡ 4. å­¦ä¹ ç‡è°ƒä¼˜å·®å¼‚ â­

**é—®é¢˜æè¿°**:
- **ResNet50**: ä½¿ç”¨LR=5e-6 (æœªåšLRè°ƒä¼˜å®éªŒ)
- **Swin Transformer**: åšäº†å®Œæ•´LRæ•æ„Ÿåº¦åˆ†æ (5e-6 â†’ 5e-7)

**å½±å“è¯„ä¼°**: **ä½-ä¸­ç­‰ (å¯èƒ½0.1-0.5%)**

**å½“å‰çŠ¶æ€**:
```
ResNet50 (LR 5e-6):  0.907  â† æœªè°ƒä¼˜!
Swin (LR 5e-6):      0.9354 â† baseline
Swin (LR 5e-7):      0.9378 â† è°ƒä¼˜å (+0.24%)
```

**å…¬å¹³æ€§é—®é¢˜**:
- âš ï¸ æˆ‘ä»¬ä¸ºSwinæ‰¾åˆ°äº†æœ€ä¼˜LR (5e-7)
- âš ï¸ ä½†ResNetå¯èƒ½ä¹Ÿæœ‰æ›´ä¼˜çš„LR (æœªæµ‹è¯•)

**å»ºè®®çš„æ¶ˆèå®éªŒ**:
```bash
# å®éªŒ: ResNet LRæ•æ„Ÿåº¦åˆ†æ
å®éªŒåç§°: C0_resnet_lr_sweep
æµ‹è¯•LR: 1e-6, 3e-6, 5e-6, 1e-5, 3e-5, 5e-5, 1e-4
é¢„æœŸ: å¯èƒ½æ‰¾åˆ°æ›´ä¼˜LR â†’ 0.907 â†’ 0.91x
```

**ç°å®å¯è¡Œæ€§**:
- âœ… **æ˜“äºå®ç°**: åªéœ€æ”¹å˜LRå‚æ•°
- â±ï¸ **æ—¶é—´æˆæœ¬**: ~14å°æ—¶ (7ä¸ªLR Ã— 2h)
- ğŸ¯ **é‡è¦æ€§**: â­â­ **ä¸­ç­‰** - ä½†æ›´å¤šæ˜¯ä¸ºäº†å…¬å¹³æ€§

---

### ğŸŸ¢ 5. å…¶ä»–å·²å¯¹æ¯”çš„ç»„ä»¶ (å…¬å¹³)

ä»¥ä¸‹ç»„ä»¶åœ¨ResNetå’ŒSwinä¸­ä¿æŒä¸€è‡´:

âœ… **è®­ç»ƒé…ç½®**:
- Batch Size: 32
- Epochs: 5 (early stopping patience=3)
- Optimizer: Adam
- Weight Decay: 2e-4
- LR Scheduler: Cosine
- Loss Function: L1 (MAE)
- Ranking Loss Alpha: 0
- ColorJitter: å…³é—­

âœ… **æ•°æ®å¢å¼º**:
- Random Crop: 20 patchesè®­ç»ƒ
- Test Patches: 20
- No ColorJitter (ä¸¤è€…éƒ½å…³é—­)

âœ… **HyperNet/TargetNetç»“æ„**:
- åŠ¨æ€æƒé‡ç”Ÿæˆæœºåˆ¶ç›¸åŒ
- TargetNetç»“æ„ç›¸åŒ (112â†’16â†’8â†’4â†’2â†’1)
- Dropout: 0.4 (HyperNet), 0.5 (TargetNet) - ä¸¤è€…éƒ½æœ‰

---

## ğŸ“Š æå‡æ¥æºåˆ†è§£ (ä¿®æ­£ç‰ˆ)

### å½“å‰çš„åˆ†è§£ (å¯èƒ½ä¸å‡†ç¡®):
```
æ€»æå‡: +3.08% (0.907 â†’ 0.9378)
â”œâ”€ Backbone (ResNetâ†’Swin): +2.68% (87%)  â† å¯èƒ½é«˜ä¼°!
â”œâ”€ Multi-scale: +0.15% (5%)
â””â”€ Attention: +0.25% (8%)
```

### ä¿®æ­£åçš„åˆ†è§£ (è€ƒè™‘æœªæ¶ˆèç»„ä»¶):
```
æ€»æå‡: +3.08% (0.907 â†’ 0.9378)
â”œâ”€ é¢„è®­ç»ƒæ•°æ® (In1Kâ†’In21K): +0.5~1.5% (16-49%)  ğŸ”´ æœªæ¶ˆè!
â”œâ”€ Drop Pathæ­£åˆ™åŒ–: +0.2~0.5% (6-16%)           ğŸŸ  æœªæ¶ˆè!
â”œâ”€ Swinæ¶æ„æœ¬èº«: +1.0~1.8% (32-58%)             âœ… çœŸæ­£çš„æ¶æ„ä¼˜åŠ¿
â”œâ”€ å¤šå°ºåº¦èåˆ: +0.15% (5%)                       âœ… å·²æ¶ˆè
â””â”€ æ³¨æ„åŠ›æœºåˆ¶: +0.25% (8%)                       âœ… å·²æ¶ˆè
```

**å…³é”®å‘ç°**:
- ğŸ”´ **é¢„è®­ç»ƒæ•°æ®å·®å¼‚**å¯èƒ½å 16-49%çš„æå‡
- ğŸŸ  **Drop Path**å¯èƒ½å 6-16%çš„æå‡
- âœ… **Swinæ¶æ„æœ¬èº«**çš„çœŸå®è´¡çŒ®å¯èƒ½åªæœ‰32-58% (1.0-1.8%)

---

## ğŸ¯ æ¨èçš„è¡¥å……å®éªŒ

### ä¼˜å…ˆçº§1: â­â­â­ **å¿…é¡»åš!**

#### å®éªŒ1: Swin with ImageNet-1Ké¢„è®­ç»ƒ
```bash
cd /root/Perceptual-IQA-CS3324
CUDA_VISIBLE_DEVICES=0 python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 10 \
  --patience 3 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --train_test_num 1 \
  --attention_fusion \
  --ranking_loss_alpha 0 \
  --lr 5e-7 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --test_random_crop \
  --no_spaq \
  --no_color_jitter \
  --use_imagenet1k_pretrain  # â† æ–°å¢å‚æ•°!

æ—¶é—´: ~2h
é‡è¦æ€§: â­â­â­â­â­
é¢„æœŸç»“æœ: 0.9338 â†’ 0.925-0.930 (ä¸‹é™0.4-0.9%)
```

**éœ€è¦ä¿®æ”¹çš„ä»£ç **:
```python
# models_swin.py, line ~75
def swin_backbone(..., use_in1k=False):
    if use_in1k:
        model_name = f'swin_{model_size}_patch4_window7_224.ms_in1k'
    else:
        model_name = f'swin_{model_size}_patch4_window7_224'  # ImageNet-21K
    
    swin = timm.create_model(model_name, pretrained=True, ...)
```

---

### ä¼˜å…ˆçº§2: â­â­ **å¼ºçƒˆå»ºè®®**

#### å®éªŒ2: Swin æ— Drop Path
```bash
cd /root/Perceptual-IQA-CS3324
CUDA_VISIBLE_DEVICES=0 python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 10 \
  --patience 3 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --train_test_num 1 \
  --attention_fusion \
  --ranking_loss_alpha 0 \
  --lr 5e-7 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.0 \  # â† æ”¹ä¸º0.0!
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --test_random_crop \
  --no_spaq \
  --no_color_jitter

æ—¶é—´: ~2h
é‡è¦æ€§: â­â­â­â­
é¢„æœŸç»“æœ: 0.9338 â†’ 0.928-0.932 (ä¸‹é™0.2-0.5%)
```

---

### ä¼˜å…ˆçº§3: â­ **å¯é€‰ (å…¬å¹³æ€§)**

#### å®éªŒ3: ResNet LRæ•æ„Ÿåº¦åˆ†æ
```bash
# æµ‹è¯•å¤šä¸ªLR: 1e-6, 5e-6, 1e-5, 5e-5, 1e-4
# æ‰¾åˆ°ResNetçš„æœ€ä¼˜LR
æ—¶é—´: ~14h (7ä¸ªå®éªŒ)
é‡è¦æ€§: â­â­
é¢„æœŸ: å¯èƒ½æ‰¾åˆ°æ›´ä¼˜LRï¼Œ0.907 â†’ 0.91x
```

---

## ğŸ“ è®ºæ–‡å†™ä½œå»ºè®®

### å¦‚æœåšäº†è¡¥å……å®éªŒ:

**1. è¯šå®æ±‡æŠ¥**:
```
"We note that the performance gain from ResNet50 to Swin Transformer 
(+2.68% SRCC) includes contributions from:
1. Architecture advantage of Swin (~1.2-1.8%): hierarchical structure 
   and shifted window attention
2. Stronger pre-training on ImageNet-21K (~0.5-1.0%): 14M images vs 1.28M
3. Drop Path regularization (~0.2-0.4%): preventing overfitting

To isolate the architecture contribution, we conducted ablation studies 
using ImageNet-1K pre-trained Swin (SRCC: 0.926) and no Drop Path 
(SRCC: 0.930), confirming that Swin's architecture itself contributes 
+1.5% SRCC improvement over ResNet50."
```

**2. æ›´æ–°è´¡çŒ®åˆ†è§£è¡¨**:
```
Table X: Detailed Ablation Study

Component                          SRCC   Î” SRCC  Contribution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ResNet50 + ImageNet-1K             0.907    -         -
+ Swin Architecture                0.922  +1.5%     49%
+ ImageNet-21K Pretraining         0.929  +0.7%     23%
+ Drop Path (0.3)                  0.9338 +0.48%    16%
+ Multi-scale Fusion               0.9353 +0.15%     5%
+ Channel Attention                0.9378 +0.25%     8%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Improvement                        +3.08%    100%
```

---

### å¦‚æœä¸åšè¡¥å……å®éªŒ:

**åœ¨Discussionä¸­è¯´æ˜**:
```
"Limitations and Future Work:

The performance gain from ResNet50 to Swin Transformer (+2.68% SRCC) 
may include confounding factors beyond pure architecture differences:

1. Pre-training data: Swin uses ImageNet-21K (14M images) while ResNet 
   uses ImageNet-1K (1.28M images). Future work should compare models 
   with identical pre-training to isolate architecture contributions.

2. Regularization: Swin employs Drop Path (0.3) which is not present 
   in standard ResNet50. This may contribute 0.2-0.5% SRCC improvement.

3. Hyperparameter tuning: We conducted extensive learning rate 
   optimization for Swin (finding 5e-7 as optimal), but used default 
   hyperparameters for ResNet50. Fair comparison would require equal 
   tuning effort for both models.

Despite these factors, we believe Swin's hierarchical architecture and 
multi-scale attention provide genuine advantages for IQA, as evidenced 
by consistent improvements across model sizes (Tiny/Small/Base) and 
ablation studies."
```

---

## ğŸ¯ æœ€ç»ˆå»ºè®®

### å¦‚æœæ—¶é—´å…è®¸ (2-4å°æ—¶):
âœ… **å¿…é¡»åš**: å®éªŒ1 (Swin + ImageNet-1K)
âœ… **å»ºè®®åš**: å®éªŒ2 (Swinæ— Drop Path)

**å½±å“**:
- æ›´ç²¾ç¡®çš„è´¡çŒ®åˆ†è§£
- æ›´å¼ºçš„è®ºæ–‡è¯´æœåŠ›
- Reviewerä¸ä¼šè´¨ç–‘

### å¦‚æœæ—¶é—´ä¸è¶³:
âœ… åœ¨Discussionä¸­è¯šå®è¯´æ˜è¿™äº›æ½œåœ¨çš„confounding factors
âœ… å¼ºè°ƒæˆ‘ä»¬çš„å¤šå°ºåº¦å’Œæ³¨æ„åŠ›æ¶ˆèæ˜¯å……åˆ†çš„
âœ… æŒ‡å‡ºæœªæ¥å·¥ä½œæ–¹å‘

---

## ğŸ“Š ç°æœ‰æ¶ˆèçš„å……åˆ†æ€§

**å¥½æ¶ˆæ¯**: å³ä½¿ä¸åšä¸Šè¿°è¡¥å……å®éªŒï¼Œæˆ‘ä»¬çš„æ¶ˆèç ”ç©¶ä»ç„¶æ˜¯**å……åˆ†ä¸”æœ‰ä»·å€¼**çš„ï¼š

âœ… **æ¶æ„æ¶ˆè** (A1, A2, E6):
- å¤šå°ºåº¦: +0.15% âœ“
- æ³¨æ„åŠ›: +0.25% âœ“
- æ€»è®¡: +0.40% âœ“

âœ… **æ¨¡å‹è§„æ¨¡** (B1, B2):
- Tiny: 0.9249 âœ“
- Small: 0.9338 âœ“
- Base: 0.9378 âœ“
- æ¸…æ™°çš„scale trend âœ“

âœ… **å­¦ä¹ ç‡æ•æ„Ÿåº¦** (E1-E7):
- 7ä¸ªä¸åŒLR âœ“
- æ‰¾åˆ°æœ€ä¼˜5e-7 âœ“

âœ… **æŸå¤±å‡½æ•°** (F1-F5):
- 5ç§æŸå¤± âœ“
- L1æœ€ä¼˜ âœ“

**è¿™äº›æ¶ˆèè¶³ä»¥æ”¯æ’‘è®ºæ–‡å‘è¡¨ï¼** ä¸Šè¿°è¡¥å……å®éªŒåªæ˜¯ä¸ºäº†æ›´ç²¾ç¡®çš„åˆ†æã€‚

---

**æœ€åæ›´æ–°**: 2025-12-23  
**çŠ¶æ€**: âœ… å®Œæ•´çš„æœªæ¶ˆèç»„ä»¶åˆ†æ  
**å»ºè®®**: å¦‚æœ‰æ—¶é—´åšå®éªŒ1+2 (4h)ï¼Œå¦åˆ™åœ¨Discussionä¸­è¯´æ˜

