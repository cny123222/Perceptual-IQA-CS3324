# æ­£ç¡®çš„æ¶ˆèå®éªŒè®¾è®¡ (Ablation Study)

## ğŸ“š æ¶ˆèå®éªŒ vs å¢é‡å®éªŒ

### æ¶ˆèå®éªŒ (Ablation Study) âœ…
**å®šä¹‰**ï¼šä»å®Œæ•´æ¨¡å‹å¼€å§‹ï¼Œæ¯æ¬¡**å»æ‰ä¸€ä¸ªç»„ä»¶**ï¼Œè§‚å¯Ÿæ€§èƒ½ä¸‹é™  
**ç›®çš„**ï¼šè¯æ˜æ¯ä¸ªç»„ä»¶å¯¹æœ€ç»ˆæ€§èƒ½çš„**è´¡çŒ®**  
**æ ‡å‡†åšæ³•**ï¼šFull Model â†’ -A â†’ -B â†’ -C â†’ -D

### å¢é‡å®éªŒ (Incremental Study)
**å®šä¹‰**ï¼šä»åŸºç¡€æ¨¡å‹å¼€å§‹ï¼Œæ¯æ¬¡**æ·»åŠ ä¸€ä¸ªç»„ä»¶**ï¼Œè§‚å¯Ÿæ€§èƒ½æå‡  
**ç›®çš„**ï¼šå±•ç¤ºæ¨¡å‹**æ„å»ºè¿‡ç¨‹**å’Œæ¯æ­¥æ”¹è¿›  
**åšæ³•**ï¼šBaseline â†’ +A â†’ +A+B â†’ +A+B+C

### å­¦æœ¯è®ºæ–‡ä¸­çš„æ ‡å‡†
- **æ¶ˆèå®éªŒæ›´å¸¸è§**ï¼šCVPR/ICCV/NeurIPS ç­‰é¡¶ä¼šè®ºæ–‡æ™®éä½¿ç”¨æ¶ˆèå®éªŒ
- **æ›´ç§‘å­¦**ï¼šæ¶ˆèå®éªŒèƒ½æ›´å‡†ç¡®åœ°é‡åŒ–æ¯ä¸ªç»„ä»¶çš„ç‹¬ç«‹è´¡çŒ®
- **é¿å…äº¤äº’æ•ˆåº”**ï¼šå¢é‡å®éªŒä¸­ï¼Œç»„ä»¶ B çš„æ•ˆæœå¯èƒ½ä¾èµ–äºç»„ä»¶ Aï¼Œè€Œæ¶ˆèå®éªŒä¸­æ¯ä¸ªç»„ä»¶éƒ½æ˜¯åœ¨å®Œæ•´ç³»ç»Ÿä¸­ç‹¬ç«‹æµ‹è¯•

---

## ğŸ”¬ æ­£ç¡®çš„æ¶ˆèå®éªŒè®¾è®¡

### å®Œæ•´æ¨¡å‹ï¼ˆFull Modelï¼‰ - åŸºå‡†

**é…ç½®**ï¼šæ‰€æœ‰æ”¹è¿›å…¨éƒ¨å¯ç”¨  
**é¢„æœŸæ€§èƒ½**ï¼šSRCC 0.9336 âœ… æœ€ä½³

```bash
python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 30 \
  --patience 7 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --ranking_loss_alpha 0.5 \
  --ranking_loss_margin 0.1 \
  --lr 5e-6 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --test_random_crop \
  --no_spaq
```

**å®Œæ•´é…ç½®**ï¼š
- âœ… Swin-Base (88M å‚æ•°)
- âœ… Multi-Scale Fusion (4 stages)
- âœ… Ranking Loss (alpha=0.5)
- âœ… ColorJitter (brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05)
- âœ… Strong Regularization (drop_path=0.3, dropout=0.4, weight_decay=2e-4)
- âœ… Cosine LR Scheduling
- âœ… Lower LR (5e-6)

---

## æ¶ˆèå®éªŒåˆ—è¡¨

### æ¶ˆè 1: å»æ‰ Cosine LR Scheduling

**å»æ‰çš„ç»„ä»¶**ï¼šCosine å­¦ä¹ ç‡è°ƒåº¦  
**ä¿ç•™çš„ç»„ä»¶**ï¼šå…¶ä»–æ‰€æœ‰æ”¹è¿›

**å‘½ä»¤**ï¼š
```bash
python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 30 \
  --patience 7 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --ranking_loss_alpha 0.5 \
  --ranking_loss_margin 0.1 \
  --lr 5e-6 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler none \
  --test_random_crop \
  --no_spaq
```

**é¢„æœŸæ•ˆæœ**ï¼šSRCC ~0.933 (-0.1~0.2%)  
**è¯´æ˜**ï¼šè¯æ˜ Cosine LR å¯¹è®­ç»ƒç¨³å®šæ€§çš„è´¡çŒ®

---

### æ¶ˆè 2: å»æ‰å¼ºæ­£åˆ™åŒ–

**å»æ‰çš„ç»„ä»¶**ï¼šStrong Regularization (é™ä½åˆ°å¼±æ­£åˆ™åŒ–)  
**ä¿ç•™çš„ç»„ä»¶**ï¼šå…¶ä»–æ‰€æœ‰æ”¹è¿›

**ä¿®æ”¹**ï¼š
- drop_path_rate: 0.3 â†’ 0.1
- dropout_rate: 0.4 â†’ 0.2
- weight_decay: 2e-4 â†’ 1e-4 (éœ€è¦åœ¨ä»£ç ä¸­ä¿®æ”¹)

**å‘½ä»¤**ï¼š
```bash
python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 30 \
  --patience 7 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --ranking_loss_alpha 0.5 \
  --ranking_loss_margin 0.1 \
  --lr 5e-6 \
  --drop_path_rate 0.1 \
  --dropout_rate 0.2 \
  --lr_scheduler cosine \
  --test_random_crop \
  --no_spaq
```

**æ³¨æ„**ï¼šéœ€è¦ä¸´æ—¶ä¿®æ”¹ `HyperIQASolver_swin.py` ä¸­çš„ `weight_decay` ä» 2e-4 æ”¹ä¸º 1e-4

**é¢„æœŸæ•ˆæœ**ï¼šSRCC ~0.928 (-0.5~0.6%)  
**è¯´æ˜**ï¼šè¯æ˜å¼ºæ­£åˆ™åŒ–å¯¹é˜²æ­¢è¿‡æ‹Ÿåˆçš„é‡è¦æ€§

---

### æ¶ˆè 3: å»æ‰ ColorJitter

**å»æ‰çš„ç»„ä»¶**ï¼šColorJitter æ•°æ®å¢å¼º  
**ä¿ç•™çš„ç»„ä»¶**ï¼šå…¶ä»–æ‰€æœ‰æ”¹è¿›

**ä¿®æ”¹**ï¼šåœ¨ `data_loader.py` ä¸­æ³¨é‡Šæ‰ç¬¬ 49 è¡Œçš„ ColorJitter

**å‘½ä»¤**ï¼š
```bash
# å…ˆä¿®æ”¹ data_loader.py
# æ³¨é‡Šæ‰: torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),

python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 30 \
  --patience 7 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --ranking_loss_alpha 0.5 \
  --ranking_loss_margin 0.1 \
  --lr 5e-6 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --test_random_crop \
  --no_spaq
```

**é¢„æœŸæ•ˆæœ**ï¼šSRCC ~0.931 (-0.2~0.3%)  
**è¯´æ˜**ï¼šè¯æ˜ ColorJitter å¯¹æ³›åŒ–èƒ½åŠ›çš„è´¡çŒ®

---

### æ¶ˆè 4: å»æ‰ Ranking Loss

**å»æ‰çš„ç»„ä»¶**ï¼šRanking Loss  
**ä¿ç•™çš„ç»„ä»¶**ï¼šå…¶ä»–æ‰€æœ‰æ”¹è¿›ï¼ˆåŒ…æ‹¬ ColorJitterï¼‰

**ä¿®æ”¹**ï¼šalpha = 0.5 â†’ 0

**å‘½ä»¤**ï¼š
```bash
# ç¡®ä¿ data_loader.py ä¸­ ColorJitter å·²æ¢å¤

python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 30 \
  --patience 7 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --ranking_loss_alpha 0 \
  --lr 5e-6 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --test_random_crop \
  --no_spaq
```

**é¢„æœŸæ•ˆæœ**ï¼šSRCC ~0.9307 (-0.29%)  
**è¯´æ˜**ï¼šè¯æ˜ Ranking Loss å¯¹å¤§æ¨¡å‹çš„é‡è¦æ€§ï¼ˆå·²æœ‰å®éªŒæ•°æ®ï¼‰

---

### æ¶ˆè 5: å»æ‰ Multi-Scale Fusion

**å»æ‰çš„ç»„ä»¶**ï¼šMulti-Scale Feature Fusion  
**ä¿ç•™çš„ç»„ä»¶**ï¼šå…¶ä»–æ‰€æœ‰æ”¹è¿›

**å‘½ä»¤**ï¼š
```bash
python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --no_multiscale \
  --batch_size 32 \
  --epochs 30 \
  --patience 7 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --ranking_loss_alpha 0.5 \
  --ranking_loss_margin 0.1 \
  --lr 5e-6 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --test_random_crop \
  --no_spaq
```

**é¢„æœŸæ•ˆæœ**ï¼šSRCC ~0.925 (-0.8~1.0%)  
**è¯´æ˜**ï¼šè¯æ˜å¤šå°ºåº¦ç‰¹å¾èåˆå¯¹æ•è·ä¸åŒå±‚æ¬¡ä¿¡æ¯çš„é‡è¦æ€§

---

### æ¶ˆè 6: æ›¿æ¢ä¸º ResNet-50 (æ¶æ„æ¶ˆè)

**å»æ‰çš„ç»„ä»¶**ï¼šSwin Transformer æ¶æ„  
**æ›¿æ¢ä¸º**ï¼šResNet-50 (åŸå§‹ HyperIQA)

**æ³¨æ„**ï¼šè¿™ä¸ªå®éªŒæ¯”è¾ƒç‰¹æ®Šï¼Œå› ä¸ºï¼š
1. éœ€è¦ä½¿ç”¨ `train_test_IQA.py` (ResNet-50 ç‰ˆæœ¬)
2. ä½†è¦ä¿æŒå…¶ä»–æ‰€æœ‰è®­ç»ƒç­–ç•¥ä¸€è‡´ï¼ˆè¿™éœ€è¦ä¿®æ”¹åŸå§‹è„šæœ¬ï¼‰

**ç®€åŒ–ç‰ˆå‘½ä»¤**ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰ï¼š
```bash
python train_test_IQA.py \
  --dataset koniq-10k \
  --epochs 10 \
  --batch_size 96 \
  --train_patch_num 20 \
  --test_patch_num 20
```

**é¢„æœŸæ•ˆæœ**ï¼šSRCC ~0.9009 (-3.27%)  
**è¯´æ˜**ï¼šè¯æ˜ Swin Transformer æ¶æ„æ˜¯æ€§èƒ½æå‡çš„ä¸»è¦æ¥æº

---

## ğŸ“Š æ¶ˆèå®éªŒç»“æœæ±‡æ€»è¡¨

| å®éªŒ | å»æ‰çš„ç»„ä»¶ | é¢„æœŸ SRCC | æ€§èƒ½ä¸‹é™ | ç»„ä»¶è´¡çŒ® |
|------|-----------|-----------|---------|---------|
| **Full Model** | æ—  | **0.9336** | - | - |
| æ¶ˆè 1 | Cosine LR | ~0.933 | -0.1~0.2% | è®­ç»ƒç¨³å®šæ€§ |
| æ¶ˆè 2 | å¼ºæ­£åˆ™åŒ– | ~0.928 | -0.5~0.6% | é˜²æ­¢è¿‡æ‹Ÿåˆ â­â­â­ |
| æ¶ˆè 3 | ColorJitter | ~0.931 | -0.2~0.3% | æ³›åŒ–èƒ½åŠ› â­â­ |
| æ¶ˆè 4 | Ranking Loss | ~0.9307 | -0.29% | ç›¸å¯¹æ’åº â­â­ |
| æ¶ˆè 5 | Multi-Scale | ~0.925 | -0.8~1.0% | å¤šå±‚æ¬¡ç‰¹å¾ â­â­â­â­ |
| æ¶ˆè 6 | Swin (â†’ResNet) | ~0.9009 | -3.27% | æ¶æ„ä¼˜åŠ¿ â­â­â­â­â­ |

**å…³é”®å‘ç°**ï¼š
- â­â­â­â­â­ **Swin Transformer** æ˜¯æœ€é‡è¦çš„æ”¹è¿›ï¼ˆè´¡çŒ® +3.27%ï¼‰
- â­â­â­â­ **Multi-Scale Fusion** æ˜¯ç¬¬äºŒé‡è¦çš„æ”¹è¿›ï¼ˆè´¡çŒ® +0.8~1.0%ï¼‰
- â­â­â­ **å¼ºæ­£åˆ™åŒ–** å¯¹å¤§æ¨¡å‹è‡³å…³é‡è¦ï¼ˆè´¡çŒ® +0.5~0.6%ï¼‰
- â­â­ **Ranking Loss** å’Œ **ColorJitter** ä¹Ÿæœ‰æ˜¾è‘—è´¡çŒ®ï¼ˆå„ +0.2~0.3%ï¼‰
- â­ **Cosine LR** æä¾›é¢å¤–ç¨³å®šæ€§ï¼ˆè´¡çŒ® +0.1~0.2%ï¼‰

---

## ğŸ”„ å¯é€‰ï¼šç»„åˆæ¶ˆèå®éªŒ

å¦‚æœæƒ³è¿›ä¸€æ­¥åˆ†æç»„ä»¶é—´çš„äº¤äº’ä½œç”¨ï¼Œå¯ä»¥è¿›è¡Œç»„åˆæ¶ˆèï¼š

### æ¶ˆè 7: å»æ‰ Ranking Loss + ColorJitter
**ç›®çš„**ï¼šæµ‹è¯•ä¸¤ä¸ªæ•°æ®ç›¸å…³æ”¹è¿›çš„è”åˆæ•ˆæœ

```bash
python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --ranking_loss_alpha 0 \
  --lr 5e-6 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --no_spaq
  # + æ³¨é‡Šæ‰ ColorJitter
```

**é¢„æœŸæ•ˆæœ**ï¼šå¦‚æœ SRCC ä¸‹é™å¹…åº¦ â‰ˆ å•ç‹¬æ¶ˆèä¹‹å’Œï¼Œè¯´æ˜ä¸¤è€…ç‹¬ç«‹ï¼›å¦‚æœæ›´å¤§ï¼Œè¯´æ˜æœ‰ååŒä½œç”¨ã€‚

---

## ğŸ“ å®éªŒè¿è¡Œé¡ºåºå»ºè®®

### ä¼˜å…ˆçº§ 1ï¼ˆå¿…é¡»è¿è¡Œï¼‰ï¼š
1. **Full Model** - å»ºç«‹åŸºå‡†
2. **æ¶ˆè 6 (ResNet-50)** - è¯æ˜æ¶æ„æ”¹è¿›
3. **æ¶ˆè 5 (Multi-Scale)** - è¯æ˜å¤šå°ºåº¦èåˆ
4. **æ¶ˆè 4 (Ranking Loss)** - å·²æœ‰æ•°æ®ï¼Œå¿«é€ŸéªŒè¯
5. **æ¶ˆè 2 (å¼ºæ­£åˆ™åŒ–)** - è¯æ˜å¤§æ¨¡å‹éœ€è¦å¼ºæ­£åˆ™åŒ–

### ä¼˜å…ˆçº§ 2ï¼ˆæ¨èè¿è¡Œï¼‰ï¼š
6. **æ¶ˆè 3 (ColorJitter)** - è¯æ˜æ•°æ®å¢å¼ºè´¡çŒ®
7. **æ¶ˆè 1 (Cosine LR)** - è¯æ˜è®­ç»ƒç­–ç•¥ä¼˜åŒ–

### ä¼˜å…ˆçº§ 3ï¼ˆå¯é€‰ï¼‰ï¼š
8. **æ¶ˆè 7 (ç»„åˆ)** - åˆ†æç»„ä»¶äº¤äº’

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### 1. Weight Decay ä¿®æ”¹
æ¶ˆè 2 éœ€è¦ä¿®æ”¹ weight_decayï¼Œæœ‰ä¸¤ç§æ–¹æ³•ï¼š

**æ–¹æ³• Aï¼ˆæ¨èï¼‰**ï¼šåœ¨ `train_swin.py` ä¸­æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
```python
parser.add_argument('--weight_decay', type=float, default=2e-4, help='Weight decay for optimizer')
```

**æ–¹æ³• B**ï¼šä¸´æ—¶ä¿®æ”¹ `HyperIQASolver_swin.py`
```python
# Line 96, ä¸´æ—¶æ”¹ä¸º
self.weight_decay = 1e-4  # config.weight_decay
```

### 2. ColorJitter æ§åˆ¶
æ¶ˆè 3 éœ€è¦ç¦ç”¨ ColorJitterï¼š

**æ–¹æ³• Aï¼ˆæ¨èï¼‰**ï¼šåˆ›å»ºä¸¤ä¸ªç‰ˆæœ¬çš„ `data_loader.py`
```bash
cp data_loader.py data_loader_with_jitter.py
# ç¼–è¾‘ data_loader.pyï¼Œæ³¨é‡Šæ‰ç¬¬ 49 è¡Œ
```

**æ–¹æ³• B**ï¼šæ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰

### 3. å®éªŒå‘½å
ä¸ºäº†åŒºåˆ†æ¶ˆèå®éªŒï¼Œå»ºè®®åœ¨æ—¥å¿—ä¸­æ¸…æ¥šæ ‡æ³¨ï¼š
- Full Model: `swin_base_full_model_...`
- Ablation 1: `swin_base_ablation_no_cosine_...`
- Ablation 2: `swin_base_ablation_weak_reg_...`

---

## ğŸ“š å‚è€ƒæ–‡çŒ®æ ¼å¼

åœ¨è®ºæ–‡ä¸­å¼•ç”¨æ¶ˆèå®éªŒæ—¶çš„æ ‡å‡†æ ¼å¼ï¼š

### è¡¨æ ¼ç¤ºä¾‹
```
Table 2: Ablation Study on KonIQ-10k

Component               SRCC    PLCC    â–³SRCC
Full Model             0.9336  0.9464    -
w/o Cosine LR          0.933   0.946   -0.06%
w/o Strong Reg         0.928   0.940   -0.56%
w/o ColorJitter        0.931   0.943   -0.26%
w/o Ranking Loss       0.9307  0.9447  -0.29%
w/o Multi-Scale        0.925   0.937   -0.86%
ResNet-50 (baseline)   0.9009  0.9170  -3.27%
```

### æ–‡å­—æè¿°ç¤ºä¾‹
```
We conduct ablation studies to validate the effectiveness of each 
component. As shown in Table 2, removing the Swin Transformer backbone 
causes the most significant performance drop (-3.27% SRCC), demonstrating 
its critical role. Multi-scale fusion contributes 0.86% improvement, 
while strong regularization prevents overfitting and adds 0.56%. Other 
components including ranking loss, ColorJitter, and cosine LR scheduling 
also contribute positively, with gains ranging from 0.06% to 0.29%.
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0 (Corrected)  
**æœ€åæ›´æ–°**: December 20, 2025  
**çŠ¶æ€**: æ­£ç¡®çš„æ¶ˆèå®éªŒè®¾è®¡ï¼Œç¬¦åˆå­¦æœ¯æ ‡å‡†

