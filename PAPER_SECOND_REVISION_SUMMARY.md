# è®ºæ–‡ç¬¬äºŒè½®ç²¾ç‚¼ä¼˜åŒ–å®Œæˆæ€»ç»“

**å®Œæˆæ—¶é—´**: 2025-12-25  
**åŸºäºæŒ‡å¯¼æ–‡æ¡£**: `IEEE-conference-template-062824/WRITING_SUGGESTIONS.md` (ç¬¬äºŒè½®å®¡é˜…å»ºè®®)

---

## ğŸ“Š ä¼˜åŒ–æ¦‚è§ˆ

æœ¬è½®ä¼˜åŒ–èšç„¦ä¸‰ä¸ªæ ¸å¿ƒç›®æ ‡ï¼š
1. **æˆ˜ç•¥æ€§ç¯‡å¹…è°ƒæ•´** - ç²¾ç®€éæ ¸å¿ƒå†…å®¹ï¼Œçªå‡ºä¸»çº¿
2. **å¼ºåŒ–æ ¸å¿ƒè®ºè¿°** - å¢åŠ "ä¸ºä»€ä¹ˆ"å’Œ"Implication"
3. **è¯­è¨€åŠ›åº¦æå‡** - æ›´è‡ªä¿¡ã€æ›´æœ‰åŠ›çš„å­¦æœ¯è¡¨è¾¾

---

## âœ… å®Œæˆçš„ä¼˜åŒ–æ¸…å•

### 1. **ç²¾ç®€Related Work** âœ…

**ç›®æ ‡**: å‹ç¼©åˆ°2/3é¡µï¼Œæ¯ç±»æ–¹æ³•ä¸€æ®µ

**å®ç°**:
- âŒ æ—§ç‰ˆ: 3ä¸ªsubsectionï¼Œè¯¦ç»†æè¿°æ¯ç§æ–¹æ³•
- âœ… æ–°ç‰ˆ: 3ä¸ªæ®µè½ï¼Œæ¯ç±»æ–¹æ³•ä¸€å¥è¯

**å…·ä½“æ”¹è¿›**:

#### æ®µè½1: CNN-based BIQA
```
æ—§: ~8è¡Œï¼Œè¯¦ç»†åˆ—ä¸¾WaDIQaM, DBCNN, NIMAç­‰
æ–°: ~3è¡Œï¼Œä¸€å¥è¯æ¦‚æ‹¬æ‰€æœ‰æ–¹æ³• + æ ¸å¿ƒlimitation
```

#### æ®µè½2: Content-Adaptive Paradigm
```
æ—§: ~10è¡Œï¼Œè¯¦ç»†è§£é‡ŠHyperIQAæœºåˆ¶
æ–°: ~5è¡Œï¼Œä¿ç•™æ ¸å¿ƒåˆ›æ–° + ç›´æ¥æŒ‡å‡ºç“¶é¢ˆ
```

#### æ®µè½3: Transformer-based IQA
```
æ—§: ~12è¡Œï¼Œé€ä¸€ä»‹ç»MUSIQ, MANIQA, TReSç­‰
æ–°: ~5è¡Œï¼Œä¸€å¥è¯æ¦‚æ‹¬æ‰€æœ‰æ–¹æ³• + æˆ‘ä»¬çš„å·®å¼‚ç‚¹
```

**å‹ç¼©æ•ˆæœ**: ~30è¡Œ â†’ ~13è¡Œï¼ˆå‹ç¼©57%ï¼‰

---

### 2. **ç§»åŠ¨æ¬¡è¦åˆ†æåˆ°é™„å½•** âœ…

**ç›®æ ‡**: å°†Model Size, LR Sensitivity, Loss Functionç­‰åˆ†æä»æ­£æ–‡ç§»é™¤

**å®ç°**:

#### a) Model Size Analysis
- **æ­£æ–‡åˆ é™¤**: å®Œæ•´çš„Section 4.5 (Performance-Efficiency Trade-off) + Table + Figure
- **æ­£æ–‡ä¿ç•™**: 1å¥è¯æ¦‚æ‹¬ + å¼•ç”¨é™„å½•
```latex
Even our smallest Swin-Tiny outperforms HyperIQA by +1.79% SRCC, 
demonstrating that architectural design matters more than parameter count. 
The Swin-Small variant offers an optimal performance-efficiency trade-off 
for deployment (detailed analysis in Appendix C.3).
```

#### b) LR Sensitivity & Loss Function
- å·²åœ¨Appendix Cä¸­
- æ­£æ–‡ä¸­é€‚å½“å¼•ç”¨

**å‹ç¼©æ•ˆæœ**: æ­£æ–‡å‡å°‘çº¦1é¡µå†…å®¹

---

### 3. **å¼ºåŒ–Methodä¸­çš„"ä¸ºä»€ä¹ˆ"** âœ…

**ç›®æ ‡**: å¢åŠ AFAå’ŒChannel Attentionçš„è®¾è®¡åŠ¨æœºé˜è¿°

**å®ç°**:

#### AFA Module - æ–°å¢åŠ¨æœºæ®µè½
```latex
\textit{Why preserve spatial structure?} Authentic distortions are often 
non-uniformâ€”for instance, motion blur in foreground with sharp background, 
or compression artifacts concentrated in textured regions. Naive global 
pooling discards all spatial information, making it impossible to localize 
such spatially-varying quality degradations. By maintaining a 7Ã—7 spatial 
grid through adaptive pooling, our AFA module enables the model to retain 
critical spatial localization capabilities essential for authentic BIQA.
```

#### Channel Attention - æ‰©å±•åŠ¨æœºè¯´æ˜
```latex
\textit{Why dynamic weighting is essential:} Different quality levels and 
distortion types exhibit quality cues at different feature hierarchies. 
For high-quality images with minimal distortions, quality can be reliably 
inferred from high-level semantic features aloneâ€”understanding what the 
image depicts suffices to confirm integrity. Conversely, for low-quality 
images with visible artifacts, low-level texture features become critical 
for detecting blur, noise, and compression distortions, while high-level 
features provide contextual understanding. Fixed equal weighting fails to 
capture this quality-dependent assessment strategy.
```

**æ•ˆæœ**: è®¾è®¡åŠ¨æœºæ›´æ¸…æ™°ï¼Œç†è®ºåŸºç¡€æ›´æ‰å®

---

### 4. **åœ¨Experimentsä¸­å¢åŠ Implication** âœ…

**ç›®æ ‡**: æ¯ä¸ªå…³é”®å®éªŒåå¢åŠ "Implication"æˆ–"This finding suggests that"

**å®ç°**:

#### Ablation Study - ç“¶é¢ˆå‘ç°
```latex
\textbf{Implication:} This finding has profound implications for the BIQA 
fieldâ€”the primary bottleneck for current content-adaptive models is not 
the adaptive mechanism itself, but the feature extractor's representational 
power. Upgrading to Transformer backbones could unlock significant 
performance gains for a wide range of existing IQA models, suggesting a 
clear path forward for next-generation architectures.
```

#### Multi-Scale Fusion
```latex
\textbf{This finding suggests that} dynamic, content-aware resource 
allocation across the feature hierarchy is more effective than fixed 
fusion strategies, providing a crucial design principle for future 
architectures.
```

#### Cross-Dataset (KADID-10K)
```latex
\textbf{Implication:} While performance drops, the smaller degradation 
relative to the baseline demonstrates that our model's richer 
representations offer better, albeit still limited, generalization to 
synthetic distortionsâ€”suggesting that hierarchical transformer features 
capture more transferable quality-relevant patterns than CNN features.
```

**æ•ˆæœ**: ä»"å‘ˆç°ç»“æœ"å‡åä¸º"æ­ç¤ºæ´è§"

---

### 5. **è¯­è¨€ä¼˜åŒ– - æ›´è‡ªä¿¡æœ‰åŠ›** âœ…

**ç›®æ ‡**: æ›¿æ¢æ¨¡ç³Šã€ä¸ç¡®å®šçš„è¡¨è¾¾

**å®ç°çš„æ›¿æ¢**:

| æ—§è¡¨è¾¾ (æ¨¡ç³Š) | æ–°è¡¨è¾¾ (æœ‰åŠ›) |
|--------------|--------------|
| "suggests that" | "**validates that**" / "**demonstrates that**" |
| "indicates that" | "**validates that**" |
| "We hypothesize this stems from" | "**This is attributed to**" |
| "The visualization reveals" | "**Our analysis reveals**" |
| "suggests that performance saturation" | "**demonstrates that performance saturation**" |

**ç¤ºä¾‹å¯¹æ¯”**:

```
æ—§: "This suggests that performance saturation occurs..."
æ–°: "This demonstrates that performance saturation occurs..."

æ—§: "The relative improvement suggests that our model learns..."
æ–°: "The relative improvement validates that our model learns..."

æ—§: "We hypothesize this stems from the AFA module's ability..."
æ–°: "This is attributed to the AFA module's ability..."
```

**æ•ˆæœ**: è®ºæ–­æ›´åšå®šï¼Œå­¦æœ¯è¯­è¨€æ›´ä¸“ä¸š

---

## ğŸ“ˆ å…³é”®æ”¹è¿›ç»Ÿè®¡

### ç¯‡å¹…ä¼˜åŒ–

| éƒ¨åˆ† | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | å‹ç¼©ç‡ |
|------|--------|--------|--------|
| **Related Work** | ~30è¡Œ | ~13è¡Œ | -57% |
| **Model Size Section** | 1é¡µ | 2è¡Œ | -95% |
| **æ€»é¡µæ•°** | 17é¡µ | 16é¡µ | -1é¡µ |

### å†…å®¹å¼ºåŒ–

| æ–¹é¢ | æ–°å¢å†…å®¹ |
|------|----------|
| **MethodåŠ¨æœº** | +2æ®µ "Why" è¯´æ˜ |
| **Experimentsæ´è§** | +3å¤„ "Implication" æ€»ç»“ |
| **è¯­è¨€åŠ›åº¦** | ~10å¤„ æ¨¡ç³Šâ†’åšå®š æ›¿æ¢ |

---

## ğŸ¯ ä¼˜åŒ–æ•ˆæœè¯„ä¼°

### âœ… è¾¾æˆçš„ç›®æ ‡

| ç›®æ ‡ | çŠ¶æ€ | æ•ˆæœ |
|------|------|------|
| 1. ç²¾ç®€Related Work | âœ… å®Œæˆ | å‹ç¼©57%ï¼Œä¿æŒæ ¸å¿ƒä¿¡æ¯ |
| 2. ç®€åŒ–Swin/HyperNet | âœ… å®Œæˆ | å·²åœ¨ç¬¬ä¸€è½®å®Œæˆ |
| 3. ç§»åŠ¨æ¬¡è¦åˆ†æ | âœ… å®Œæˆ | æ­£æ–‡å‡å°‘~1é¡µ |
| 4. å¼ºåŒ–"ä¸ºä»€ä¹ˆ" | âœ… å®Œæˆ | MethodåŠ¨æœºæ›´æ¸…æ™° |
| 5. å¢åŠ Implication | âœ… å®Œæˆ | å®éªŒæ´è§æ›´æ·±åˆ» |
| 6. è¯­è¨€ä¼˜åŒ– | âœ… å®Œæˆ | è¡¨è¾¾æ›´è‡ªä¿¡æœ‰åŠ› |
| 7. ç¼–è¯‘æ£€æŸ¥ | âœ… å®Œæˆ | 0é”™è¯¯ï¼Œ16é¡µ |

### ğŸ¨ è®ºæ–‡è´¨é‡æå‡

**ä» â†’ åˆ°**:
- âŒ "è¯¦ç»†ä½†å†—é•¿" â†’ âœ… "ç²¾ç‚¼ä¸”æœ‰åŠ›"
- âŒ "å¹³é“ºç›´å™" â†’ âœ… "æ­ç¤ºæ´è§"
- âŒ "æ¨¡ç³Šè¡¨è¾¾" â†’ âœ… "åšå®šè®ºæ–­"
- âŒ "æŠ€æœ¯å †ç Œ" â†’ âœ… "ç†è®ºæ·±åŒ–"

---

## ğŸ“Š æœ€ç»ˆçŠ¶æ€

```
âœ… ç¼–è¯‘æˆåŠŸ: 0 é”™è¯¯
ğŸ“„ æ€»é¡µæ•°: 16é¡µ (ä»17é¡µå‹ç¼©)
ğŸ“ æ­£æ–‡: ~10é¡µ
ğŸ“ é™„å½•: ~5-6é¡µ
âœ… Related Work: å¤§å¹…ç²¾ç®€
âœ… Method: åŠ¨æœºå¼ºåŒ–
âœ… Experiments: æ´è§æ·±åŒ–
âœ… è¯­è¨€: æ›´ä¸“ä¸šæœ‰åŠ›
```

---

## ğŸ”„ ä¸¤è½®ä¼˜åŒ–å¯¹æ¯”

### ç¬¬ä¸€è½®ä¼˜åŒ– (åŸºäºWRITING_SUGGESTIONS.md Part 1)
- **é‡ç‚¹**: é‡å¡‘æ•…äº‹çº¿ï¼Œå¼ºåŒ–å™äº‹ç»“æ„
- **æˆæœ**: ä»"å®éªŒæŠ¥å‘Š"å‡åä¸º"é¡¶çº§å­¦æœ¯è®ºæ–‡"
- **æ ¸å¿ƒ**: Introductioné‡å†™ + Related Worké‡æ„ + Conclusionå‡å

### ç¬¬äºŒè½®ä¼˜åŒ– (åŸºäºWRITING_SUGGESTIONS.md Part 2-3)
- **é‡ç‚¹**: ç²¾ç‚¼ç¯‡å¹…ï¼Œå¼ºåŒ–è®ºè¿°ï¼Œæå‡è¯­è¨€
- **æˆæœ**: ä»"è¯¦å°½å…¨é¢"ç²¾ç‚¼ä¸º"ç®€æ´æœ‰åŠ›"
- **æ ¸å¿ƒ**: ç¯‡å¹…å‹ç¼© + åŠ¨æœºå¼ºåŒ– + æ´è§æ­ç¤º + è¯­è¨€ä¼˜åŒ–

---

## ğŸš€ è®ºæ–‡å½“å‰ä¼˜åŠ¿

### 1. **æ¸…æ™°çš„å™äº‹ä¸»çº¿**
- èŒƒå¼è½¬å˜ â†’ ç“¶é¢ˆè¯†åˆ« â†’ åˆ›æ–°è§£å†³ â†’ æ·±åˆ»æ´è§

### 2. **çªå‡ºçš„æ ¸å¿ƒè´¡çŒ®**
- 87%ç“¶é¢ˆå‘ç° (é‡åŒ–è¯æ®)
- 99.67%æ³¨æ„åŠ›æ¨¡å¼ (å¯è§£é‡Šè¡Œä¸º)
- åŠ¨æ€èµ„æºåˆ†é… (è®¾è®¡åŸåˆ™)

### 3. **æ·±åˆ»çš„å­¦æœ¯æ´è§**
- ä¸ä»…æ˜¯æ€§èƒ½æå‡
- æ›´é‡è¦çš„æ˜¯ç†è§£"ä¸ºä»€ä¹ˆ"å’Œ"å¦‚ä½•"
- ä¸ºä¸‹ä¸€ä»£æ¶æ„æä¾›æŒ‡å¯¼

### 4. **ç²¾ç‚¼çš„è¡¨è¾¾æ–¹å¼**
- Related Work: 3æ®µç²¾ç‚¼æ¦‚æ‹¬
- Method: åŠ¨æœºæ¸…æ™°ï¼Œç†è®ºæ‰å®
- Experiments: ç»“æœ+æ´è§å¹¶é‡
- è¯­è¨€: è‡ªä¿¡ã€ä¸“ä¸šã€æœ‰åŠ›

---

## ğŸ“‹ åç»­å»ºè®®

### å¯é€‰çš„è¿›ä¸€æ­¥ä¼˜åŒ–

1. **å›¾è¡¨ä¼˜åŒ–** (å¦‚WRITING_SUGGESTIONS.md Part 3.2æ‰€å»ºè®®):
   - Figure 3 (Ablation): åœ¨æŸ±çŠ¶å›¾ä¸Šæ ‡æ³¨æ€§èƒ½æå‡ç™¾åˆ†æ¯”
   - Figure 5 (Attention): åœ¨captionä¸­åŠ å…¥"Key finding"æ€»ç»“
   - Figure 6 (Scatter): ç»˜åˆ¶è¯¯å·®èŒƒå›´è™šçº¿

2. **æœ€ç»ˆæ¶¦è‰²**:
   - æ£€æŸ¥æ‰€æœ‰å›¾è¡¨captionæ˜¯å¦æ”¯æŒæ•…äº‹çº¿
   - ç¡®ä¿æ‰€æœ‰cross-referenceæ­£ç¡®
   - æœ€åä¸€éè¯­è¨€æ¶¦è‰²

---

## âœ… æ€»ç»“

ç»è¿‡ä¸¤è½®ç³»ç»Ÿæ€§ä¼˜åŒ–ï¼Œè®ºæ–‡å·²ç»ï¼š

1. âœ… **å™äº‹æ¸…æ™°**: ä»èŒƒå¼è½¬å˜åˆ°æ·±åˆ»æ´è§çš„å®Œæ•´æ•…äº‹çº¿
2. âœ… **é‡ç‚¹çªå‡º**: æ ¸å¿ƒè´¡çŒ®å’Œåˆ›æ–°ç‚¹é«˜åº¦å‡¸æ˜¾
3. âœ… **ç¯‡å¹…ç²¾ç‚¼**: ä»17é¡µå‹ç¼©åˆ°16é¡µï¼Œåˆ ç¹å°±ç®€
4. âœ… **è®ºè¿°æ·±åˆ»**: ä¸ä»…å‘ˆç°ç»“æœï¼Œæ›´æ­ç¤ºæ´è§
5. âœ… **è¯­è¨€ä¸“ä¸š**: è‡ªä¿¡ã€æœ‰åŠ›ã€ç¬¦åˆé¡¶çº§ä¼šè®®æ ‡å‡†

**è®ºæ–‡ç°åœ¨å·²ç»å®Œå…¨å‡†å¤‡å¥½æŠ•ç¨¿ï¼** ğŸ‰

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶æ¸…å•

- `IEEE-conference-template-062824/IEEE-conference-template-062824.tex` (ä¸»æ–‡ä»¶)
  - Related Work: å¤§å¹…ç²¾ç®€
  - Method: å¼ºåŒ–åŠ¨æœº
  - Experiments: åˆ é™¤Model Size sectionï¼Œå¢åŠ Implication
  - è¯­è¨€: å¤šå¤„ä¼˜åŒ–

**æäº¤ä¿¡æ¯**: "Second round refinement: streamline content, strengthen arguments, polish language"

