# üöÄ Quick Start Guide - Tonight's Experiments (v2)

**Date**: Dec 22, 2025  
**Hardware**: 2 √ó GPU  
**Baseline**: Alpha=0.3 (SRCC 0.9352, PLCC 0.9460)

---

## üìä What Changed

### v1 ‚Üí v2 Updates:
1. ‚úÖ **2 GPUs** instead of 3 (adjusted parallel execution)
2. ‚úÖ **No tee** - logs saved automatically by train_swin.py
3. ‚úÖ **D experiments unchanged** - already one variable at a time
4. ‚úÖ **E experiments expanded** - 5 LR points instead of 2

---

## üéØ Tonight's Mission

Run **18 experiments** in **10 parallel batches**  
Total time: **~15 hours** (perfect for overnight)

### Experiments List:
- **3** Core Ablations (A1-A3): Remove Attention, Ranking, Multi-scale
- **3** Ranking Sensitivity (C1-C3): Alpha 0.1, 0.5, 0.7
- **2** Model Size (B1-B2): Tiny, Small
- **6** Regularization (D1-D6): Each changes ONE parameter only
  - D1-D2: Weight decay (1e-4, 3e-4)
  - D3-D4: Drop path (0.2, 0.4)
  - D5-D6: Dropout (0.3, 0.5)
- **4** Learning Rate (E1-E2, E4-E5): 0.5√ó, 0.75√ó, 1.5√ó, 2√ó
  - E3 (1√ó baseline) = already done!

---

## ‚ö° How to Run (2 GPUs)

### Recommended: Tmux Script

```bash
cd /root/Perceptual-IQA-CS3324
./run_experiments_2gpus.sh
```

**Features**:
- ‚úÖ Each experiment in separate tmux window
- ‚úÖ Automatic GPU assignment (GPU 0 and GPU 1)
- ‚úÖ Easy to monitor individual experiments
- ‚úÖ Built-in monitoring window with GPU status
- ‚úÖ Can attach/detach anytime

**How to use**:
```bash
# After running the script, detach with:
Ctrl+b, d

# Tomorrow morning, re-attach:
tmux attach -t ablation_2gpu

# List all experiments:
tmux list-windows -t ablation_2gpu

# Switch windows:
Ctrl+b, w    # Interactive window selector
Ctrl+b, n    # Next window
Ctrl+b, p    # Previous window
```

---

## üìä Execution Timeline (2 GPUs)

### Parallel Execution (15 hours total):

| Time | Batch | GPU 0 | GPU 1 | Duration |
|------|-------|-------|-------|----------|
| 00:00-01:30 | 1 | A1 (No Att) | A2 (No Rank) | 1.5h |
| 01:30-03:00 | 2 | A3 (No Multi) | - | 1.5h |
| 03:00-04:30 | 3 | C1 (Œ±=0.1) | C2 (Œ±=0.5) | 1.5h |
| 04:30-06:00 | 4 | C3 (Œ±=0.7) | - | 1.5h |
| 06:00-07:30 | 5 | B1 (Tiny) | B2 (Small) | 1.5h |
| 07:30-09:00 | 6 | D1 (WD‚Üì) | D2 (WD‚Üë) | 1.5h |
| 09:00-10:30 | 7 | D3 (DP‚Üì) | D4 (DP‚Üë) | 1.5h |
| 10:30-12:00 | 8 | D5 (DO‚Üì) | D6 (DO‚Üë) | 1.5h |
| 12:00-13:30 | 9 | E1 (LR 0.5√ó) | E2 (LR 0.75√ó) | 1.5h |
| 13:30-15:00 | 10 | E4 (LR 1.5√ó) | E5 (LR 2√ó) | 1.5h |

**If started at 11 PM ‚Üí Done by 2 PM next day** üåû

---

## üìÅ Where to Find Results

### Logs location:
```
/root/Perceptual-IQA-CS3324/logs/
```

**Log naming** (auto-generated by train_swin.py):
- Format: `swin_[config]_YYYYMMDD_HHMMSS.log`
- Search by experiment type in log content

### Checkpoints location:
```
/root/Perceptual-IQA-CS3324/checkpoints/
```

**Checkpoint naming**:
- Format: `koniq-10k-swin-[config]_YYYYMMDD_HHMMSS/`
- Best models: `best_model_srcc_X.XXXX_plcc_X.XXXX.pkl`

---

## üîç Monitoring Progress

### Check GPU usage:
```bash
watch -n 1 nvidia-smi
```

### Count running experiments:
```bash
ps aux | grep train_swin.py | grep -v grep | wc -l
```

### Check latest logs:
```bash
cd /root/Perceptual-IQA-CS3324/logs
ls -lt *.log | head -10
tail -f <latest_log_file>
```

### Monitor via tmux:
```bash
tmux attach -t ablation_2gpu
# Navigate to "monitor" window to see GPU status and latest logs
```

---

## üìà Expected Results

### Core Ablations (vs baseline 0.9352):
- **A1 (Remove Attention)**: ~0.9325 (-0.27%)
- **A2 (Remove Ranking)**: ~0.9347 (-0.05%)
- **A3 (Remove Multi-scale)**: ~0.9318 (-0.34%)

### Ranking Sensitivity:
- **C1 (Alpha=0.1)**: ~0.9340
- **C2 (Alpha=0.5)**: ~0.9343
- **C3 (Alpha=0.7)**: ~0.9330

### Model Size:
- **B1 (Tiny)**: ~0.91-0.92
- **B2 (Small)**: ~0.92-0.93

### Regularization & LR:
- Will show sensitivity curves for paper

---

## üéØ Tomorrow Tasks

### 1. Check completion status:
```bash
cd /root/Perceptual-IQA-CS3324
ls -lh logs/*.log | wc -l  # Should see 18+ new log files
```

### 2. Extract best SRCC from all experiments:
```bash
# Create a results summary script
cat > extract_results.sh << 'EOF'
#!/bin/bash
echo "Experiment Results Summary"
echo "=========================="
for log in logs/swin_*.log; do
    if [ -f "$log" ]; then
        echo "$(basename $log):"
        grep "New best model" $log | tail -1 | awk '{print "  ", $0}'
    fi
done
EOF

chmod +x extract_results.sh
./extract_results.sh
```

### 3. Organize results by experiment type:
```bash
# Core Ablations
grep -l "no.*attention\|ranking_loss_alpha 0\|no_multiscale" logs/swin_*.log | \
  xargs -I {} bash -c 'echo {}; grep "New best" {} | tail -1'

# Ranking Sensitivity
grep -l "ranking_loss_alpha 0\.[1357]" logs/swin_*.log | \
  xargs -I {} bash -c 'echo {}; grep "New best" {} | tail -1'

# Model Size
grep -l "model_size.*tiny\|model_size.*small" logs/swin_*.log | \
  xargs -I {} bash -c 'echo {}; grep "New best" {} | tail -1'

# Regularization
grep -l "weight_decay\|drop_path\|dropout" logs/swin_*.log | \
  xargs -I {} bash -c 'echo {}; grep "New best" {} | tail -1'

# Learning Rate
grep -l "lr.*2\.5e-6\|lr.*3\.75e-6\|lr.*7\.5e-6\|lr.*1e-5" logs/swin_*.log | \
  xargs -I {} bash -c 'echo {}; grep "New best" {} | tail -1'
```

### 4. Update documentation:
- `VALIDATION_AND_ABLATION_LOG.md`
- `EXPERIMENTS_TO_RUN_v2.md` (mark experiments as done)
- Create results summary table

### 5. Create visualizations:
- Component contribution bar chart
- Ranking loss sensitivity curve (5 points: 0, 0.1, 0.3, 0.5, 0.7)
- Regularization sensitivity curves (3 separate plots)
- Learning rate sensitivity curve (5 points)

### 6. Cross-dataset testing (if new best model found):
```bash
python cross_dataset_test.py \
  --checkpoint <new_best_checkpoint>.pkl \
  --model_size base \
  --test_patch_num 20 \
  --test_random_crop
```

---

## üö® Troubleshooting

### If experiments fail to start:
```bash
# Check CUDA
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Device count: {torch.cuda.device_count()}')"

# Check dataset
ls -lh /root/Perceptual-IQA-CS3324/koniq-10k/
```

### If GPU OOM:
```bash
# Reduce batch size in experiment commands
# Change --batch_size 32 to --batch_size 16
```

### If tmux session crashes:
```bash
# Kill and restart
tmux kill-session -t ablation_2gpu
./run_experiments_2gpus.sh
```

### Check individual experiment status:
```bash
tmux attach -t ablation_2gpu
# Use Ctrl+b, w to see all windows
# Look for windows that have finished (command prompt visible)
```

---

## üí° Key Improvements in v2

1. **2-GPU optimized**: Perfect for your hardware
2. **No log duplication**: train_swin.py saves logs automatically
3. **D experiments clarified**: Each changes only ONE parameter
4. **More LR points**: 5 learning rates for better sensitivity curve
5. **Better monitoring**: Enhanced monitoring window with log preview

---

## üéâ Ready to Go!

**Run tonight**:

```bash
cd /root/Perceptual-IQA-CS3324
./run_experiments_2gpus.sh

# Detach and sleep üò¥
# Ctrl+b, d
```

**Good luck! üöÄ**


