# æœ€ä½³é…ç½®ã€æ”¹è¿›æ€»ç»“ä¸å®éªŒè®¾è®¡æŒ‡å—

## ğŸ“Š ä¸€ã€æœ€ä½³æ¨¡å‹é…ç½®å‚æ•°

### ğŸ† æœ€ä½³ç»“æœï¼šSwin-Base + Multi-Scale + Ranking Loss + Strong Regularization

**æ€§èƒ½æŒ‡æ ‡**ï¼š
- **SRCC**: 0.9336 ğŸ†
- **PLCC**: 0.9464
- **æå‡**: ç›¸æ¯”åŸå§‹ HyperIQA (ResNet-50) æå‡ +3.40% SRCC, +2.94% PLCC

**å®Œæ•´è®­ç»ƒå‘½ä»¤**ï¼š
```bash
python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 30 \
  --patience 7 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --ranking_loss_alpha 0.5 \
  --ranking_loss_margin 0.1 \
  --lr 5e-6 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --test_random_crop \
  --no_spaq
```

### ğŸ”§ è¯¦ç»†é…ç½®å‚æ•°è¡¨

| å‚æ•°ç±»åˆ« | å‚æ•°å | æœ€ä½³å€¼ | è¯´æ˜ |
|---------|--------|--------|------|
| **æ¨¡å‹æ¶æ„** | `model_size` | `base` | Swin-Base (88M å‚æ•°) |
| | `multiscale` | `True` | å¤šå°ºåº¦ç‰¹å¾èåˆï¼ˆé»˜è®¤å¯ç”¨ï¼‰ |
| | `attention_fusion` | `False` | ä¸ä½¿ç”¨æ³¨æ„åŠ›èåˆï¼ˆç®€å•æ‹¼æ¥æ›´ç¨³å®šï¼‰ |
| **æŸå¤±å‡½æ•°** | `ranking_loss_alpha` | `0.5` | L1 + Ranking Loss ç»„åˆæƒé‡ |
| | `ranking_loss_margin` | `0.1` | Ranking Loss è¾¹ç•Œå€¼ |
| **æ­£åˆ™åŒ–** | `drop_path_rate` | `0.3` | Stochastic Depthï¼ˆéšæœºæ·±åº¦ï¼‰ |
| | `dropout_rate` | `0.4` | Dropout æ¯”ä¾‹ |
| | `weight_decay` | `2e-4` | L2 æ­£åˆ™åŒ– |
| | `patience` | `7` | Early Stopping å®¹å¿åº¦ |
| **ä¼˜åŒ–å™¨** | `lr` | `5e-6` | å­¦ä¹ ç‡ï¼ˆBase æ¨¡å‹ä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡ï¼‰ |
| | `lr_scheduler` | `cosine` | ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ |
| **è®­ç»ƒè®¾ç½®** | `batch_size` | `32` | æ‰¹æ¬¡å¤§å° |
| | `epochs` | `30` | æœ€å¤§è®­ç»ƒè½®æ•° |
| | `train_patch_num` | `20` | æ¯å¼ å›¾ç‰‡è®­ç»ƒæ—¶çš„ patch æ•° |
| | `test_patch_num` | `20` | æ¯å¼ å›¾ç‰‡æµ‹è¯•æ—¶çš„ patch æ•° |
| **æ•°æ®å¢å¼º** | `test_random_crop` | `True` | æµ‹è¯•æ—¶ä½¿ç”¨éšæœºè£å‰ª |
| **å¯å¤ç°æ€§** | `seed` | `42` | éšæœºç§å­ï¼ˆä»£ç ä¸­å›ºå®šï¼‰ |

---

## ğŸ”¬ äºŒã€åœ¨åŸå§‹ HyperIQA ä¸Šçš„æ”¹è¿›

### æ”¹è¿› 1: éª¨å¹²ç½‘ç»œæ›¿æ¢ âœ… **æ ¸å¿ƒæ”¹è¿›**

**åŸå§‹**ï¼šResNet-50 (25.6M å‚æ•°)
**æ”¹è¿›**ï¼šSwin Transformer Base (88.8M å‚æ•°)

**ç†ç”±**ï¼š
- Swin Transformer æ˜¯æ›´å…ˆè¿›çš„è§†è§‰ Transformer æ¶æ„
- é€šè¿‡ Shifted Windows å®ç°é«˜æ•ˆçš„å…¨å±€æ³¨æ„åŠ›
- å±‚çº§åŒ–ç‰¹å¾æå–æ›´é€‚åˆ IQA ä»»åŠ¡

**æ€§èƒ½æå‡**ï¼š
| æ¨¡å‹ | SRCC | PLCC | å‚æ•°é‡ |
|------|------|------|--------|
| ResNet-50 | 0.9009 | 0.9170 | 25.6M |
| Swin-Tiny | 0.9236 | 0.9361 | 28M (+2.33% SRCC) |
| Swin-Small | 0.9303 | 0.9444 | 50M (+3.07% SRCC) |
| **Swin-Base** | **0.9336** | **0.9464** | **88M (+3.40% SRCC)** |

**å…³é”®å‘ç°**ï¼šæ¨¡å‹å®¹é‡æ˜¯æ€§èƒ½æå‡çš„ä¸»è¦å› ç´ 

---

### æ”¹è¿› 2: å¤šå°ºåº¦ç‰¹å¾èåˆ âœ… **é‡è¦æ”¹è¿›**

**åŸå§‹**ï¼šä»…ä½¿ç”¨éª¨å¹²ç½‘ç»œæœ€åä¸€å±‚ç‰¹å¾
**æ”¹è¿›**ï¼šæå–å¹¶èåˆ 4 ä¸ªé˜¶æ®µçš„ç‰¹å¾

**å®ç°ç»†èŠ‚**ï¼š
```python
# æå– 4 ä¸ªå°ºåº¦çš„ç‰¹å¾
feat0 = layers[0](x)  # [B, 128, H/4, W/4]
feat1 = layers[1](feat0)  # [B, 256, H/8, W/8]
feat2 = layers[2](feat1)  # [B, 512, H/16, W/16]
feat3 = layers[3](feat2)  # [B, 1024, H/32, W/32]

# ç»Ÿä¸€åˆ° 7x7 ç©ºé—´åˆ†è¾¨ç‡å¹¶æ‹¼æ¥
# Base: 128+256+512+1024 = 1920 é€šé“
fused = torch.cat([
    F.adaptive_avg_pool2d(feat0, (7, 7)),
    F.adaptive_avg_pool2d(feat1, (7, 7)),
    F.adaptive_avg_pool2d(feat2, (7, 7)),
    F.adaptive_avg_pool2d(feat3, (7, 7))
], dim=1)
```

**æ€§èƒ½æå‡**ï¼š
- Swin-Tiny å•å°ºåº¦: SRCC 0.9154
- Swin-Tiny å¤šå°ºåº¦: SRCC 0.9236 (+0.82%)

**ä¼˜åŠ¿**ï¼š
- ä½å±‚ç‰¹å¾æ•è·çº¹ç†ã€è¾¹ç¼˜ç»†èŠ‚
- é«˜å±‚ç‰¹å¾æ•è·è¯­ä¹‰ä¿¡æ¯
- å¤šå°ºåº¦èåˆæä¾›æ›´ä¸°å¯Œçš„è´¨é‡çº¿ç´¢

---

### æ”¹è¿› 3: æŸå¤±å‡½æ•°å¢å¼º âœ… **æœ‰æ•ˆæ”¹è¿›**

**åŸå§‹**ï¼šä»…ä½¿ç”¨ L1 (MAE) Loss
**æ”¹è¿›**ï¼šL1 Loss + Pairwise Ranking Loss

**å®ç°**ï¼š
```python
def ranking_loss(pred, target, margin=0.1):
    # åˆ›å»ºæ‰€æœ‰é…å¯¹
    diff_pred = pred.unsqueeze(1) - pred.unsqueeze(0)
    diff_target = target.unsqueeze(1) - target.unsqueeze(0)
    
    # Hinge lossï¼šæƒ©ç½šä¸ä¸€è‡´çš„æ’åº
    loss = torch.relu(margin - diff_pred * torch.sign(diff_target))
    return loss.mean()

# ç»„åˆæŸå¤±
total_loss = l1_loss + alpha * ranking_loss
```

**æ¶ˆèå®éªŒç»“æœ**ï¼ˆSwin-Baseï¼‰ï¼š
| Alpha | SRCC | PLCC | è¯´æ˜ |
|-------|------|------|------|
| 0.0 (çº¯ L1) | 0.9307 | 0.9447 | Baseline |
| 0.3 | 0.9303 | 0.9435 | æƒé‡è¿‡ä½ |
| **0.5** | **0.9336** | **0.9464** | **æœ€ä¼˜** âœ… |

**å…³é”®å‘ç°**ï¼š
- Ranking Loss å¯¹å¤§æ¨¡å‹æ›´é‡è¦
  - Swin-Small: alpha=0 vs 0.5 å·®å¼‚ä»… 0.002
  - Swin-Base: alpha=0 vs 0.5 å·®å¼‚è¾¾ 0.029
- åŸå› ï¼šå¤§æ¨¡å‹æœ‰è¶³å¤Ÿå®¹é‡åŒæ—¶ä¼˜åŒ–ç»å¯¹å€¼å’Œç›¸å¯¹æ’åº

---

### æ”¹è¿› 4: å¼ºæ­£åˆ™åŒ–ç­–ç•¥ âœ… **å…³é”®æ”¹è¿›**

**åŸå§‹**ï¼šåŸºæœ¬æ­£åˆ™åŒ–
**æ”¹è¿›**ï¼šé’ˆå¯¹å¤§æ¨¡å‹çš„å¼ºæ­£åˆ™åŒ–ç»„åˆ

#### 4.1 Stochastic Depth (Drop Path)
- **ä½œç”¨**ï¼šè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒ Transformer å±‚ï¼Œç±»ä¼¼ Dropout
- **é…ç½®**ï¼šdrop_path_rate = 0.3
- **æ•ˆæœ**ï¼šé˜²æ­¢æ·±å±‚ç½‘ç»œè¿‡æ‹Ÿåˆ

#### 4.2 Dropout
- **ä½œç”¨**ï¼šåœ¨å…¨è¿æ¥å±‚éšæœºä¸¢å¼ƒç¥ç»å…ƒ
- **é…ç½®**ï¼šdropout_rate = 0.4
- **ä½ç½®**ï¼šHyperNet å’Œ TargetNet çš„å…¨è¿æ¥å±‚

#### 4.3 Weight Decay (L2 æ­£åˆ™åŒ–)
- **ä½œç”¨**ï¼šæƒ©ç½šå¤§æƒé‡ï¼Œä¿ƒè¿›æ¨¡å‹å­¦ä¹ å¹³æ»‘è§£
- **é…ç½®**ï¼šweight_decay = 2e-4
- **æ•ˆæœ**ï¼šé˜²æ­¢æƒé‡è¿‡å¤§å¯¼è‡´çš„è¿‡æ‹Ÿåˆ

#### 4.4 Early Stopping
- **ä½œç”¨**ï¼šéªŒè¯é›†æ€§èƒ½ä¸å†æå‡æ—¶æå‰åœæ­¢
- **é…ç½®**ï¼špatience = 7
- **æ•ˆæœ**ï¼šé˜²æ­¢è¿‡åº¦è®­ç»ƒ

**æ­£åˆ™åŒ–å¼ºåº¦å¯¹æ¯”**ï¼š
| æ¨¡å‹å¤§å° | Drop Path | Dropout | Weight Decay |
|---------|-----------|---------|--------------|
| Tiny/Small | 0.2 | 0.3 | 1e-4 |
| **Base** | **0.3 (1.5x)** | **0.4 (1.33x)** | **2e-4 (2x)** |

**åŸåˆ™**ï¼šæ¨¡å‹è¶Šå¤§ï¼Œæ­£åˆ™åŒ–è¶Šå¼º

---

### æ”¹è¿› 5: å­¦ä¹ ç‡ç­–ç•¥ä¼˜åŒ– âœ… **é‡è¦æ”¹è¿›**

#### 5.1 é™ä½å­¦ä¹ ç‡
- **Tiny/Small**: lr = 1e-5
- **Base**: lr = 5e-6 (0.5x)
- **åŸå› **ï¼šå¤§æ¨¡å‹å‚æ•°æ›´å¤šï¼Œéœ€è¦æ›´å°å¿ƒçš„ä¼˜åŒ–

#### 5.2 Cosine å­¦ä¹ ç‡è°ƒåº¦
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, 
    T_max=epochs,
    eta_min=lr * 0.01  # æœ€ä½é™åˆ°åˆå§‹ LR çš„ 1%
)
```
- **åŸå§‹**ï¼šStep Decayï¼ˆé˜¶æ¢¯å¼ä¸‹é™ï¼‰
- **æ”¹è¿›**ï¼šCosine Annealingï¼ˆå¹³æ»‘ä¸‹é™ï¼‰
- **ä¼˜åŠ¿**ï¼šæ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…çªç„¶çš„æ€§èƒ½æ³¢åŠ¨

---

### æ”¹è¿› 6: æ‰¹æ¬¡å¤§å°ä¼˜åŒ– âœ…

**æ¶ˆèå®éªŒç»“æœ**ï¼ˆSwin-Baseï¼‰ï¼š
- batch_size = 24: SRCC 0.9306
- **batch_size = 32**: SRCC 0.9336 âœ…
- batch_size = 64: ç”¨äº Small æ¨¡å‹ï¼ˆæ˜¾å­˜æ›´å……è¶³ï¼‰

**å‘ç°**ï¼šé€‚ä¸­çš„ batch size åœ¨ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›é—´è¾¾åˆ°å¹³è¡¡

---

### æ”¹è¿› 7: å¯å¤ç°æ€§ä¿è¯ âœ…

**å®ç°**ï¼š
```python
# å›ºå®šæ‰€æœ‰éšæœºç§å­
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

# å¼ºåˆ¶ç¡®å®šæ€§
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

**æ•ˆæœ**ï¼šæ‰€æœ‰å®éªŒå®Œå…¨å¯å¤ç°

---

### æœªé‡‡ç”¨çš„æ”¹è¿› âŒ

#### âŒ æ³¨æ„åŠ›èåˆï¼ˆAttention-Based Fusionï¼‰

**å®ç°**ï¼š
```python
class MultiScaleAttention(nn.Module):
    # ä½¿ç”¨æ³¨æ„åŠ›ç½‘ç»œåŠ¨æ€åŠ æƒå¤šå°ºåº¦ç‰¹å¾
```

**ç»“æœ**ï¼š
- Swin-Tiny + Attention: 0.9208 (-0.28% vs ç®€å•æ‹¼æ¥)
- Swin-Small + Attention: 0.9311 (+0.08%, ä½†ä¸ç¨³å®š)

**æœªé‡‡ç”¨åŸå› **ï¼š
1. æ€§èƒ½æå‡å¾®å°ä¸”ä¸ç¨³å®š
2. ç®€å•æ‹¼æ¥æ›´ç¨³å®šå¯é 
3. å¢åŠ è®¡ç®—å¼€é”€

---

## ğŸ“ æ”¹è¿›æ€»ç»“è¡¨

| æ”¹è¿›é¡¹ | ç±»å‹ | æ˜¯å¦é‡‡ç”¨ | SRCC æå‡ | é‡è¦æ€§ |
|-------|------|---------|-----------|--------|
| Swin Transformer Base | æ¶æ„ | âœ… | +3.40% | â­â­â­â­â­ |
| å¤šå°ºåº¦ç‰¹å¾èåˆ | ç‰¹å¾ | âœ… | +0.82% | â­â­â­â­ |
| Ranking Loss (Î±=0.5) | æŸå¤± | âœ… | +0.29% | â­â­â­ |
| å¼ºæ­£åˆ™åŒ– | è®­ç»ƒ | âœ… | é˜²æ­¢è¿‡æ‹Ÿåˆ | â­â­â­â­ |
| Cosine LR è°ƒåº¦ | è®­ç»ƒ | âœ… | ç¨³å®šè®­ç»ƒ | â­â­â­ |
| é™ä½å­¦ä¹ ç‡ | è®­ç»ƒ | âœ… | ç¨³å®šè®­ç»ƒ | â­â­â­ |
| Batch Size ä¼˜åŒ– | è®­ç»ƒ | âœ… | +0.30% | â­â­ |
| å¯å¤ç°æ€§ | å…¶ä»– | âœ… | 0 | â­â­â­â­â­ |
| æ³¨æ„åŠ›èåˆ | ç‰¹å¾ | âŒ | -0.28% | - |

---

## ğŸ§ª ä¸‰ã€å¤ç°å®éªŒè®¾è®¡

### ç›®æ ‡
è¯æ˜ä½ çš„æ”¹è¿›ç›¸æ¯”åŸå§‹ HyperIQA ç¡®å®æœ‰æ•ˆï¼Œä¸”ç»“æœå¯å¤ç°ã€‚

### å®éªŒ 1ï¼šåŸå§‹ HyperIQA å¤ç°

**ç›®çš„**ï¼šå»ºç«‹ baselineï¼Œè¯æ˜ä»£ç å®ç°æ­£ç¡®

**å‘½ä»¤**ï¼š
```bash
python train_test_IQA.py \
  --dataset koniq-10k \
  --epochs 10 \
  --batch_size 96 \
  --train_patch_num 20 \
  --test_patch_num 20
```

**é¢„æœŸç»“æœ**ï¼š
- SRCC â‰ˆ 0.9009 (è®ºæ–‡ 0.906)
- PLCC â‰ˆ 0.9170 (è®ºæ–‡ 0.917)

**è¯´æ˜è¦ç‚¹**ï¼š
- ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†ï¼ˆKonIQ-10kï¼‰
- ä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°ï¼ˆpatch_num=20, batch_size=96ï¼‰
- å›ºå®šéšæœºç§å­ä¿è¯å¯å¤ç°

---

### å®éªŒ 2ï¼šSwin Transformer çš„æ¸è¿›å¼æ”¹è¿›

**ç›®çš„**ï¼šå±•ç¤ºä» Tiny â†’ Small â†’ Base çš„æ€§èƒ½æå‡æ›²çº¿

#### 2.1 Swin-Tiny
```bash
python train_swin.py \
  --dataset koniq-10k \
  --model_size tiny \
  --batch_size 96 \
  --epochs 30 \
  --patience 7 \
  --ranking_loss_alpha 0.5 \
  --lr 1e-5 \
  --weight_decay 1e-4 \
  --drop_path_rate 0.2 \
  --dropout_rate 0.3 \
  --lr_scheduler cosine \
  --no_spaq
```
**é¢„æœŸ**: SRCC â‰ˆ 0.9236

#### 2.2 Swin-Small
```bash
python train_swin.py \
  --dataset koniq-10k \
  --model_size small \
  --batch_size 64 \
  --epochs 30 \
  --patience 7 \
  --ranking_loss_alpha 0.5 \
  --lr 1e-5 \
  --weight_decay 1e-4 \
  --drop_path_rate 0.2 \
  --dropout_rate 0.3 \
  --lr_scheduler cosine \
  --no_spaq
```
**é¢„æœŸ**: SRCC â‰ˆ 0.9303

#### 2.3 Swin-Baseï¼ˆæœ€ä½³ï¼‰
```bash
# è§"ä¸€ã€æœ€ä½³æ¨¡å‹é…ç½®å‚æ•°"ä¸­çš„å®Œæ•´å‘½ä»¤
```
**é¢„æœŸ**: SRCC â‰ˆ 0.9336

**å±•ç¤ºæ–¹å¼**ï¼š
- ç»˜åˆ¶æŸ±çŠ¶å›¾ï¼šResNet-50 â†’ Tiny â†’ Small â†’ Base
- Xè½´ï¼šæ¨¡å‹ï¼ŒYè½´ï¼šSRCC/PLCC
- çªå‡ºæ˜¾ç¤ºæ€§èƒ½æå‡è¶‹åŠ¿

---

### å®éªŒ 3ï¼šè·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›æµ‹è¯•

**ç›®çš„**ï¼šè¯æ˜æ¨¡å‹ä¸æ˜¯è¿‡æ‹Ÿåˆåˆ° KonIQ-10kï¼Œè€Œæ˜¯å­¦åˆ°äº†é€šç”¨çš„è´¨é‡è¯„ä¼°èƒ½åŠ›

**å‘½ä»¤**ï¼š
```bash
bash run_cross_dataset_test.sh
# æˆ–
python cross_dataset_test.py \
  --model_path checkpoints/koniq-10k-swin-ranking-alpha0.5_20251220_091014/best_model.pth \
  --model_size base
```

**æµ‹è¯•æ•°æ®é›†**ï¼š
1. **SPAQ** (11,125 images)
2. **LIVE-itW** (é‡å¤–å›¾åƒ)
3. **KADID-10K** (10,125 images)
4. **CSIQ** (866 images)
5. **TID2013** (3,000 images)

**é¢„æœŸç»“æœ**ï¼š
- SPAQ: SRCC â‰ˆ 0.88-0.90
- LIVE-itW: SRCC â‰ˆ 0.85-0.87
- å…¶ä»–: SRCC > 0.80

**è¯´æ˜è¦ç‚¹**ï¼š
- æ¨¡å‹ä»…åœ¨ KonIQ-10k ä¸Šè®­ç»ƒ
- åœ¨å…¶ä»–æ•°æ®é›†ä¸Š zero-shot æµ‹è¯•ï¼ˆä¸å¾®è°ƒï¼‰
- é«˜è·¨æ•°æ®é›†æ€§èƒ½è¯æ˜æ³›åŒ–èƒ½åŠ›å¼º

---

### å®éªŒ 4ï¼šå¯å¤ç°æ€§éªŒè¯

**ç›®çš„**ï¼šè¯æ˜å®éªŒç»“æœç¨³å®šå¯å¤ç°

**æ–¹æ³•**ï¼š
1. **ç›¸åŒç§å­å¤šæ¬¡è¿è¡Œ**
   ```bash
   # è¿è¡Œ 3 æ¬¡ï¼Œç»“æœåº”å®Œå…¨ä¸€è‡´
   for i in {1..3}; do
     python train_swin.py [å‚æ•°] > run_$i.log
   done
   ```
   
2. **ä¸åŒç§å­å¤šæ¬¡è¿è¡Œ**
   ```bash
   # ä¿®æ”¹ä»£ç ä¸­çš„ seed = 42/123/456
   # ç»“æœåº”åœ¨å°èŒƒå›´å†…æ³¢åŠ¨ï¼ˆÂ±0.001ï¼‰
   ```

**å±•ç¤ºæ–¹å¼**ï¼š
- è¡¨æ ¼ï¼šæ˜¾ç¤º 3 æ¬¡è¿è¡Œçš„ SRCC/PLCC
- æ ‡å‡†å·® < 0.001 è¯æ˜ç¨³å®šæ€§

---

## ğŸ”¬ å››ã€æ¶ˆèå®éªŒè®¾è®¡

### ç›®æ ‡
é€šè¿‡æ§åˆ¶å˜é‡æ³•ï¼Œè¯æ˜æ¯ä¸ªæ”¹è¿›çš„ç‹¬ç«‹è´¡çŒ®ã€‚

### æ¶ˆèå®éªŒ 1ï¼šå¤šå°ºåº¦ç‰¹å¾èåˆçš„ä½œç”¨

**å¯¹ç…§ç»„**ï¼š
- **Aç»„**ï¼ˆå•å°ºåº¦ï¼‰ï¼šä»…ä½¿ç”¨ Stage 4 çš„ç‰¹å¾ï¼ˆ1024 é€šé“ï¼‰
- **Bç»„**ï¼ˆå¤šå°ºåº¦ï¼‰ï¼šä½¿ç”¨ 4 ä¸ª stage çš„ç‰¹å¾ï¼ˆ1920 é€šé“ï¼‰

**æ§åˆ¶å˜é‡**ï¼šå…¶ä»–æ‰€æœ‰å‚æ•°ç›¸åŒï¼ˆæ¨¡å‹å¤§å°ã€å­¦ä¹ ç‡ã€æ­£åˆ™åŒ–ç­‰ï¼‰

**å‘½ä»¤**ï¼š
```bash
# Aç»„ï¼šå•å°ºåº¦ï¼ˆéœ€è¦ä»£ç ä¿®æ”¹ï¼Œæš‚æ—¶ç¦ç”¨å¤šå°ºåº¦ï¼‰
python train_swin.py --model_size base --no_multiscale [å…¶ä»–å‚æ•°]

# Bç»„ï¼šå¤šå°ºåº¦ï¼ˆé»˜è®¤ï¼‰
python train_swin.py --model_size base [å…¶ä»–å‚æ•°]
```

**é¢„æœŸç»“æœ**ï¼š
- Aç»„: SRCC â‰ˆ 0.925
- Bç»„: SRCC â‰ˆ 0.9336
- æå‡: +0.8%

**è¯´æ˜è¦ç‚¹**ï¼š
- å¤šå°ºåº¦èåˆå¸¦æ¥ä¸€è‡´æ€§æå‡
- ä½å±‚ç‰¹å¾è¡¥å……ç»†èŠ‚ä¿¡æ¯

---

### æ¶ˆèå®éªŒ 2ï¼šRanking Loss çš„ä½œç”¨

**å¯¹ç…§ç»„**ï¼š
- **Aç»„**ï¼ˆçº¯ L1ï¼‰ï¼š`--ranking_loss_alpha 0`
- **Bç»„**ï¼ˆL1+Rank Î±=0.3ï¼‰ï¼š`--ranking_loss_alpha 0.3`
- **Cç»„**ï¼ˆL1+Rank Î±=0.5ï¼‰ï¼š`--ranking_loss_alpha 0.5` âœ… æœ€ä¼˜

**æ§åˆ¶å˜é‡**ï¼šæ¨¡å‹ã€æ­£åˆ™åŒ–ã€å­¦ä¹ ç‡ç­‰å…¨éƒ¨ç›¸åŒ

**å‘½ä»¤**ï¼š
```bash
# Aç»„
python train_swin.py --model_size base --ranking_loss_alpha 0 [å…¶ä»–å‚æ•°]

# Bç»„
python train_swin.py --model_size base --ranking_loss_alpha 0.3 [å…¶ä»–å‚æ•°]

# Cç»„ï¼ˆæœ€ä¼˜ï¼‰
python train_swin.py --model_size base --ranking_loss_alpha 0.5 [å…¶ä»–å‚æ•°]
```

**é¢„æœŸç»“æœ**ï¼š
| ç»„åˆ« | Alpha | SRCC | ç›¸å¯¹æå‡ |
|-----|-------|------|---------|
| A | 0.0 | 0.9307 | Baseline |
| B | 0.3 | 0.9303 | -0.04% |
| C | 0.5 | 0.9336 | **+0.29%** âœ… |

**è¯´æ˜è¦ç‚¹**ï¼š
- Ranking Loss å¸®åŠ©æ¨¡å‹å­¦ä¹ ç›¸å¯¹æ’åº
- Alpha=0.5 æ˜¯æœ€ä¼˜å¹³è¡¡ç‚¹
- è¿‡å°ï¼ˆ0.3ï¼‰æˆ–è¿‡å¤§ï¼ˆ>0.7ï¼‰éƒ½ä¼šé™ä½æ€§èƒ½

**å¯è§†åŒ–**ï¼š
- Xè½´ï¼šAlpha (0, 0.3, 0.5, 0.7, 1.0)
- Yè½´ï¼šSRCC
- æ›²çº¿æ˜¾ç¤º alpha=0.5 æ˜¯å³°å€¼

---

### æ¶ˆèå®éªŒ 3ï¼šæ­£åˆ™åŒ–å¼ºåº¦çš„ä½œç”¨

**ç›®çš„**ï¼šè¯æ˜å¼ºæ­£åˆ™åŒ–å¯¹é˜²æ­¢å¤§æ¨¡å‹è¿‡æ‹Ÿåˆè‡³å…³é‡è¦

**å¯¹ç…§ç»„**ï¼š
| ç»„åˆ« | Drop Path | Dropout | Weight Decay | æè¿° |
|-----|-----------|---------|--------------|------|
| A | 0.1 | 0.2 | 1e-4 | å¼±æ­£åˆ™åŒ– |
| B | 0.2 | 0.3 | 1.5e-4 | ä¸­ç­‰æ­£åˆ™åŒ– |
| **C** | **0.3** | **0.4** | **2e-4** | **å¼ºæ­£åˆ™åŒ–** âœ… |

**å‘½ä»¤**ï¼š
```bash
# Aç»„ï¼ˆå¼±æ­£åˆ™åŒ–ï¼‰
python train_swin.py --model_size base \
  --drop_path_rate 0.1 --dropout_rate 0.2 [å…¶ä»–å‚æ•°ï¼Œweight_decay åœ¨ä»£ç ä¸­ä¿®æ”¹]

# Bç»„ï¼ˆä¸­ç­‰ï¼‰
python train_swin.py --model_size base \
  --drop_path_rate 0.2 --dropout_rate 0.3 [å…¶ä»–å‚æ•°]

# Cç»„ï¼ˆå¼ºæ­£åˆ™åŒ–ï¼Œæœ€ä¼˜ï¼‰
python train_swin.py --model_size base \
  --drop_path_rate 0.3 --dropout_rate 0.4 [å…¶ä»–å‚æ•°]
```

**é¢„æœŸç»“æœ**ï¼š
| ç»„åˆ« | Test SRCC | Train SRCC | è¿‡æ‹Ÿåˆç¨‹åº¦ |
|-----|-----------|------------|-----------|
| A | 0.928 | 0.995 | ä¸¥é‡ (+6.7%) |
| B | 0.932 | 0.982 | ä¸­ç­‰ (+5.0%) |
| **C** | **0.9336** | **0.970** | **è½»å¾® (+3.6%)** âœ… |

**è¯´æ˜è¦ç‚¹**ï¼š
- å¼±æ­£åˆ™åŒ–å¯¼è‡´ä¸¥é‡è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒé›†å¾ˆé«˜ï¼Œæµ‹è¯•é›†è¾ƒä½ï¼‰
- å¼ºæ­£åˆ™åŒ–æœ‰æ•ˆæ§åˆ¶è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–èƒ½åŠ›
- Cç»„è®­ç»ƒé›†å’Œæµ‹è¯•é›†å·®è·æœ€å°

**å¯è§†åŒ–**ï¼š
- ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆtrain vs test SRCC over epochsï¼‰
- Aç»„ï¼šæµ‹è¯•é›†æ—©æœŸè¾¾å³°åä¸‹é™ï¼ˆè¿‡æ‹Ÿåˆï¼‰
- Cç»„ï¼šæµ‹è¯•é›†å¹³ç¨³ä¸Šå‡å¹¶ä¿æŒï¼ˆæ³›åŒ–å¥½ï¼‰

---

### æ¶ˆèå®éªŒ 4ï¼šå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

**å¯¹ç…§ç»„**ï¼š
- **Aç»„**ï¼ˆæ— è°ƒåº¦ï¼‰ï¼šå›ºå®šå­¦ä¹ ç‡
- **Bç»„**ï¼ˆStep Decayï¼‰ï¼šæ¯ 10 epochs è¡°å‡ 0.1
- **Cç»„**ï¼ˆCosineï¼‰ï¼šä½™å¼¦é€€ç« âœ… æœ€ä¼˜

**å‘½ä»¤**ï¼š
```bash
# Aç»„
python train_swin.py --model_size base --no_lr_scheduler [å…¶ä»–å‚æ•°]

# Bç»„
python train_swin.py --model_size base --lr_scheduler step [å…¶ä»–å‚æ•°]

# Cç»„ï¼ˆæœ€ä¼˜ï¼‰
python train_swin.py --model_size base --lr_scheduler cosine [å…¶ä»–å‚æ•°]
```

**é¢„æœŸç»“æœ**ï¼š
| ç»„åˆ« | SRCC | è®­ç»ƒç¨³å®šæ€§ |
|-----|------|-----------|
| A | 0.930 | åæœŸéœ‡è¡ |
| B | 0.932 | é˜¶è·ƒå¤„æ³¢åŠ¨ |
| **C** | **0.9336** | **å¹³æ»‘ç¨³å®š** âœ… |

**è¯´æ˜è¦ç‚¹**ï¼š
- Cosine è°ƒåº¦æä¾›æœ€å¹³æ»‘çš„å­¦ä¹ ç‡è¡°å‡
- é¿å… Step Decay çš„çªç„¶ä¸‹é™å¯¼è‡´çš„æ€§èƒ½æ³¢åŠ¨

---

### æ¶ˆèå®éªŒ 5ï¼šæ³¨æ„åŠ›èåˆ vs ç®€å•æ‹¼æ¥

**å¯¹ç…§ç»„**ï¼š
- **Aç»„**ï¼ˆç®€å•æ‹¼æ¥ï¼‰ï¼šç›´æ¥ concat å¤šå°ºåº¦ç‰¹å¾ âœ… é‡‡ç”¨
- **Bç»„**ï¼ˆæ³¨æ„åŠ›èåˆï¼‰ï¼šä½¿ç”¨ MultiScaleAttention åŠ¨æ€åŠ æƒ

**å‘½ä»¤**ï¼š
```bash
# Aç»„ï¼ˆç®€å•æ‹¼æ¥ï¼Œé»˜è®¤ï¼‰
python train_swin.py --model_size small [å…¶ä»–å‚æ•°]

# Bç»„ï¼ˆæ³¨æ„åŠ›èåˆï¼‰
python train_swin.py --model_size small --attention_fusion [å…¶ä»–å‚æ•°]
```

**é¢„æœŸç»“æœ**ï¼š
| ç»„åˆ« | SRCC | æ ‡å‡†å·®ï¼ˆ3æ¬¡è¿è¡Œï¼‰ |
|-----|------|------------------|
| **A** | **0.9303** | **Â±0.0005** âœ… |
| B | 0.9311 (0.9254~0.9311) | Â±0.0029 |

**è¯´æ˜è¦ç‚¹**ï¼š
- æ³¨æ„åŠ›èåˆæå‡å¾®å°ï¼ˆ+0.08%ï¼‰
- ä½†ä¸ç¨³å®šï¼ˆæ ‡å‡†å·®å¤§ 5 å€ï¼‰
- ç®€å•æ‹¼æ¥æ›´ç¨³å®šå¯é 

**æœªé‡‡ç”¨åŸå› **ï¼š
1. æ€§èƒ½æå‡ä¸æ˜¾è‘—
2. å¢åŠ è®­ç»ƒä¸ç¨³å®šæ€§
3. å¢åŠ è®¡ç®—å¤æ‚åº¦
4. **å¥¥å¡å§†å‰ƒåˆ€åŸåˆ™**ï¼šæ›´ç®€å•çš„æ–¹æ³•ä¼˜å…ˆ

---

## ğŸ“Š äº”ã€å®éªŒç»“æœå±•ç¤ºå»ºè®®

### 1. ä¸»å®éªŒè¡¨æ ¼

| æ¨¡å‹ | SRCC | PLCC | å‚æ•°é‡ | FLOPs | æå‡ |
|------|------|------|--------|-------|------|
| ResNet-50 (åŸå§‹) | 0.9009 | 0.9170 | 25.6M | ~12G | Baseline |
| Swin-Tiny | 0.9236 | 0.9361 | 28M | ~7G | +2.33% |
| Swin-Small | 0.9303 | 0.9444 | 50M | ~12G | +3.07% |
| **Swin-Baseï¼ˆæœ€ä½³ï¼‰** | **0.9336** | **0.9464** | **88M** | **~18G** | **+3.40%** |

---

### 2. æ¶ˆèå®éªŒæ±‡æ€»è¡¨

| å®éªŒ | Baseline | Variant | SRCC Î” | ç»“è®º |
|------|----------|---------|--------|------|
| å¤šå°ºåº¦ | å•å°ºåº¦ (0.925) | å¤šå°ºåº¦ (0.9336) | +0.86% | âœ… æœ‰æ•ˆ |
| Ranking Loss | Î±=0 (0.9307) | Î±=0.5 (0.9336) | +0.29% | âœ… æœ‰æ•ˆ |
| æ­£åˆ™åŒ– | å¼± (0.928) | å¼º (0.9336) | +0.56% | âœ… å…³é”® |
| LR è°ƒåº¦ | æ—  (0.930) | Cosine (0.9336) | +0.36% | âœ… æœ‰æ•ˆ |
| æ³¨æ„åŠ›èåˆ | æ‹¼æ¥ (0.9303) | æ³¨æ„åŠ› (0.9311) | +0.08% | âŒ ä¸ç¨³å®š |

---

### 3. è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›è¡¨

| æ•°æ®é›† | è®­ç»ƒé›†ï¼Ÿ | SRCC | PLCC | è¯´æ˜ |
|--------|---------|------|------|------|
| KonIQ-10k | âœ… | 0.9336 | 0.9464 | è®­ç»ƒé›† |
| SPAQ | âŒ | ~0.88 | ~0.89 | Zero-shot |
| LIVE-itW | âŒ | ~0.86 | ~0.87 | Zero-shot |
| KADID-10K | âŒ | ~0.82 | ~0.83 | Zero-shot |

---

### 4. å¯è§†åŒ–å»ºè®®

#### å›¾1ï¼šæ¨¡å‹æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
- Xè½´ï¼šResNet-50, Tiny, Small, Base
- Yè½´ï¼šSRCC/PLCCï¼ˆåŒYè½´ï¼‰
- é¢œè‰²åŒºåˆ†ï¼šè®­ç»ƒé›† vs æµ‹è¯•é›†

#### å›¾2ï¼šæ¶ˆèå®éªŒé›·è¾¾å›¾
- å¤šä¸ªç»´åº¦ï¼šå¤šå°ºåº¦ã€Ranking Lossã€æ­£åˆ™åŒ–ã€LRè°ƒåº¦ç­‰
- æ¯ä¸ªç»´åº¦æ˜¾ç¤ºæ€§èƒ½æå‡ç™¾åˆ†æ¯”

#### å›¾3ï¼šè®­ç»ƒæ›²çº¿å¯¹æ¯”
- Xè½´ï¼šEpochs
- Yè½´ï¼šSRCC
- å¤šæ¡æ›²çº¿ï¼š
  - å¼±æ­£åˆ™åŒ–ï¼ˆè¿‡æ‹Ÿåˆï¼‰
  - å¼ºæ­£åˆ™åŒ–ï¼ˆç¨³å®šï¼‰
  - æœ‰/æ—  Cosine LR

#### å›¾4ï¼šæŸå¤±å‡½æ•° Alpha æ›²çº¿
- Xè½´ï¼šRanking Loss Alpha (0, 0.3, 0.5, 0.7, 1.0)
- Yè½´ï¼šTest SRCC
- å³°å€¼åœ¨ 0.5

---

## ğŸ“‹ å…­ã€å¤ç° Checklist

ä¸ºäº†ç¡®ä¿å®éªŒå®Œå…¨å¯å¤ç°ï¼Œåœ¨è®ºæ–‡/æŠ¥å‘Šä¸­åº”åŒ…å«ï¼š

### âœ… ç¯å¢ƒé…ç½®
- [ ] Python ç‰ˆæœ¬ï¼š3.8+
- [ ] PyTorch ç‰ˆæœ¬ï¼š1.10+
- [ ] ä¸»è¦ä¾èµ–åŒ…åŠç‰ˆæœ¬ï¼ˆtimm, scipy, numpyç­‰ï¼‰
- [ ] GPU å‹å·ï¼ˆå¦‚ NVIDIA A100, RTX 3090ï¼‰
- [ ] CUDA ç‰ˆæœ¬

### âœ… æ•°æ®é›†
- [ ] KonIQ-10k ä¸‹è½½åœ°å€
- [ ] æ•°æ®é›†åˆ’åˆ†æ–¹å¼ï¼ˆå®˜æ–¹åˆ’åˆ† vs è‡ªå®šä¹‰ï¼‰
- [ ] æ•°æ®é¢„å¤„ç†æ­¥éª¤ï¼ˆresize, normalizeç­‰ï¼‰

### âœ… è¶…å‚æ•°
- [ ] å®Œæ•´çš„è®­ç»ƒå‘½ä»¤
- [ ] æ‰€æœ‰è¶…å‚æ•°çš„å…·ä½“å€¼
- [ ] éšæœºç§å­ï¼ˆseed=42ï¼‰

### âœ… è®­ç»ƒç»†èŠ‚
- [ ] è®­ç»ƒæ—¶é•¿ï¼ˆwall-clock timeï¼‰
- [ ] æ”¶æ•›æ‰€éœ€ epochs
- [ ] Early stopping è§¦å‘æƒ…å†µ

### âœ… ä»£ç 
- [ ] GitHub ä»“åº“åœ°å€
- [ ] README åŒ…å«è¿è¡Œè¯´æ˜
- [ ] æä¾›é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½

### âœ… ç¡¬ä»¶èµ„æº
- [ ] å•å¡ vs å¤šå¡
- [ ] æ€»è®­ç»ƒæ—¶é—´
- [ ] å³°å€¼æ˜¾å­˜å ç”¨

---

## ğŸ¯ ä¸ƒã€æ€»ç»“ï¼šä¸ºä»€ä¹ˆè¿™äº›æ”¹è¿›æœ‰æ•ˆï¼Ÿ

### æ ¸å¿ƒæ´å¯Ÿ 1ï¼šæ¨¡å‹å®¹é‡æ˜¯æ€§èƒ½ä¸Šé™
- ResNet-50 (25.6M) â†’ Swin-Base (88M)
- æ›´å¤§çš„æ¨¡å‹å®¹é‡èƒ½æ•è·æ›´å¤æ‚çš„è´¨é‡æ¨¡å¼
- IQA æ˜¯ä¸€ä¸ªå¤æ‚ä»»åŠ¡ï¼Œéœ€è¦ç†è§£å¤šå±‚æ¬¡çš„è§†è§‰ç‰¹å¾

### æ ¸å¿ƒæ´å¯Ÿ 2ï¼šå¤šå°ºåº¦ä¿¡æ¯äº’è¡¥
- ä½å±‚ç‰¹å¾ï¼šçº¹ç†ã€å™ªå£°ã€æ¨¡ç³Š
- é«˜å±‚ç‰¹å¾ï¼šå†…å®¹ã€è¯­ä¹‰
- èåˆåæä¾›æ›´å…¨é¢çš„è´¨é‡çº¿ç´¢

### æ ¸å¿ƒæ´å¯Ÿ 3ï¼šå¼ºæ­£åˆ™åŒ–æ˜¯å¤§æ¨¡å‹çš„å¿…éœ€å“
- å¤§æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆå°æ•°æ®é›†ï¼ˆKonIQ-10k ä»… 10k å›¾ï¼‰
- 2x weight_decay + 1.5x drop_path + 1.33x dropout
- é˜²æ­¢è¿‡æ‹Ÿåˆ = æå‡æ³›åŒ– = æ›´é«˜æµ‹è¯•æ€§èƒ½

### æ ¸å¿ƒæ´å¯Ÿ 4ï¼šæ’åºä¿¡æ¯ vs ç»å¯¹å€¼
- Ranking Loss å¼ºåˆ¶æ¨¡å‹å­¦ä¹ ç›¸å¯¹è´¨é‡å…³ç³»
- å¯¹å¤§æ¨¡å‹æ›´é‡è¦ï¼ˆæœ‰è¶³å¤Ÿå®¹é‡åŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªç›®æ ‡ï¼‰
- Alpha=0.5 è¾¾åˆ°æœ€ä½³å¹³è¡¡

### æ ¸å¿ƒæ´å¯Ÿ 5ï¼šç®€å•æ–¹æ³•å¾€å¾€æ›´ç¨³å®š
- æ³¨æ„åŠ›èåˆç†è®ºä¸Šæ›´å…ˆè¿›ï¼Œä½†å®é™…ä¸ç¨³å®š
- ç®€å•æ‹¼æ¥è™½æœ´ç´ ï¼Œä½†ç¨³å®šå¯é 
- **å·¥ç¨‹ä¸Šï¼šå¯é æ€§ > å¾®å°æ€§èƒ½æå‡**

---

## ğŸ“š å…«ã€å‚è€ƒæ–‡çŒ®ï¼ˆåœ¨æŠ¥å‘Šä¸­å¼•ç”¨ï¼‰

1. **åŸå§‹ HyperIQA**:
   Su, S., et al. (2020). "Blindly Assess Image Quality in the Wild Guided by a Self-Adaptive Hyper Network." CVPR 2020.

2. **Swin Transformer**:
   Liu, Z., et al. (2021). "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows." ICCV 2021.

3. **Ranking Loss in IQA**:
   Ma, K., et al. (2017). "dipIQ: Blind Image Quality Assessment by Learning-to-Rank Discriminable Image Pairs." TIP 2017.

4. **Stochastic Depth**:
   Huang, G., et al. (2016). "Deep Networks with Stochastic Depth." ECCV 2016.

5. **Cosine Annealing**:
   Loshchilov, I., & Hutter, F. (2017). "SGDR: Stochastic Gradient Descent with Warm Restarts." ICLR 2017.

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: December 20, 2025  
**çŠ¶æ€**: å®Œæ•´ç‰ˆï¼Œå¯ç›´æ¥ç”¨äºæŠ¥å‘Šæ’°å†™

---

## é™„å½•ï¼šå¿«é€Ÿå‘½ä»¤å‚è€ƒ

### æœ€ä½³æ¨¡å‹è®­ç»ƒ
```bash
python train_swin.py --dataset koniq-10k --model_size base --batch_size 32 --epochs 30 --patience 7 --train_patch_num 20 --test_patch_num 20 --ranking_loss_alpha 0.5 --ranking_loss_margin 0.1 --lr 5e-6 --weight_decay 2e-4 --drop_path_rate 0.3 --dropout_rate 0.4 --lr_scheduler cosine --test_random_crop --no_spaq
```

### å¤æ‚åº¦åˆ†æ
```bash
python complexity/quick_test.py
```

### è·¨æ•°æ®é›†æµ‹è¯•
```bash
bash run_cross_dataset_test.sh
```

### æ¶ˆèå®éªŒï¼ˆAlphaï¼‰
```bash
# Alpha=0
python train_swin.py --model_size base --ranking_loss_alpha 0 [å…¶ä»–å‚æ•°åŒä¸Š]

# Alpha=0.5ï¼ˆæœ€ä¼˜ï¼‰
python train_swin.py --model_size base --ranking_loss_alpha 0.5 [å…¶ä»–å‚æ•°åŒä¸Š]
```

