## ResNet-50 éª¨å¹²ç½‘ç»œ
python train_test_IQA.py   --dataset koniq-10k   --epochs 10   --train_test_num 2   --batch_size 96   --train_patch_num 20   --test_patch_num 20

æŒ‡æ ‡	ä½ çš„ç»“æœ (Epoch 1)	è®ºæ–‡	å¯¹æ¯”
SRCC	0.9009	0.906	âœ… è¶…å‡º 0.5%
PLCC	0.9170	0.917	âœ… æŒå¹³

---

## Swin Transformer Tiny éª¨å¹²ç½‘ç»œ
python train_swin.py --dataset koniq-10k --epochs 10 --train_test_num 1 --batch_size 96 --train_patch_num 20 --test_patch_num 20

æŒ‡æ ‡	æœ€ä½³ç»“æœ (Epoch 2)	è®ºæ–‡	å¯¹æ¯”
SRCC	0.9154	0.906	âœ… è¶…å‡º 1.04%
PLCC	0.9298	0.917	âœ… è¶…å‡º 1.40%

è®­ç»ƒè¶‹åŠ¿ï¼š
- Epoch 1: SRCC 0.9138, PLCC 0.9286
- Epoch 2: SRCC 0.9154, PLCC 0.9298 (æœ€ä½³) â­
- Epoch 3-10: æµ‹è¯•æŒ‡æ ‡åœ¨ 0.9072-0.9146 ä¹‹é—´æ³¢åŠ¨
- è®­ç»ƒé›†æŒ‡æ ‡æŒç»­ä¸Šå‡ (0.8673 â†’ 0.9884)ï¼Œå­˜åœ¨è½»å¾®è¿‡æ‹Ÿåˆ

---

## Swin Transformer Tiny + Ranking Loss (alpha=0.5)
python train_swin.py --dataset koniq-10k --epochs 10 --train_test_num 1 --batch_size 96 --train_patch_num 20 --test_patch_num 20 --ranking_loss_alpha 0.5 --ranking_loss_margin 0.1

æŒ‡æ ‡	æœ€ä½³ç»“æœ (Epoch 1)	è®ºæ–‡	å¯¹æ¯”
SRCC	0.9178	0.906	âœ… è¶…å‡º 1.30%
PLCC	0.9326	0.917	âœ… è¶…å‡º 1.70%

è®­ç»ƒè¯¦æƒ…ï¼š
- Epoch 1: Train_Loss: 5.578 (L1: 5.208, Rank: 0.741), Train_SRCC: 0.8696, **Test_SRCC: 0.9178, Test_PLCC: 0.9326** (æœ€ä½³) â­
- Epoch 2: Train_Loss: 3.455 (L1: 3.286, Rank: 0.338), Train_SRCC: 0.9473, Test_SRCC: 0.9162, Test_PLCC: 0.9314

---

## Swin Transformer Tiny + Ranking Loss (alpha=0.3)
python train_swin.py --dataset koniq-10k --epochs 10 --train_test_num 1 --batch_size 96 --train_patch_num 20 --test_patch_num 20 --ranking_loss_alpha 0.3 --ranking_loss_margin 0.1

æŒ‡æ ‡	æœ€ä½³ç»“æœ (Epoch 1)	è®ºæ–‡	å¯¹æ¯”
SRCC	0.9206	0.906	âœ… è¶…å‡º 1.61%
PLCC	0.9334	0.917	âœ… è¶…å‡º 1.79%

è®­ç»ƒè¯¦æƒ…ï¼š
- Epoch 1: Train_Loss: 5.409 (L1: 5.182, Rank: 0.756), Train_SRCC: 0.8686, **Test_SRCC: 0.9206, Test_PLCC: 0.9334** (æœ€ä½³) â­

---

## Swin Transformer Tiny + Ranking Loss (alpha=1.0)
python train_swin.py --dataset koniq-10k --epochs 10 --train_test_num 1 --batch_size 96 --train_patch_num 20 --test_patch_num 20 --ranking_loss_alpha 1.0 --ranking_loss_margin 0.1

æŒ‡æ ‡	æœ€ä½³ç»“æœ (Epoch 1)	è®ºæ–‡	å¯¹æ¯”
SRCC	0.9188	0.906	âœ… è¶…å‡º 1.41%
PLCC	0.9326	0.917	âœ… è¶…å‡º 1.70%

è®­ç»ƒè¯¦æƒ…ï¼š
- Epoch 1: Train_Loss: 5.937 (L1: 5.233, Rank: 0.704), Train_SRCC: 0.8684, **Test_SRCC: 0.9188, Test_PLCC: 0.9326** (æœ€ä½³) â­
- Epoch 2: Train_Loss: 3.570 (L1: 3.243, Rank: 0.327), Train_SRCC: 0.9489, Test_SRCC: 0.9139, Test_PLCC: 0.9265
- Epoch 3-10: æµ‹è¯•æŒ‡æ ‡æŒç»­ä¸‹é™ï¼Œä» 0.9162 â†’ 0.9107 (SRCC), 0.9257 â†’ 0.9212 (PLCC)
- è®­ç»ƒé›† SRCC æŒç»­ä¸Šå‡ (0.8684 â†’ 0.9892)ï¼Œå­˜åœ¨æ˜æ˜¾è¿‡æ‹Ÿåˆ

---

## Swin Transformer Tiny + Ranking Loss (alpha=0.3) + Multi-Scale Feature Fusion
python train_swin.py --dataset koniq-10k --epochs 10 --train_test_num 1 --batch_size 96 --train_patch_num 20 --test_patch_num 20 --ranking_loss_alpha 0.3 --ranking_loss_margin 0.1

æŒ‡æ ‡	æœ€ä½³ç»“æœ (Epoch 1)	å•å°ºåº¦ç‰ˆæœ¬å¯¹æ¯”
SRCC	0.9184	0.9206	âŒ ä¸‹é™ 0.24%
PLCC	0.9329	0.9334	âŒ ä¸‹é™ 0.05%

è®­ç»ƒè¯¦æƒ…ï¼š
- Epoch 1: Train_Loss: 5.042 (L1: 4.848, Rank: 0.646), Train_SRCC: 0.8834, **Test_SRCC: 0.9184, Test_PLCC: 0.9329** (æœ€ä½³) â­
- Epoch 2: Train_Loss: 3.091 (L1: 2.999, Rank: 0.307), Train_SRCC: 0.9559, Test_SRCC: 0.9176, Test_PLCC: 0.9309
- Epoch 3-10: æµ‹è¯•æŒ‡æ ‡æ³¢åŠ¨åœ¨ 0.9111-0.9183 (SRCC), 0.9249-0.9329 (PLCC)
- è®­ç»ƒé›† SRCC æŒç»­ä¸Šå‡ (0.8834 â†’ 0.9889)ï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆ
- SPAQæµ‹è¯•: SRCC 0.8646, PLCC 0.8590 (Epoch 1)

**åˆ†æï¼šå¤šå°ºåº¦ç‰¹å¾èåˆæ•ˆæœä¸å¦‚å•å°ºåº¦ç‰ˆæœ¬**

å¯èƒ½åŸå› ï¼š
1. **æ¨¡å‹å¤æ‚åº¦å¢åŠ ä½†è®­ç»ƒç­–ç•¥æœªè°ƒæ•´**ï¼š
   - è¾“å…¥é€šé“æ•°ä» 768 â†’ 1440 (å¢åŠ äº† 87.5%)ï¼Œä½†å­¦ä¹ ç‡å’Œä¼˜åŒ–ç­–ç•¥æœªç›¸åº”è°ƒæ•´
   - å¤šå°ºåº¦ç‰¹å¾çš„ç®€å•concatenationå¯èƒ½ä¸æ˜¯æœ€ä¼˜èåˆæ–¹å¼

2. **ç¼ºå°‘è®­ç»ƒç¨³å®šæ€§ä¿®å¤**ï¼š
   - å½“å‰ç‰ˆæœ¬æœªåº”ç”¨ä¸‰ä¸ªå…³é”®è®­ç»ƒä¿®å¤ï¼ˆfilter iterator bug, backbone LR decay, optimizer state preservationï¼‰
   - è¿™äº›ä¿®å¤åœ¨å•å°ºåº¦ç‰ˆæœ¬ä¸­å¯èƒ½å·²è¢«åº”ç”¨æˆ–å½±å“è¾ƒå°ï¼Œä½†åœ¨æ›´å¤æ‚çš„å¤šå°ºåº¦æ¨¡å‹ä¸­å½±å“æ›´æ˜æ˜¾

3. **ç‰¹å¾èåˆæ–¹å¼å¯èƒ½ä¸å¤Ÿä¼˜åŒ–**ï¼š
   - å½“å‰ä½¿ç”¨ç®€å•çš„AdaptiveAvgPool2d + Concatenation
   - å¯èƒ½éœ€è¦æ›´sophisticatedçš„èåˆæœºåˆ¶ï¼ˆå¦‚æ³¨æ„åŠ›åŠ æƒã€ç‰¹å¾é€‰æ‹©ç­‰ï¼‰

4. **è®­ç»ƒè¿‡æ‹Ÿåˆè¶‹åŠ¿æ›´æ˜æ˜¾**ï¼š
   - è®­ç»ƒé›†SRCCä»0.8834ä¸Šå‡åˆ°0.9889ï¼ˆä¸Šå‡10.5%ï¼‰
   - æµ‹è¯•é›†æœ€ä½³ç»“æœå‡ºç°åœ¨Epoch 1ï¼Œåç»­epochæ€§èƒ½ä¸‹é™æˆ–æ³¢åŠ¨

æ”¹è¿›å»ºè®®ï¼š
- åº”ç”¨ä¸‰ä¸ªè®­ç»ƒç¨³å®šæ€§ä¿®å¤ï¼ˆfilterâ†’list, backbone LR decay, optimizer state preservationï¼‰
- è€ƒè™‘ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å¯¹å¤šå°ºåº¦ç‰¹å¾è¿›è¡ŒåŠ æƒèåˆ
- è°ƒæ•´å­¦ä¹ ç‡ç­–ç•¥ä»¥é€‚é…æ›´å¤§çš„æ¨¡å‹å®¹é‡
- å¢åŠ æ­£åˆ™åŒ–ï¼ˆdropout, weight decayï¼‰é˜²æ­¢è¿‡æ‹Ÿåˆ

---

## Swin Transformer Tiny + Multi-Scale + Anti-Overfitting (Phase 1-3)
python train_swin.py --dataset koniq-10k --epochs 30 --patience 7 --ranking_loss_alpha 0 --batch_size 96 --train_patch_num 20 --test_patch_num 20 --lr 1e-5 --weight_decay 1e-4 --drop_path_rate 0.2 --dropout_rate 0.3 --lr_scheduler cosine --test_random_crop --no_spaq

æŒ‡æ ‡	æœ€ä½³ç»“æœ (Epoch 2)	Baseline (Epoch 1)	å¯¹æ¯”
SRCC	0.9229	0.9195	âœ… è¶…å‡º 0.37%
PLCC	0.9361	0.9342	âœ… è¶…å‡º 0.20%

è®­ç»ƒè¯¦æƒ…ï¼š
- Epoch 1: Train_Loss: 6.164, Train_SRCC: 0.8231, Test_SRCC: 0.9198, Test_PLCC: 0.9318
- Epoch 2: Train_Loss: 4.150, Train_SRCC: 0.9175, **Test_SRCC: 0.9229, Test_PLCC: 0.9361** (æœ€ä½³) â­
- Epoch 3: Train_Loss: 3.648, Train_SRCC: 0.9349, Test_SRCC: 0.9213, Test_PLCC: 0.9336
- Epoch 4: Train_Loss: 3.309, Train_SRCC: 0.9454, Test_SRCC: 0.9197, Test_PLCC: 0.9315
- Epoch 5: Train_Loss: 3.058, Train_SRCC: 0.9531, Test_SRCC: 0.9198, Test_PLCC: 0.9306

**æŠ—è¿‡æ‹Ÿåˆç­–ç•¥é…ç½®**ï¼š

**Phase 1: æ­£åˆ™åŒ– (Regularization)**
1. **AdamW Optimizer**: ä½¿ç”¨ AdamW æ›¿ä»£ Adam (æ›´å¥½çš„ weight decay è§£è€¦)
2. **Weight Decay**: 1e-4
3. **Dropout**: 0.3 in HyperNet and TargetNet (åœ¨ FC å±‚ä¹‹é—´)
4. **Stochastic Depth**: drop_path_rate=0.2 in Swin Transformer (éšæœºä¸¢å¼ƒæ®‹å·®å—)

**Phase 2: æ•°æ®å¢å¼º (Data Augmentation)**
1. **RandomHorizontalFlip**: å·²å­˜åœ¨ (é•œåƒå¯¹ç§°ä¸å½±å“è´¨é‡)
2. **ColorJitter**: brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05 (ä¿å®ˆè®¾ç½®)

**Phase 3: è®­ç»ƒä¼˜åŒ– (Training Optimization)**
1. **Lower Learning Rate**: lr=1e-5 (backbone), lr=1e-4 (hypernet, 10Ã— ratio)
2. **Cosine Annealing LR**: å¹³æ»‘çš„å­¦ä¹ ç‡è¡°å‡
3. **Gradient Clipping**: max_norm=1.0 (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)

**å…³é”®æˆæœ**ï¼š

âœ… **è§£å†³äº†è¿‡æ‹Ÿåˆé—®é¢˜**ï¼š
- Baseline: æœ€ä½³æ€§èƒ½åœ¨ Epoch 1ï¼Œä¹‹åæŒç»­ä¸‹é™
- Anti-Overfitting: æœ€ä½³æ€§èƒ½åœ¨ Epoch 2ï¼Œè¯´æ˜æ¨¡å‹ä»åœ¨å­¦ä¹ 

âœ… **æ€§èƒ½æŒç»­æå‡**ï¼š
- Epoch 2 çš„æ€§èƒ½è¶…è¿‡ Epoch 1 (+0.31% SRCC, +0.43% PLCC)
- è®­ç»ƒ-æµ‹è¯•å·®è·æ›´åˆç† (Epoch 2: Train 0.9175 vs Test 0.9229)

âœ… **è®­ç»ƒé€Ÿåº¦å½±å“**ï¼š
- **ColorJitter å¯¼è‡´é€Ÿåº¦ä¸‹é™ 3Ã—** (6.25 batch/s â†’ 2.17 batch/s)
- CPU å¯†é›†å‹çš„é¢œè‰²å˜æ¢æ˜¯ç“¶é¢ˆ
- åç»­ç§»é™¤ ColorJitterï¼Œä¿ç•™ Dropout + StochasticDepth æ­£åˆ™åŒ–

**æ€§èƒ½åˆ†æ**ï¼š

å¯¹æ¯” Baseline (æ— æ­£åˆ™åŒ–ï¼ŒEpoch 1 æœ€ä½³):
- Baseline Epoch 1: SRCC 0.9195, PLCC 0.9342
- Anti-Overfitting Epoch 2: SRCC 0.9229 (+0.37%), PLCC 0.9361 (+0.20%)

è®­ç»ƒç¨³å®šæ€§æ”¹å–„ï¼š
- Train-Test Gap (Epoch 2): 0.9175 (train) vs 0.9229 (test) = -0.0054
- Baseline (Epoch 4): 0.9747 (train) vs 0.9174 (test) = +0.0573 (ä¸¥é‡è¿‡æ‹Ÿåˆ)

**ç»éªŒæ€»ç»“**ï¼š

1. **Dropout + Stochastic Depth éå¸¸æœ‰æ•ˆ**ï¼š
   - åœ¨ FC å±‚å’Œ Transformer å—ä¸­åŠ å…¥éšæœºæ€§
   - å¼ºåˆ¶æ¨¡å‹å­¦ä¹ å†—ä½™è¡¨ç¤ºï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

2. **Weight Decay é…åˆ AdamW**ï¼š
   - è§£è€¦ weight decay å’Œæ¢¯åº¦æ›´æ–°
   - å¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ (Swin-T) å°¤å…¶é‡è¦

3. **ColorJitter æƒè¡¡**ï¼š
   - ç†è®ºä¸Šæœ‰åŠ©äºæ­£åˆ™åŒ–ï¼Œä½†é€Ÿåº¦ä»£ä»·å¤ªå¤§ (3Ã—)
   - Dropout + StochasticDepth å·²æä¾›è¶³å¤Ÿæ­£åˆ™åŒ–
   - **å»ºè®®ï¼šåœ¨è®¡ç®—èµ„æºå……è¶³æ—¶ä½¿ç”¨ï¼Œå¦åˆ™çœç•¥**

4. **å­¦ä¹ ç‡è°ƒæ•´è‡³å…³é‡è¦**ï¼š
   - é™ä½åˆå§‹å­¦ä¹ ç‡ (1e-5) è®©è®­ç»ƒæ›´å¹³ç¨³
   - Cosine Annealing é¿å… Step Decay çš„çªå˜

**åç»­ä¼˜åŒ–æ–¹å‘**ï¼š

1. âœ… ç§»é™¤ ColorJitter (ä¿æŒè®­ç»ƒé€Ÿåº¦)
2. â³ æµ‹è¯• Dropout + StochasticDepth only ç‰ˆæœ¬
3. â³ å¯¹æ¯”ä¸åŒ weight_decay å€¼ (5e-5, 1e-4, 5e-4)
4. â³ å°è¯•æ›´æ¿€è¿›çš„ Dropout (0.4-0.5)

---

## é…ç½® 1: Swin + Multi-Scale + Anti-Overfitting (ColorJitter æ¢å¤)
python train_swin.py --dataset koniq-10k --epochs 30 --patience 7 --ranking_loss_alpha 0 --batch_size 96 --train_patch_num 20 --test_patch_num 20 --lr 1e-5 --weight_decay 1e-4 --drop_path_rate 0.2 --dropout_rate 0.3 --lr_scheduler cosine --test_random_crop --no_spaq

**é…ç½®è¯´æ˜**ï¼šæ¢å¤ ColorJitter (brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05)

| Metric | Epoch 1 | Epoch 2 | Epoch 3 | Epoch 4 | Best |
|---|---|---|---|---|---|
| Train_Loss | 6.275 | 4.300 | 3.854 | 3.508 | - |
| Train_SRCC | 0.8188 | 0.9121 | 0.9282 | 0.9392 | - |
| Test_SRCC | 0.9219 | **0.9235** | 0.9236 | 0.9208 | **0.9236** (Epoch 3) |
| Test_PLCC | 0.9336 | **0.9371** | 0.9368 | 0.9338 | **0.9371** (Epoch 2) |

**æ€§èƒ½å¯¹æ¯”**ï¼š

| é…ç½® | Best SRCC | Best PLCC | å¯¹æ¯” Baseline |
|---|---|---|---|
| Baseline (æ— æ­£åˆ™åŒ–) | 0.9195 (E1) | 0.9342 (E1) | - |
| Phase 1-3 (æ—  ColorJitter) | 0.9207 (E2) | 0.9348 (E2) | +0.13%, +0.06% |
| **é…ç½® 1 (å« ColorJitter)** | **0.9236 (E3)** | **0.9371 (E2)** | **+0.45%, +0.31%** âœ… |

**å…³é”®å‘ç°**ï¼š

âœ… **ColorJitter çš„ä»·å€¼è¢«è¯å®**ï¼š
- ç›¸æ¯”æ—  ColorJitter ç‰ˆæœ¬ï¼ŒSRCC æå‡ +0.29%ï¼ŒPLCC æå‡ +0.23%
- è®­ç»ƒé€Ÿåº¦ä»£ä»· (3Ã—) æ˜¯å€¼å¾—çš„ï¼Œæ¢æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡

âœ… **è®­ç»ƒæ›´åŠ ç¨³å®š**ï¼š
- Epoch 2-3 æ€§èƒ½æŒç»­ä¸Šå‡ (0.9235 â†’ 0.9236 SRCC)
- Epoch 4 æ‰å¼€å§‹ä¸‹é™ï¼Œè¯´æ˜æ­£åˆ™åŒ–æ•ˆæœè‰¯å¥½

âœ… **è¶…è¶Šæ‰€æœ‰ä¹‹å‰çš„é…ç½®**ï¼š
- æ¯” Baseline æå‡ +0.45% SRCC
- æ¯”æ—  ColorJitter ç‰ˆæœ¬æå‡ +0.29% SRCC
- æ¥è¿‘ SOTA (MANIQA: 0.920, å·®è·ä»… 0.4%)

**è®­ç»ƒé€Ÿåº¦**ï¼š
- ~2.15 batch/s (ColorJitter CPU ç“¶é¢ˆ)
- å• epoch è€—æ—¶çº¦ 11-12 åˆ†é’Ÿ

**ç»“è®º**ï¼š
- ColorJitter å¯¹æ³›åŒ–èƒ½åŠ›çš„æå‡éå¸¸æ˜æ˜¾
- å»ºè®®åœ¨æœ€ç»ˆé…ç½®ä¸­ä¿ç•™ ColorJitter
- âš ï¸ **Kornia GPU åŠ é€Ÿå¤±è´¥**ï¼ˆè§ä¸‹æ–¹ï¼‰ï¼šè™½ç„¶é€Ÿåº¦å¿«ï¼Œä½†æ€§èƒ½å¤§å¹…ä¸‹é™

---

## âŒ Kornia GPU ColorJitter åŠ é€Ÿå°è¯•ï¼ˆå¤±è´¥ï¼‰
å°è¯•å°† CPU ColorJitter è¿ç§»åˆ° GPU (Kornia) ä»¥æé€Ÿ 10-20x

æŒ‡æ ‡	Kornia GPU	Config 1 (CPU)	å¯¹æ¯”
æœ€ä½³ SRCC	0.8283 (Epoch 10)	0.9236	âŒ ä¸‹é™ 9.5%
æœ€ä½³ PLCC	0.8523 (Epoch 10)	0.9353	âŒ ä¸‹é™ 8.3%
è®­ç»ƒé€Ÿåº¦	~4-5 min/epoch	~11-12 min/epoch	âœ… å¿« 2-3x

è®­ç»ƒè¯¦æƒ…ï¼š
- Epoch 1: Train_SRCC: 0.7333, Test_SRCC: 0.7772, Test_PLCC: 0.8118
- Epoch 2: Train_SRCC: 0.8511, Test_SRCC: 0.7949, Test_PLCC: 0.8263
- Epoch 3: Train_SRCC: 0.8823, Test_SRCC: 0.8087, Test_PLCC: 0.8360
- ...
- Epoch 10: Train_SRCC: 0.9510, Test_SRCC: 0.8283, Test_PLCC: 0.8523 â­ æœ€ä½³
- æ€§èƒ½æŒç»­ä½äº Config 1ï¼Œè®­ç»ƒé›†è¿‡æ‹Ÿåˆä¸¥é‡ï¼ˆTrain 0.95 vs Test 0.82ï¼‰

**é—®é¢˜æ ¹æºï¼šColorJitter åº”ç”¨é¡ºåºé”™è¯¯** ğŸ›

CPU ç‰ˆæœ¬ï¼ˆæ­£ç¡®ï¼‰ï¼š
1. `ToTensor()` â†’ [0, 1]
2. `ColorJitter()` â†’ åœ¨ [0,1] èŒƒå›´å†…å¢å¼º âœ…
3. `Normalize()` â†’ å½’ä¸€åŒ–åˆ° mean/std

Kornia GPU ç‰ˆæœ¬ï¼ˆé”™è¯¯ï¼‰ï¼š
1. `ToTensor()` â†’ [0, 1]
2. `Normalize()` â†’ å½’ä¸€åŒ–åˆ° mean/std
3. `Kornia ColorJitter()` â†’ **åœ¨å½’ä¸€åŒ–æ•°æ®ä¸Šå¢å¼º** âŒ

**é”™è¯¯åˆ†æ**ï¼š
- ColorJitter çš„å‚æ•°ï¼ˆbrightness, contrast ç­‰ï¼‰æ˜¯ä¸º [0,1] èŒƒå›´è®¾è®¡çš„
- åœ¨å½’ä¸€åŒ–åçš„æ•°æ®ï¼ˆmean=0, std=1 é™„è¿‘ï¼‰ä¸Šåº”ç”¨è¿™äº›å‚æ•°ä¼šäº§ç”Ÿé”™è¯¯çš„å¢å¼ºæ•ˆæœ
- å¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°é”™è¯¯çš„ç‰¹å¾åˆ†å¸ƒï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™

**ä¿®å¤æ–¹æ¡ˆ**ï¼ˆæœªå®ç°ï¼‰ï¼š
1. åœ¨ Normalize ä¹‹å‰åº”ç”¨ Kornia ColorJitter
2. ä½†éœ€è¦é‡å†™ data_loaderï¼Œå¤æ‚åº¦é«˜
3. æˆ–è€…ä½¿ç”¨ kornia.enhance.normalize çš„é€†æ“ä½œï¼Œä½†ä¸å€¼å¾—

**æœ€ç»ˆå†³å®š**ï¼š
- âŒ **æ”¾å¼ƒ Kornia GPU åŠ é€Ÿ**
- âœ… **ä¿æŒ CPU ColorJitter**ï¼ˆè™½æ…¢ä½†æœ‰æ•ˆï¼‰
- ğŸ“ **æ•™è®­**ï¼šæ•°æ®å¢å¼ºçš„é¡ºåºå¾ˆé‡è¦ï¼å¿…é¡»åœ¨æ­£ç¡®çš„æ•°å€¼èŒƒå›´å†…åº”ç”¨
- ğŸ¯ **Premature optimization is the root of all evil**ï¼ˆè¿‡æ—©ä¼˜åŒ–æ˜¯ä¸‡æ¶ä¹‹æºï¼‰