#!/bin/bash

################################################################################
# QualiCLIP Pre-training + Fine-tuning Auto Pipeline
# 
# åŠŸèƒ½ï¼š
# 1. ç›‘æŽ§é¢„è®­ç»ƒè¿›ç¨‹ï¼Œç­‰å¾…å®Œæˆ
# 2. éªŒè¯é¢„è®­ç»ƒcheckpoint
# 3. è‡ªåŠ¨å¯åŠ¨å¾®è°ƒè®­ç»ƒï¼ˆä½¿ç”¨ä¼˜åŒ–çš„å­¦ä¹ çŽ‡ï¼‰
################################################################################

set -e  # Exit on error

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}========================================${NC}"
echo -e "${BLUE}QualiCLIP Auto Training Pipeline${NC}"
echo -e "${BLUE}========================================${NC}"
echo ""

# é…ç½®
PRETRAIN_LOG="/root/Perceptual-IQA-CS3324/logs/qualiclip_pretrain_run.log"
CHECKPOINT_DIR="/root/Perceptual-IQA-CS3324/checkpoints"
FINETUNE_LOG="/root/Perceptual-IQA-CS3324/logs/qualiclip_finetune_run.log"

################################################################################
# Step 1: ç­‰å¾…é¢„è®­ç»ƒå®Œæˆ
################################################################################

echo -e "${YELLOW}[Step 1] Waiting for pre-training to complete...${NC}"
echo ""

# æŸ¥æ‰¾é¢„è®­ç»ƒè¿›ç¨‹
PRETRAIN_PID=$(ps aux | grep "[p]retrain_qualiclip.py" | awk '{print $2}')

if [ -z "$PRETRAIN_PID" ]; then
    echo -e "${YELLOW}âš  No pre-training process found. Assuming already completed.${NC}"
else
    echo -e "Found pre-training process: PID ${PRETRAIN_PID}"
    echo "Monitoring progress..."
    echo ""
    
    # ç›‘æŽ§è¿›ç¨‹
    while kill -0 $PRETRAIN_PID 2>/dev/null; do
        # æ˜¾ç¤ºæœ€æ–°è¿›åº¦
        if [ -f "$PRETRAIN_LOG" ]; then
            LAST_LINE=$(tail -1 "$PRETRAIN_LOG" | grep -oP 'Epoch \d+/\d+' || echo "Training...")
            echo -ne "\r  Current: $LAST_LINE   "
        fi
        sleep 10
    done
    echo ""
    echo -e "${GREEN}âœ“ Pre-training process completed!${NC}"
fi

sleep 5

################################################################################
# Step 2: éªŒè¯é¢„è®­ç»ƒcheckpoint
################################################################################

echo ""
echo -e "${YELLOW}[Step 2] Validating pre-training checkpoint...${NC}"
echo ""

# æŸ¥æ‰¾æœ€æ–°çš„é¢„è®­ç»ƒcheckpointç›®å½•
PRETRAIN_DIR=$(ls -td ${CHECKPOINT_DIR}/qualiclip_pretrain_* 2>/dev/null | head -1)

if [ -z "$PRETRAIN_DIR" ]; then
    echo -e "${RED}âœ— Error: No pre-training checkpoint directory found!${NC}"
    exit 1
fi

echo "Found checkpoint directory: $PRETRAIN_DIR"

# æŸ¥æ‰¾epoch10çš„æƒé‡æ–‡ä»¶
PRETRAIN_WEIGHTS="$PRETRAIN_DIR/swin_base_epoch10.pkl"

if [ ! -f "$PRETRAIN_WEIGHTS" ]; then
    # å¦‚æžœæ²¡æœ‰epoch10ï¼Œå°è¯•æ‰¾æœ€æ–°çš„
    PRETRAIN_WEIGHTS=$(ls -t "$PRETRAIN_DIR"/swin_base_epoch*.pkl 2>/dev/null | head -1)
    if [ -z "$PRETRAIN_WEIGHTS" ]; then
        echo -e "${RED}âœ— Error: No checkpoint file found in $PRETRAIN_DIR${NC}"
        exit 1
    fi
    echo -e "${YELLOW}âš  epoch10 not found, using: $(basename $PRETRAIN_WEIGHTS)${NC}"
fi

echo -e "${GREEN}âœ“ Found pre-trained weights: $PRETRAIN_WEIGHTS${NC}"
FILE_SIZE=$(du -h "$PRETRAIN_WEIGHTS" | cut -f1)
echo "  File size: $FILE_SIZE"
echo ""

################################################################################
# Step 3: å¯åŠ¨å¾®è°ƒè®­ç»ƒ
################################################################################

echo -e "${YELLOW}[Step 3] Starting fine-tuning with QualiCLIP pre-trained encoder...${NC}"
echo ""

# è®­ç»ƒå‚æ•°ï¼ˆåŸºäºŽç”¨æˆ·ç»éªŒä¼˜åŒ–ï¼‰
DATASET="koniq-10k"
MODEL_SIZE="base"
BATCH_SIZE=8
EPOCHS=50
LR_MAIN=1e-6              # HyperNetå­¦ä¹ çŽ‡ï¼ˆç”¨æˆ·è¯´1e-6æ•ˆæžœå¥½ï¼‰
LR_ENCODER=5e-7           # Encoderå­¦ä¹ çŽ‡ï¼ˆæ›´å°ï¼Œä¿æŠ¤é¢„è®­ç»ƒç‰¹å¾ï¼‰

echo "Training Configuration:"
echo "  Dataset: $DATASET"
echo "  Model Size: $MODEL_SIZE"
echo "  Batch Size: $BATCH_SIZE"
echo "  Total Epochs: $EPOCHS"
echo "  HyperNet LR: $LR_MAIN"
echo "  Encoder LR: $LR_ENCODER"
echo "  Pre-trained Weights: $PRETRAIN_WEIGHTS"
echo ""
echo -e "${BLUE}Starting training in 5 seconds...${NC}"
sleep 5

# è¿›å…¥é¡¹ç›®ç›®å½•
cd /root/Perceptual-IQA-CS3324

# å¯åŠ¨å¾®è°ƒè®­ç»ƒ
echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}Starting Fine-tuning...${NC}"
echo -e "${GREEN}========================================${NC}"
echo ""

python train_swin.py \
    --dataset "$DATASET" \
    --model_size "$MODEL_SIZE" \
    --batch_size $BATCH_SIZE \
    --epochs $EPOCHS \
    --lr $LR_MAIN \
    --pretrained_encoder "$PRETRAIN_WEIGHTS" \
    --lr_encoder_pretrained $LR_ENCODER \
    --no_color_jitter \
    --no_spaq \
    2>&1 | tee "$FINETUNE_LOG"

# æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸ
if [ ${PIPESTATUS[0]} -eq 0 ]; then
    echo ""
    echo -e "${GREEN}========================================${NC}"
    echo -e "${GREEN}âœ“ Fine-tuning completed successfully!${NC}"
    echo -e "${GREEN}========================================${NC}"
    echo ""
    echo "Model saved to: checkpoints/"
    echo "Training log: $FINETUNE_LOG"
else
    echo ""
    echo -e "${RED}========================================${NC}"
    echo -e "${RED}âœ— Fine-tuning failed!${NC}"
    echo -e "${RED}========================================${NC}"
    echo ""
    echo "Check log file: $FINETUNE_LOG"
    exit 1
fi

################################################################################
# Step 4: è·¨æ•°æ®é›†æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
################################################################################

echo ""
echo -e "${YELLOW}[Step 4] Would you like to run cross-dataset evaluation?${NC}"
echo "Press Enter to skip, or type 'yes' to run tests:"
read -t 30 RUN_TESTS || RUN_TESTS=""

if [ "$RUN_TESTS" = "yes" ]; then
    echo ""
    echo -e "${BLUE}Running cross-dataset evaluation...${NC}"
    
    # Find the most recent checkpoint directory
    BEST_MODEL=$(find checkpoints/ -name "best_model.pkl" -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d" ")
    
    if [ -f "$BEST_MODEL" ]; then
        python test_swin.py \
            --model_path "$BEST_MODEL" \
            --test_datasets spaq kadid agiqa \
            2>&1 | tee logs/qualiclip_cross_dataset_test.log
        
        echo -e "${GREEN}âœ“ Cross-dataset evaluation completed!${NC}"
    else
        echo -e "${RED}âœ— Best model not found in checkpoints/${NC}"
    fi
else
    echo "Skipping cross-dataset evaluation."
fi

################################################################################
# Summary
################################################################################

echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}Pipeline Completed!${NC}"
echo -e "${GREEN}========================================${NC}"
echo ""
echo "Summary:"
echo "  Pre-trained weights: $PRETRAIN_WEIGHTS"
echo "  Fine-tuned model: checkpoints/"
echo "  Training log: $FINETUNE_LOG"
echo ""
echo "Next steps:"
echo "  1. Check training metrics in: $FINETUNE_LOG"
echo "  2. Evaluate on test sets"
echo "  3. Compare with baseline results"
echo ""
echo -e "${GREEN}Done! ðŸŽ‰${NC}"

