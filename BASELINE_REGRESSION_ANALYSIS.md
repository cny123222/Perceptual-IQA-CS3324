# Baseline æ€§èƒ½å›é€€åˆ†æ

## ğŸ”´ é—®é¢˜æè¿°

ResNet-50 baseline æ— æ³•å¤ç°ä¹‹å‰çš„ç»“æœï¼š

| æŒ‡æ ‡ | ä¹‹å‰æˆåŠŸ (12æœˆ15æ—¥) | å½“å‰å®éªŒ (12æœˆ20æ—¥) | å·®è· |
|------|---------------------|---------------------|------|
| **SRCC** | **0.9005-0.9009** | **0.8854** | **-0.0155 (-1.72%)** |
| **PLCC** | **0.9187-0.9191** | **0.9068** | **-0.0119 (-1.30%)** |

**è¿™ä¸ªå·®è·å¤ªå¤§äº†ï¼è¶…å‡ºäº†æ­£å¸¸çš„éšæœºæ³¢åŠ¨èŒƒå›´ã€‚**

---

## ğŸ” æ ¹å› åˆ†æ

###  é—®é¢˜ï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨å˜äº†ï¼

**å‘ç°çš„å…³é”®å·®å¼‚**ï¼š

####  ä¹‹å‰æˆåŠŸçš„å®éªŒï¼ˆæ¨æµ‹ï¼‰
ä½¿ç”¨åŸå§‹ HyperIQA çš„ **Step Decay** ç­–ç•¥ï¼š
```python
# æ¯ 6 ä¸ª epoch å­¦ä¹ ç‡é™¤ä»¥ 10
hypernet_lr = lr * lr_ratio / pow(10, (epoch // 6))
backbone_lr = lr  # ä¿æŒä¸å˜
```

**è®­ç»ƒåŠ¨æ€**ï¼ˆ10 epochsï¼‰ï¼š
- Epoch 1-6: HyperNet LR = 0.0002, Backbone LR = 0.00002
- Epoch 7-10: HyperNet LR = 0.00002, Backbone LR = 0.00002

####  å½“å‰å®éªŒ
ä½¿ç”¨ **Cosine Annealing** ç­–ç•¥ï¼š
```python
CosineAnnealingLR(T_max=10, eta_min=1e-6)
```

**è®­ç»ƒåŠ¨æ€**ï¼ˆ10 epochsï¼‰ï¼š
- Epoch 1: HyperNet LR â‰ˆ 0.000195
- Epoch 2: HyperNet LR â‰ˆ 0.000181
- Epoch 3: HyperNet LR â‰ˆ 0.000159
- ...
- Epoch 10: HyperNet LR â‰ˆ 0.000001

**å…³é”®å·®å¼‚**ï¼š
- Step decay: å‰6ä¸ªepochä¿æŒé«˜å­¦ä¹ ç‡ï¼Œå……åˆ†å­¦ä¹ 
- Cosine: å­¦ä¹ ç‡æŒç»­ä¸‹é™ï¼Œåœ¨æ—©æœŸå°±é™ä½äº†å­¦ä¹ èƒ½åŠ›

---

## ğŸ“Š å®éªŒè¯æ®

### ä¹‹å‰æˆåŠŸçš„å®éªŒï¼ˆ12æœˆ15æ—¥ï¼‰

**æ—¥å¿—**: `logs/resnet50_20251215_184253.log`
```
Epoch 1: SRCC 0.9005, PLCC 0.9187
```

**æ—¥å¿—**: `logs/resnet50_20251215_191130.log`
```
Epoch 1: SRCC 0.9000, PLCC 0.9191
Epoch 2: SRCC 0.8994, PLCC 0.9157
```

### å½“å‰å®éªŒï¼ˆ12æœˆ20æ—¥ï¼‰

**æ—¥å¿—**: `logs/resnet50_baseline_20251220_233008.log`
```
Epoch 1: SRCC 0.8817, PLCC 0.9047
Epoch 2: SRCC 0.8854, PLCC 0.9068
Epoch 3: SRCC 0.8838, PLCC 0.9031
```

**å·®è·åˆ†æ**ï¼š
- Epoch 1: 0.9005 â†’ 0.8817 (**-0.0188, -2.09%**)
- è¿™è¿œè¶…æ­£å¸¸çš„ Â±0.003 æ³¢åŠ¨èŒƒå›´ï¼

---

## ğŸ¯ é—®é¢˜æ ¹æº

### ä»£ç å˜æ›´è®°å½•

åœ¨ `HyerIQASolver.py` ä¸­ï¼š

```python
# é»˜è®¤è®¾ç½®ï¼ˆå½“å‰ï¼‰
self.use_lr_scheduler = getattr(config, 'use_lr_scheduler', True)  # Enable by default
self.lr_scheduler_type = getattr(config, 'lr_scheduler_type', 'cosine')  # 'cosine' or 'step'
```

**é—®é¢˜**ï¼š
1. âœ… `use_lr_scheduler=True` æ˜¯å¯¹çš„
2. âŒ `lr_scheduler_type='cosine'` æ˜¯é”™çš„ï¼åº”è¯¥æ˜¯ `'step'`

### ä¸ºä»€ä¹ˆ Cosine å¯¹ baseline ä¸å¥½ï¼Ÿ

#### 1. è®­ç»ƒæ—¶é—´å¤ªçŸ­ï¼ˆ10 epochsï¼‰

Cosine Annealing è®¾è®¡ç”¨äº**é•¿æ—¶é—´è®­ç»ƒ**ï¼ˆ100+ epochsï¼‰ï¼š
- æ…¢æ…¢é™ä½å­¦ä¹ ç‡ï¼Œå……åˆ†æ¢ç´¢å‚æ•°ç©ºé—´
- åœ¨è®­ç»ƒåæœŸå¾®è°ƒ

ä½†å¯¹äº **10 epochs** çš„çŸ­è®­ç»ƒï¼š
- Cosine é™å¾—å¤ªå¿«
- Epoch 1-3 å°±å·²ç»é™ä½äº† 30-40%
- æ¨¡å‹æ²¡æœ‰å……åˆ†å­¦ä¹ 

#### 2. åŸå§‹ HyperIQA çš„è®¾è®¡

Step decay çš„è®¾è®¡ç†å¿µï¼š
- **å‰6ä¸ªepoch**ï¼šé«˜å­¦ä¹ ç‡ï¼Œå¿«é€Ÿæ”¶æ•›åˆ°å¥½çš„åŒºåŸŸ
- **å4ä¸ªepoch**ï¼šä½å­¦ä¹ ç‡ï¼Œå¾®è°ƒï¼ˆä½†å¯¹äº10 epochsé€šå¸¸ä¸éœ€è¦ï¼‰

å®é™…ä¸Šï¼ŒHyperIQA åœ¨ **Epoch 1-2 å°±è¾¾åˆ°æœ€ä½³**ï¼Œè¯´æ˜ï¼š
- é«˜å­¦ä¹ ç‡çš„å¿«é€Ÿæ”¶æ•›å¾ˆé‡è¦
- ä¸éœ€è¦å¤ªå¤šçš„å¾®è°ƒ

#### 3. å­¦ä¹ ç‡å¯¹æ¯”ï¼ˆ10 epochsï¼‰

| Epoch | Step Decay (HyperNet) | Cosine (HyperNet) | å·®å¼‚ |
|-------|----------------------|-------------------|------|
| 1 | 0.0002 | 0.000195 | -2.5% |
| 2 | 0.0002 | 0.000181 | **-9.5%** |
| 3 | 0.0002 | 0.000159 | **-20.5%** |
| 4 | 0.0002 | 0.000131 | **-34.5%** |
| 5 | 0.0002 | 0.000100 | **-50.0%** |
| 6 | 0.0002 | 0.000069 | **-65.5%** |
| 7 | 0.00002 | 0.000041 | +105% |
| 8 | 0.00002 | 0.000019 | -5% |
| 9 | 0.00002 | 0.000005 | -75% |
| 10 | 0.00002 | 0.000001 | -95% |

**å…³é”®è§‚å¯Ÿ**ï¼š
- Epoch 1-6: Cosine å­¦ä¹ ç‡**æŒç»­é™ä½**ï¼Œå½±å“å­¦ä¹ 
- Epoch 7-10: Step decay æ‰é™ä½ï¼Œä½†æ­¤æ—¶æ¨¡å‹å·²ç»æ”¶æ•›äº†

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šä½¿ç”¨åŸå§‹çš„ Step Decayï¼ˆæ¨èï¼‰ âœ…

ä¿®æ”¹è®­ç»ƒå‘½ä»¤ï¼Œæ·»åŠ  LR scheduler å‚æ•°ï¼š

```bash
python train_test_IQA.py \
  --dataset koniq-10k \
  --epochs 10 \
  --batch_size 96 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --lr 2e-5 \
  --weight_decay 5e-4 \
  --lr_scheduler step \        # ä½¿ç”¨ step decay
  --no_spaq
```

**æˆ–è€…**ï¼Œå¦‚æœæ²¡æœ‰ `--lr_scheduler` å‚æ•°ï¼Œéœ€è¦æ·»åŠ åˆ° `train_test_IQA.py`ï¼š

```python
parser.add_argument('--lr_scheduler', dest='lr_scheduler', 
                   type=str, default='step', 
                   choices=['step', 'cosine', 'none'],
                   help='Learning rate scheduler type')
```

### æ–¹æ¡ˆ Bï¼šç¦ç”¨ LR Schedulerï¼ˆæ¬¡ä¼˜ï¼‰ âš ï¸

ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ï¼š

```bash
python train_test_IQA.py \
  --dataset koniq-10k \
  --epochs 10 \
  --batch_size 96 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --no_lr_scheduler \          # ç¦ç”¨ scheduler
  --no_spaq
```

**é¢„æœŸç»“æœ**ï¼š
- å¯èƒ½è¾¾åˆ° 0.895-0.900 SRCC
- ä¸å¦‚ step decayï¼Œä½†æ¯” cosine å¥½

### æ–¹æ¡ˆ Cï¼šè°ƒæ•´ Cosine å‚æ•°ï¼ˆä¸æ¨èï¼‰ âŒ

ä½¿ç”¨æ›´å¤§çš„ `T_max` å’Œæ›´é«˜çš„ `eta_min`ï¼š

```python
# åœ¨ä»£ç ä¸­ä¿®æ”¹
CosineAnnealingLR(T_max=20, eta_min=5e-5)  # é™å¾—æ›´æ…¢
```

**é—®é¢˜**ï¼š
- éœ€è¦ä¿®æ”¹ä»£ç 
- ç ´åäº†åŸå§‹å®ç°çš„å®Œæ•´æ€§
- ä¸æ˜¯æ ‡å‡†åšæ³•

---

## ğŸ”§ ä»£ç ä¿®å¤

### 1. æ·»åŠ  LR Scheduler å‚æ•°åˆ° train_test_IQA.py

```python
# åœ¨ train_test_IQA.py çš„å‚æ•°è§£æéƒ¨åˆ†æ·»åŠ 
parser.add_argument('--lr_scheduler', dest='lr_scheduler_type', 
                   type=str, default='step',  # æ”¹å›é»˜è®¤ step
                   choices=['step', 'cosine', 'none'],
                   help='Learning rate scheduler type (default: step for original HyperIQA)')

parser.add_argument('--no_lr_scheduler', dest='use_lr_scheduler', 
                   action='store_false',
                   help='Disable learning rate scheduler')
```

### 2. æ›´æ–° HyerIQASolver.py çš„é»˜è®¤å€¼

```python
# å°†é»˜è®¤å€¼æ”¹å›åŸå§‹å®ç°
self.lr_scheduler_type = getattr(config, 'lr_scheduler_type', 'step')  # é»˜è®¤ stepï¼Œä¸æ˜¯ cosine
```

---

## ğŸ“š åŸå§‹ HyperIQA è®ºæ–‡çš„é…ç½®

æ ¹æ®åŸå§‹è®ºæ–‡å’Œä»£ç ï¼š

### è®­ç»ƒé…ç½®
- **Optimizer**: Adam
- **Learning Rate**: 2e-5 (backbone), 2e-4 (hypernetwork)
- **Weight Decay**: 5e-4
- **Batch Size**: 96
- **Epochs**: 10-15ï¼ˆé€šå¸¸åœ¨ epoch 1-2 è¾¾åˆ°æœ€ä½³ï¼‰
- **LR Scheduler**: **Step decay, divide by 10 every 6 epochs**
- **Patch Num**: train=20, test=25

### é¢„æœŸæ€§èƒ½ï¼ˆKonIQ-10kï¼‰
- **SRCC**: ~0.906 (è®ºæ–‡æŠ¥å‘Š)
- **PLCC**: ~0.917 (è®ºæ–‡æŠ¥å‘Š)
- **å®é™…å¤ç°**: 0.9005-0.9009 SRCCï¼ˆæ¥è¿‘è®ºæ–‡ï¼‰

---

## ğŸ¯ éªŒè¯æ­¥éª¤

### æ­¥éª¤ 1ï¼šæ£€æŸ¥å½“å‰ä»£ç çš„é»˜è®¤å€¼

```bash
grep "lr_scheduler_type.*default" HyerIQASolver.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```python
self.lr_scheduler_type = getattr(config, 'lr_scheduler_type', 'cosine')
```

**é—®é¢˜**ï¼šé»˜è®¤å€¼åº”è¯¥æ˜¯ `'step'`ï¼Œä¸æ˜¯ `'cosine'`ï¼

### æ­¥éª¤ 2ï¼šä¿®å¤å¹¶é‡æ–°è¿è¡Œ

ä¿®æ”¹ `HyerIQASolver.py` ç¬¬36è¡Œï¼š
```python
# ä¹‹å‰ï¼ˆé”™è¯¯ï¼‰
self.lr_scheduler_type = getattr(config, 'lr_scheduler_type', 'cosine')

# ä¿®æ”¹ä¸ºï¼ˆæ­£ç¡®ï¼‰
self.lr_scheduler_type = getattr(config, 'lr_scheduler_type', 'step')
```

### æ­¥éª¤ 3ï¼šé‡æ–°è¿è¡Œ baseline

```bash
python train_test_IQA.py \
  --dataset koniq-10k \
  --epochs 10 \
  --batch_size 96 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --no_spaq
```

**é¢„æœŸç»“æœ**ï¼š
- SRCC: 0.9000-0.9010
- PLCC: 0.9170-0.9190
- Epoch 1 å°±è¾¾åˆ°æœ€ä½³

---

## ğŸ“Š å¯¹æ¯”æ€»ç»“

| é…ç½® | SRCC | PLCC | é—®é¢˜ |
|------|------|------|------|
| **åŸå§‹ï¼ˆStep Decayï¼‰** | **0.9005-0.9009** | **0.9187-0.9191** | âœ… æ­£ç¡® |
| **å½“å‰ï¼ˆCosineï¼‰** | **0.8854** | **0.9068** | âŒ æ€§èƒ½ä¸‹é™ 1.7% |
| **é¢„æœŸï¼ˆä¿®å¤åï¼‰** | **~0.9005** | **~0.9185** | âœ… åº”è¯¥æ¢å¤ |

---

## ğŸ”¬ æ·±å…¥è§£é‡Šï¼šä¸ºä»€ä¹ˆ LR Scheduler è¿™ä¹ˆé‡è¦ï¼Ÿ

### 1. HyperIQA çš„å¿«é€Ÿæ”¶æ•›ç‰¹æ€§

HyperIQA çš„è®¾è®¡ç‰¹ç‚¹ï¼š
- **é¢„è®­ç»ƒçš„ ResNet-50**ï¼šå·²ç»æœ‰å¾ˆå¥½çš„ç‰¹å¾æå–èƒ½åŠ›
- **HyperNetwork å¾ˆå°**ï¼šåªéœ€è¦å­¦ä¹ å¦‚ä½•ç”Ÿæˆç›®æ ‡ç½‘ç»œçš„æƒé‡
- **æ•°æ®é›†ä¸å¤§**ï¼šKonIQ-10k åªæœ‰ 7046 å¼ è®­ç»ƒå›¾åƒ

å› æ­¤ï¼š
- âœ… **Epoch 1** å°±èƒ½è¾¾åˆ°å¾ˆå¥½çš„æ€§èƒ½ï¼ˆ~0.900ï¼‰
- âœ… **Epoch 1-2** è¾¾åˆ°æœ€ä½³
- âŒ ä¸éœ€è¦é•¿æ—¶é—´çš„å¾®è°ƒ

### 2. Step Decay çš„ä¼˜åŠ¿

**å‰6ä¸ªepochä¿æŒé«˜å­¦ä¹ ç‡**ï¼š
- å¿«é€Ÿæ‰¾åˆ°å¥½çš„å‚æ•°åŒºåŸŸ
- å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæƒé‡
- åœ¨å°‘æ•°epochå†…è¾¾åˆ°æœ€ä½³

**åç»­é™ä½å­¦ä¹ ç‡**ï¼š
- å¾®è°ƒï¼ˆä½†é€šå¸¸ä¸éœ€è¦ï¼Œå› ä¸ºå·²ç»æ”¶æ•›ï¼‰
- é˜²æ­¢éœ‡è¡

### 3. Cosine çš„åŠ£åŠ¿ï¼ˆå¯¹çŸ­è®­ç»ƒï¼‰

**å­¦ä¹ ç‡æŒç»­ä¸‹é™**ï¼š
- Epoch 1-3 å°±é™ä½ 20-30%
- é™åˆ¶äº†æ—©æœŸçš„å­¦ä¹ èƒ½åŠ›
- æ¨¡å‹æ²¡æœ‰å……åˆ†æ¢ç´¢å‚æ•°ç©ºé—´

**é€‚åˆé•¿è®­ç»ƒ**ï¼š
- 100+ epochs
- ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆéé¢„è®­ç»ƒï¼‰
- éœ€è¦æ…¢æ…¢æ”¶æ•›çš„åœºæ™¯

---

## âœ… ç»“è®º

1. âŒ **å½“å‰é—®é¢˜**ï¼šé»˜è®¤ LR scheduler ä» `step` å˜æˆäº† `cosine`
2. ğŸ“‰ **æ€§èƒ½å½±å“**ï¼šSRCC ä» 0.9005 ä¸‹é™åˆ° 0.8854 (-1.7%)
3. ğŸ”§ **ä¿®å¤æ–¹æ³•**ï¼šå°†é»˜è®¤å€¼æ”¹å› `step`
4. âœ… **é¢„æœŸç»“æœ**ï¼šåº”è¯¥èƒ½æ¢å¤åˆ° 0.9005 å·¦å³

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**åˆ›å»ºæ—¶é—´**: 2025-12-21  
**çŠ¶æ€**: é—®é¢˜å·²è¯Šæ–­ï¼Œç­‰å¾…ä¿®å¤éªŒè¯  
**ä¼˜å…ˆçº§**: ğŸ”´ HIGH - å½±å“æ‰€æœ‰ baseline å®éªŒ

