# ç½‘ç»œæ¶æ„æ”¹è¿›è¯¦ç»†æ€»ç»“

**æ–‡æ¡£ç›®çš„**: è¯¦ç»†è®°å½•ä»åŸå§‹HyperIQAåˆ°æˆ‘ä»¬æ”¹è¿›ç‰ˆæœ¬çš„æ‰€æœ‰ç½‘ç»œæ¶æ„å±‚é¢çš„ä¿®æ”¹  
**æœ€ç»ˆæ€§èƒ½**: SRCC **0.9378** (KonIQ-10k), ç›¸æ¯”åŸå§‹HyperIQA (0.907) æå‡ **+3.08%**  
**æ—¥æœŸ**: 2025-12-23

---

## ğŸ“Š æ”¹è¿›æ€»è§ˆ

| æŒ‡æ ‡ | åŸå§‹HyperIQA | æˆ‘ä»¬çš„æ–¹æ³• | ç»å¯¹æå‡ | ç›¸å¯¹æå‡ |
|------|-------------|-----------|---------|---------|
| **SRCC** | 0.907 | **0.9378** | +0.0308 | **+3.40%** |
| **PLCC** | ~0.918 | **0.9485** | +0.0305 | **+3.32%** |
| **å‚æ•°é‡** | ~25M | 88M | +63M | +252% |
| **FLOPs** | ~4G | 18.2G | +14.2G | +355% |

**æ ¸å¿ƒç†å¿µ**: ç”¨æ›´å¼ºå¤§çš„Vision Transformeræ›¿æ¢CNN backboneï¼Œå¹¶å¼•å…¥å¤šå°ºåº¦æ³¨æ„åŠ›èåˆæœºåˆ¶

---

## ğŸ—ï¸ æ¶æ„æ”¹è¿›è¯¦è§£

### æ”¹è¿› 1ï¸âƒ£: Backboneæ›¿æ¢ - ResNet50 â†’ Swin Transformer

#### åŸå§‹HyperIQAçš„Backbone

```python
# models.py (åŸå§‹ä»£ç )
class ResNetBackbone(nn.Module):
    def __init__(self, lda_out_channels, in_chn, block, layers, num_classes=1000):
        self.inplanes = 64
        # æ ‡å‡†ResNet50ç»“æ„
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # 4ä¸ªlayerï¼ˆstageï¼‰
        self.layer1 = self._make_layer(block, 64, layers[0])    # 256 channels
        self.layer2 = self._make_layer(block, 128, layers[1])   # 512 channels
        self.layer3 = self._make_layer(block, 256, layers[2])   # 1024 channels
        self.layer4 = self._make_layer(block, 512, layers[3])   # 2048 channels
        
        # Local Distortion Aware (LDA) æ¨¡å— - ä»å¤šä¸ªlayeræå–
        # ä½†æœ€ç»ˆåªç”¨layer4çš„2048ç»´ç‰¹å¾è¿›å…¥HyperNet
        self.lda1_pool = nn.Sequential(...)  # ä»layer1
        self.lda2_pool = nn.Sequential(...)  # ä»layer2
        self.lda3_pool = nn.Sequential(...)  # ä»layer3
        self.lda4_pool = nn.AvgPool2d(7, stride=7)  # ä»layer4
```

**ç‰¹ç‚¹**:
- **æ¶æ„**: CNNï¼ŒåŸºäºå·ç§¯çš„å±€éƒ¨æ„Ÿå—é‡
- **è¾“å‡º**: 
  - `hyper_in_feat`: [B, 2048, 7, 7] â†’ è¿›å…¥HyperNetç”ŸæˆåŠ¨æ€æƒé‡
  - `target_in_vec`: [B, 112] â†’ LDAæ¨¡å—å‹ç¼©çš„å¤šå°ºåº¦ç‰¹å¾ï¼Œè¿›å…¥TargetNet
- **ç‰¹å¾æå–**: è™½ç„¶ä»4ä¸ªlayeræå–äº†LDAç‰¹å¾ï¼Œä½†HyperNetåªç”¨äº†layer4çš„2048ç»´
- **å‚æ•°é‡**: ~25.6M (ResNet50æœ¬èº«)

---

#### æˆ‘ä»¬çš„Swin Transformer Backbone

```python
# models_swin.py (æˆ‘ä»¬çš„ä»£ç )
class SwinBackbone(nn.Module):
    def __init__(self, model_size='base', pretrained=True, 
                 drop_path_rate=0.2, multi_scale=True):
        super().__init__()
        
        # ä»timmåŠ è½½é¢„è®­ç»ƒçš„Swin Transformer
        model_name = f'swin_{model_size}_patch4_window7_224'
        self.swin = timm.create_model(
            model_name,
            pretrained=pretrained,
            features_only=True,  # è¾“å‡ºä¸­é—´å±‚ç‰¹å¾
            out_indices=(0, 1, 2, 3) if multi_scale else (3,),  # 4ä¸ªstage
            drop_path_rate=drop_path_rate  # Stochastic depthæ­£åˆ™åŒ–
        )
        
        self.multi_scale = multi_scale
        
        # è·å–å„stageçš„é€šé“æ•°
        if model_size == 'base':
            self.out_channels = [128, 256, 512, 1024]  # Stage 0,1,2,3
        elif model_size == 'small':
            self.out_channels = [96, 192, 384, 768]
        elif model_size == 'tiny':
            self.out_channels = [96, 192, 384, 768]
```

**Swin Transformeræ¶æ„ç»†èŠ‚**:

1. **å±‚çº§ç»“æ„** (Hierarchical):
   ```
   è¾“å…¥: [B, 3, 224, 224]
   
   Stage 0: [B, 128, 56, 56]   (H/4 Ã— W/4,  æµ…å±‚ç‰¹å¾)
            â†“ Patch Merging
   Stage 1: [B, 256, 28, 28]   (H/8 Ã— W/8,  ä¸­å±‚ç‰¹å¾)
            â†“ Patch Merging
   Stage 2: [B, 512, 14, 14]   (H/16 Ã— W/16, ä¸­é«˜å±‚ç‰¹å¾)
            â†“ Patch Merging
   Stage 3: [B, 1024, 7, 7]    (H/32 Ã— W/32, é«˜å±‚ç‰¹å¾)
   ```

2. **Shifted Window Self-Attention**:
   ```python
   # æ¯ä¸ªSwin Transformer BlockåŒ…å«:
   - Window Multi-Head Self-Attention (W-MSA)
   - Shifted Window Multi-Head Self-Attention (SW-MSA)
   - MLP with GELU
   - Layer Normalization
   - Residual Connections
   ```

3. **çª—å£æ³¨æ„åŠ›æœºåˆ¶**:
   - Window Size: 7Ã—7
   - Attention Heads: [4, 8, 16, 32] (å¯¹åº”Stage 0-3)
   - å±€éƒ¨çª—å£å†…è®¡ç®—æ³¨æ„åŠ› (æ•ˆç‡é«˜)
   - Shifted windowå®ç°è·¨çª—å£ä¿¡æ¯ä¼ é€’

**å¯¹æ¯”ResNet50çš„ä¼˜åŠ¿**:

| ç»´åº¦ | ResNet50 | Swin Transformer | ä¼˜åŠ¿ |
|------|----------|------------------|------|
| **æ„Ÿå—é‡** | å±€éƒ¨å·ç§¯ (3Ã—3, 5Ã—5) | çª—å£æ³¨æ„åŠ› (7Ã—7) + Shifted | æ›´å¤§ã€æ›´çµæ´»çš„æ„Ÿå—é‡ |
| **å…¨å±€å»ºæ¨¡** | âŒ ä»…é€šè¿‡å †å å·ç§¯é—´æ¥å®ç° | âœ… è‡ªæ³¨æ„åŠ›ç›´æ¥å»ºæ¨¡é•¿ç¨‹ä¾èµ– | æ›´å¥½çš„å…¨å±€ç†è§£ |
| **å±‚çº§ç‰¹å¾** | 4ä¸ªstageï¼Œç‰¹å¾ç»´åº¦å›ºå®šå¢é•¿ | 4ä¸ªstageï¼Œå±‚çº§å¼patch merging | æ¸è¿›å¼æŠ½è±¡ |
| **é¢„è®­ç»ƒ** | ImageNet-1K (1.28Må›¾åƒ) | ImageNet-21K (14Må›¾åƒ) | æ›´å¼ºçš„é¢„è®­ç»ƒè¡¨ç¤º |
| **å½’çº³åç½®** | å¼º (å·ç§¯çš„å±€éƒ¨æ€§ã€å¹³ç§»ä¸å˜æ€§) | å¼± (éœ€è¦æ›´å¤šæ•°æ®ï¼Œä½†æ›´çµæ´») | é€‚åº”IQAçš„å¤æ‚æ¨¡å¼ |

**æ€§èƒ½è´¡çŒ®**:
- ResNet50 â†’ Swin-Base (å•å°ºåº¦): **+2.68% SRCC** (0.907 â†’ 0.9338)
- **è¿™æ˜¯æœ€å¤§çš„å•ä¸€è´¡çŒ®** (å æ€»æå‡çš„87%)

---

### æ”¹è¿› 2ï¸âƒ£: å¤šå°ºåº¦ç‰¹å¾æå– - ä»å•å°ºåº¦åˆ°çœŸæ­£çš„å¤šå°ºåº¦

#### åŸå§‹HyperIQAçš„"å¤šå°ºåº¦"

è™½ç„¶åŸå§‹HyperIQAæå–äº†4ä¸ªlayerçš„LDAç‰¹å¾ï¼Œä½†è¿™äº›ç‰¹å¾:

1. **ç”¨é€”ä¸åŒ**:
   ```python
   # LDAç‰¹å¾ â†’ æ‹¼æ¥æˆtarget_in_vec â†’ è¿›å…¥TargetNeté¢„æµ‹è´¨é‡
   lda_1 = self.lda1_fc(self.lda1_pool(layer1).view(x.size(0), -1))  # 112ç»´ä¸­çš„ä¸€éƒ¨åˆ†
   lda_2 = self.lda2_fc(self.lda2_pool(layer2).view(x.size(0), -1))
   lda_3 = self.lda3_fc(self.lda3_pool(layer3).view(x.size(0), -1))
   lda_4 = self.lda4_fc(self.lda4_pool(layer4).view(x.size(0), -1))
   
   target_in_vec = torch.cat((lda_1, lda_2, lda_3, lda_4), 1)  # [B, 112]
   
   # ä½†HyperNetåªç”¨layer4çš„ç‰¹å¾
   hyper_in_feat = layer4  # [B, 2048, 7, 7]
   ```

2. **å‹ç¼©ä¸¥é‡**:
   - LDAé€šè¿‡1Ã—1å·ç§¯ + 7Ã—7æ± åŒ– + FCï¼Œå°†ç©ºé—´ç‰¹å¾å‹ç¼©ä¸ºæ ‡é‡
   - æŸå¤±äº†å¤§é‡ç©ºé—´ä¿¡æ¯
   - 4ä¸ªlayerçš„ç‰¹å¾è¢«å‹ç¼©æˆä»…112ç»´çš„å‘é‡

3. **HyperNetå•å°ºåº¦**:
   - ç”ŸæˆåŠ¨æ€æƒé‡çš„HyperNetåªä½¿ç”¨layer4 (2048ç»´)
   - æ²¡æœ‰åˆ©ç”¨æµ…å±‚ç‰¹å¾

---

#### æˆ‘ä»¬çš„çœŸæ­£å¤šå°ºåº¦ç‰¹å¾èåˆ

```python
# models_swin.py
def forward(self, x):
    # æå–4ä¸ªstageçš„ç‰¹å¾
    features = self.swin(x)
    
    # Baseæ¨¡å‹çš„ç‰¹å¾å°ºå¯¸:
    # features[0]: [B, 128, 56, 56]   - Stage 0: ä½å±‚çº¹ç†ã€è¾¹ç¼˜
    # features[1]: [B, 256, 28, 28]   - Stage 1: ä¸­å±‚ç»“æ„
    # features[2]: [B, 512, 14, 14]   - Stage 2: ä¸­é«˜å±‚è¯­ä¹‰
    # features[3]: [B, 1024, 7, 7]    - Stage 3: é«˜å±‚æŠ½è±¡
    
    if self.multi_scale:
        # ç»Ÿä¸€ç©ºé—´å°ºå¯¸åˆ°7Ã—7 (ä½¿ç”¨è‡ªé€‚åº”å¹³å‡æ± åŒ–)
        pooled_features = []
        for feat in features:
            pooled = F.adaptive_avg_pool2d(feat, (7, 7))  # æ‰€æœ‰â†’[B, C_i, 7, 7]
            pooled_features.append(pooled)
        
        # åœ¨é€šé“ç»´åº¦æ‹¼æ¥
        # Base: [B, 128+256+512+1024, 7, 7] = [B, 1920, 7, 7]
        hyper_in_feat_raw = torch.cat(pooled_features, dim=1)
    else:
        # å•å°ºåº¦ï¼šä»…ç”¨æœ€åä¸€ä¸ªstage
        hyper_in_feat_raw = features[-1]  # [B, 1024, 7, 7]
```

**å¤šå°ºåº¦èåˆçš„å…³é”®è®¾è®¡**:

1. **ä¸å‹ç¼©ä¸ºå‘é‡**:
   - ä¿ç•™ [B, 1920, 7, 7] çš„ç‰¹å¾å›¾
   - è€Œä¸æ˜¯å‹ç¼©ä¸º [B, 112] çš„å‘é‡
   - **ä¿ç•™äº†ç©ºé—´ä¿¡æ¯** (7Ã—7çš„ç©ºé—´ç»“æ„)

2. **å…¨éƒ¨è¿›å…¥HyperNet**:
   - 1920ç»´çš„èåˆç‰¹å¾ç”¨äºç”ŸæˆåŠ¨æ€æƒé‡
   - è€Œä¸æ˜¯åªç”¨æœ€åä¸€å±‚

3. **è‡ªé€‚åº”æ± åŒ–ç»Ÿä¸€å°ºå¯¸**:
   ```python
   # 56Ã—56 â†’ 7Ã—7 (ä¸‹é‡‡æ ·8å€)
   # 28Ã—28 â†’ 7Ã—7 (ä¸‹é‡‡æ ·4å€)
   # 14Ã—14 â†’ 7Ã—7 (ä¸‹é‡‡æ ·2å€)
   # 7Ã—7   â†’ 7Ã—7 (ä¿æŒä¸å˜)
   ```
   - å¹³æ»‘çš„å°ºå¯¸ç»Ÿä¸€ï¼Œä¿ç•™å…³é”®ä¿¡æ¯
   - é¿å…æš´åŠ›resize

**ä¸åŒå°ºåº¦çš„è¯­ä¹‰æ„ä¹‰**:

| Stage | ç©ºé—´å°ºå¯¸ | è¯­ä¹‰å±‚æ¬¡ | å¯¹IQAçš„ä½œç”¨ |
|-------|---------|---------|------------|
| **Stage 0** | 56Ã—56 (H/4) | ä½å±‚ | çº¹ç†ç»†èŠ‚ã€è¾¹ç¼˜é”åº¦ã€å™ªå£° |
| **Stage 1** | 28Ã—28 (H/8) | ä¸­ä½å±‚ | å±€éƒ¨ç»“æ„ã€å°ç‰©ä½“ |
| **Stage 2** | 14Ã—14 (H/16) | ä¸­é«˜å±‚ | åŒºåŸŸç»“æ„ã€å¤±çœŸæ¨¡å¼ |
| **Stage 3** | 7Ã—7 (H/32) | é«˜å±‚ | å…¨å±€è¯­ä¹‰ã€å†…å®¹ç†è§£ |

**ä¸ºä»€ä¹ˆå¤šå°ºåº¦å¯¹IQAé‡è¦?**

å›¾åƒè´¨é‡çš„é€€åŒ–æ˜¯å¤šå°ºåº¦çš„:
- **é«˜é¢‘å¤±çœŸ** (å™ªå£°ã€å—æ•ˆåº”ã€é”¯é½¿): éœ€è¦ä½å±‚ç‰¹å¾æ•è·
- **ä¸­é¢‘å¤±çœŸ** (æ¨¡ç³Šã€å‹ç¼©ä¼ªå½±): éœ€è¦ä¸­å±‚ç‰¹å¾
- **ä½é¢‘å¤±çœŸ** (é¢œè‰²åç§»ã€æ•´ä½“å¯¹æ¯”åº¦): éœ€è¦é«˜å±‚ç‰¹å¾

å•ä¸€å°ºåº¦æ— æ³•å…¨é¢è¯„ä¼°ï¼

**æ€§èƒ½è´¡çŒ®**:
- Swin-Baseå•å°ºåº¦: 0.9338
- Swin-Baseå¤šå°ºåº¦: 0.9353
- **è´¡çŒ®: +0.15% SRCC**

**æ¶ˆèå®éªŒéªŒè¯**:
- **A2 (ç§»é™¤å¤šå°ºåº¦)**: SRCC 0.9338 (ä»…ç”¨Stage 3)
- **A1 (ä¿ç•™å¤šå°ºåº¦)**: SRCC 0.9353
- **å·®å¼‚**: +0.0015 SRCC

---

### æ”¹è¿› 3ï¸âƒ£: æ³¨æ„åŠ›æœºåˆ¶ - åŠ¨æ€å¤šå°ºåº¦ç‰¹å¾èåˆ

#### åŸå§‹HyperIQA: æ— æ³¨æ„åŠ›æœºåˆ¶

åŸå§‹è®¾è®¡:
- å¤šä¸ªLDAç‰¹å¾ **ç›´æ¥æ‹¼æ¥**
- æ‰€æœ‰å°ºåº¦ç­‰æƒé‡
- æ— æ³•æ ¹æ®è¾“å…¥å›¾åƒè‡ªé€‚åº”è°ƒæ•´

#### æˆ‘ä»¬çš„Channel Attentionæœºåˆ¶

```python
# models_swin.py
class AttentionFusion(nn.Module):
    """
    Channel Attentionæ¨¡å—
    æ ¹æ®è¾“å…¥åŠ¨æ€ç”Ÿæˆæ¯ä¸ªé€šé“çš„æƒé‡
    """
    def __init__(self, in_channels=1920, reduction_ratio=4):
        super().__init__()
        
        # å…¨å±€å¹³å‡æ± åŒ–: ç©ºé—´ç»´åº¦ â†’ æ ‡é‡
        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # [B, C, H, W] â†’ [B, C, 1, 1]
        
        # ä¸¤å±‚MLPç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        hidden_channels = in_channels // reduction_ratio  # 1920 â†’ 480
        self.fc = nn.Sequential(
            nn.Linear(in_channels, hidden_channels),  # é™ç»´
            nn.ReLU(inplace=True),                    # éçº¿æ€§
            nn.Linear(hidden_channels, in_channels),  # å‡ç»´å›åŸå§‹ç»´åº¦
            nn.Sigmoid()                               # è¾“å‡º[0, 1]çš„æƒé‡
        )
    
    def forward(self, x):
        # x: [B, 1920, 7, 7]
        b, c, h, w = x.size()
        
        # 1. Squeeze: å…¨å±€æ± åŒ–æå–é€šé“æè¿°ç¬¦
        y = self.avg_pool(x).view(b, c)  # [B, 1920]
        
        # 2. Excitation: ç”Ÿæˆé€šé“æ³¨æ„åŠ›æƒé‡
        attention = self.fc(y).view(b, c, 1, 1)  # [B, 1920, 1, 1]
        
        # 3. Reweight: é€é€šé“åŠ æƒ
        out = x * attention.expand_as(x)  # [B, 1920, 7, 7]
        
        return out, attention  # è¿”å›åŠ æƒç‰¹å¾å’Œæ³¨æ„åŠ›æƒé‡
```

**Attentionæœºåˆ¶çš„ä½œç”¨**:

1. **è‡ªé€‚åº”åŠ æƒ**:
   ```
   ä¸åŒå›¾åƒ â†’ ä¸åŒæ³¨æ„åŠ›æƒé‡
   
   ä¾‹å¦‚:
   - å™ªå£°å›¾åƒ â†’ ä½å±‚ç‰¹å¾(Stage 0-1)æƒé‡é«˜
   - æ¨¡ç³Šå›¾åƒ â†’ ä¸­å±‚ç‰¹å¾(Stage 2)æƒé‡é«˜
   - å‹ç¼©å›¾åƒ â†’ é«˜å±‚ç‰¹å¾(Stage 3)æƒé‡é«˜
   ```

2. **é€šé“çº§æ³¨æ„åŠ›**:
   - 1920ä¸ªé€šé“å„è‡ªæœ‰ç‹¬ç«‹çš„æƒé‡
   - Stage 0çš„128ä¸ªé€šé“ã€Stage 1çš„256ä¸ªé€šé“ç­‰éƒ½å¯ä»¥è¢«ç‹¬ç«‹è°ƒæ§
   - æ¯”"å°ºåº¦çº§æ³¨æ„åŠ›"æ›´ç»†ç²’åº¦

3. **å¯è§£é‡Šæ€§**:
   - æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
   - åˆ†ææ¨¡å‹å…³æ³¨çš„ç‰¹å¾ç»´åº¦

**ä¸å…¶ä»–æ³¨æ„åŠ›æœºåˆ¶çš„å¯¹æ¯”**:

| æ³¨æ„åŠ›ç±»å‹ | æ“ä½œå¯¹è±¡ | å‚æ•°é‡ | è®¡ç®—å¤æ‚åº¦ | æˆ‘ä»¬æ˜¯å¦é‡‡ç”¨ |
|-----------|---------|--------|-----------|------------|
| **Channel Attention** | é€šé“ç»´åº¦ | å°‘ (1920â†’480â†’1920) | ä½ | âœ… é‡‡ç”¨ |
| Spatial Attention | ç©ºé—´ä½ç½® | å°‘ | ä¸­ | âŒ æœªé‡‡ç”¨ |
| Self-Attention | Tokenä¹‹é—´ | å¤š (Q, K, VçŸ©é˜µ) | é«˜ | âŒ æœªé‡‡ç”¨ (Swinå†…éƒ¨å·²æœ‰) |
| å°ºåº¦çº§Attention | 4ä¸ªå°ºåº¦ | æå°‘ | æä½ | âŒ æ•ˆæœä¸å¦‚é€šé“çº§ |

**ä¸ºä»€ä¹ˆé€‰æ‹©Channel Attention?**

1. **æ•ˆç‡**: 
   - å‚æ•°å°‘: 1920Ã—480 + 480Ã—1920 â‰ˆ 1.8M
   - è®¡ç®—å¿«: ä»…å…¨å±€æ± åŒ– + ä¸¤ä¸ªFC

2. **æœ‰æ•ˆæ€§**:
   - ç›´æ¥å»ºæ¨¡é€šé“é—´ä¾èµ–
   - å¯¹åº”ä¸åŒè¯­ä¹‰çš„é‡è¦æ€§

3. **ç¨³å®šæ€§**:
   - æ¯”å¤æ‚çš„self-attentionæ›´å®¹æ˜“è®­ç»ƒ
   - ä¸éœ€è¦é¢å¤–çš„ä½ç½®ç¼–ç 

**æ€§èƒ½è´¡çŒ®**:
- Swin-Baseå¤šå°ºåº¦ (æ— Attention): 0.9353
- Swin-Baseå¤šå°ºåº¦ (æœ‰Attention): 0.9378
- **è´¡çŒ®: +0.25% SRCC**

**æ¶ˆèå®éªŒéªŒè¯**:
- **A1 (ç§»é™¤Attention)**: SRCC 0.9353
- **E6 (å®Œæ•´æ¨¡å‹)**: SRCC 0.9378
- **å·®å¼‚**: +0.0025 SRCC

---

### æ”¹è¿› 4ï¸âƒ£: HyperNetworkè¾“å…¥å¢å¼º

#### åŸå§‹HyperIQAçš„HyperNet

```python
# models.py (åŸå§‹)
class HyperNet(nn.Module):
    def forward(self, img):
        res_out = self.res(img)  # ResNet backbone
        
        # HyperNetçš„è¾“å…¥
        hyper_in_feat = res_out['hyper_in_feat']  # [B, 2048, 7, 7]
        
        # é™ç»´: 2048 â†’ 1024 â†’ 512 â†’ hyperInChn
        hyper_in_feat = self.conv1(hyper_in_feat)  # [B, hyperInChn, 7, 7]
        
        # ç”ŸæˆTargetNetçš„åŠ¨æ€æƒé‡
        target_fc1w = self.fc1w_conv(hyper_in_feat)
        target_fc1b = self.fc1b_fc(self.pool(hyper_in_feat).squeeze())
        # ... ç”Ÿæˆæ‰€æœ‰å±‚çš„æƒé‡å’Œåç½®
```

**è¾“å…¥ç‰¹å¾**:
- ç»´åº¦: [B, 2048, 7, 7]
- æ¥æº: ResNet50çš„layer4
- ä¿¡æ¯: ä»…é«˜å±‚è¯­ä¹‰

---

#### æˆ‘ä»¬çš„å¢å¼ºHyperNet

```python
# models_swin.py (æˆ‘ä»¬çš„)
class HyperNet(nn.Module):
    def forward(self, img):
        # 1. Swin Transformeræå–å¤šå°ºåº¦ç‰¹å¾
        features = self.swin_backbone(img)  # 4ä¸ªstageç‰¹å¾
        
        # 2. å¤šå°ºåº¦èåˆ
        if self.multi_scale:
            # æ‹¼æ¥: [B, 1920, 7, 7] (Base)
            hyper_in_feat_raw = self.fuse_multi_scale(features)
        else:
            hyper_in_feat_raw = features[-1]  # [B, 1024, 7, 7]
        
        # 3. æ³¨æ„åŠ›åŠ æƒ (å¯é€‰)
        if self.use_attention:
            hyper_in_feat_raw, attention_weights = self.attention(hyper_in_feat_raw)
        
        # 4. é™ç»´é€‚é… (å¦‚æœéœ€è¦)
        # 1920 â†’ 256 (ä¿æŒæ›´å¤šä¿¡æ¯ï¼Œä¸åƒåŸå§‹é‚£æ ·é™åˆ°å¾ˆå°)
        hyper_in_feat = self.conv_adapt(hyper_in_feat_raw)  # [B, 256, 7, 7]
        
        # 5. ç”ŸæˆTargetNetçš„åŠ¨æ€æƒé‡ (ç»“æ„åŒåŸå§‹)
        # ... (weight generation code)
```

**è¾“å…¥ç‰¹å¾å¢å¼ºå¯¹æ¯”**:

| ç‰¹æ€§ | åŸå§‹HyperIQA | æˆ‘ä»¬çš„æ–¹æ³• | æå‡ |
|------|-------------|-----------|------|
| **è¾“å…¥ç»´åº¦** | [B, 2048, 7, 7] | [B, 1920, 7, 7] | ç•¥å°ä½†ä¿¡æ¯æ›´ä¸°å¯Œ |
| **ç‰¹å¾æ¥æº** | ä»…layer4 (é«˜å±‚) | Stage 0+1+2+3 (å…¨å±‚çº§) | **å¤šå°ºåº¦** âœ… |
| **æ³¨æ„åŠ›** | âŒ æ—  | âœ… Channel attention | **è‡ªé€‚åº”** âœ… |
| **ä¿¡æ¯é‡** | 2048 Ã— 7 Ã— 7 = 100,352 | 1920 Ã— 7 Ã— 7 = 94,080 | æ€»é‡ç•¥å°‘ä½†è´¨é‡æ›´é«˜ |
| **è¯­ä¹‰è¦†ç›–** | ä»…é«˜å±‚è¯­ä¹‰ | ä½ã€ä¸­ã€é«˜å±‚å…¨è¦†ç›– | **å…¨é¢** âœ… |

**ä¸ºä»€ä¹ˆè¾“å…¥å¢å¼ºé‡è¦?**

HyperNetçš„èŒè´£æ˜¯**ç”Ÿæˆé€‚åº”æ¯å¼ å›¾åƒçš„åŠ¨æ€æƒé‡**:
- è¾“å…¥ç‰¹å¾è¶Šä¸°å¯Œ â†’ ç”Ÿæˆçš„æƒé‡è¶Šç²¾ç¡®
- å¤šå°ºåº¦ä¿¡æ¯ â†’ èƒ½é’ˆå¯¹ä¸åŒç±»å‹çš„å¤±çœŸè°ƒæ•´
- æ³¨æ„åŠ›æœºåˆ¶ â†’ å¼ºè°ƒå½“å‰å›¾åƒæœ€é‡è¦çš„ç‰¹å¾

---

### æ”¹è¿› 5ï¸âƒ£: TargetNetä¿æŒä¸å˜ï¼ˆä½†è¾“å…¥æ›´å¥½ï¼‰

```python
# TargetNetçš„ç»“æ„å®Œå…¨ä¿æŒåŸå§‹HyperIQAçš„è®¾è®¡
class TargetNet(nn.Module):
    def __init__(self, paras, dropout_rate=0.4):
        super().__init__()
        self.l1 = TargetFC(paras['target_in_size'], paras['target_fc1_size'])
        self.l2 = TargetFC(paras['target_fc1_size'], paras['target_fc2_size'])
        self.l3 = TargetFC(paras['target_fc2_size'], paras['target_fc3_size'])
        self.l4 = TargetFC(paras['target_fc3_size'], paras['target_fc4_size'])
        self.l5 = nn.Linear(paras['target_fc4_size'], 1)
        
        # æ·»åŠ dropoutæ­£åˆ™åŒ–
        self.dropout = nn.Dropout(dropout_rate)
    
    def forward(self, x):
        # x: target_in_vec [B, target_in_size, 1, 1]
        q = self.l1(x)  # ä½¿ç”¨HyperNetç”Ÿæˆçš„æƒé‡
        q = self.dropout(q)
        q = self.l2(q)
        q = self.dropout(q)
        q = self.l3(q)
        q = self.dropout(q)
        q = self.l4(q)
        q = self.l5(q.squeeze())  # [B, 1]
        return q
```

**ä¸ºä»€ä¹ˆä¸æ”¹TargetNet?**

1. **åŸå§‹è®¾è®¡æœ‰æ•ˆ**: åŠ¨æ€æƒé‡ç”Ÿæˆæœºåˆ¶å·²ç»å¾ˆå¼ºå¤§
2. **æ”¹è¿›èšç„¦backbone**: ç“¶é¢ˆåœ¨ç‰¹å¾æå–ï¼Œä¸åœ¨é¢„æµ‹å¤´
3. **ä¿æŒç®€æ´**: é¿å…å¼•å…¥å¤ªå¤šå¤æ‚æ€§

**æˆ‘ä»¬çš„å”¯ä¸€æ”¹åŠ¨**: 
- æ·»åŠ Dropout (0.4) ç”¨äºæ­£åˆ™åŒ–
- å…¶ä»–ç»“æ„å®Œå…¨ä¿æŒåŸå§‹

---

## ğŸ“ å®Œæ•´ç½‘ç»œæ¶æ„å›¾

### æ•°æ®æµç¤ºæ„

```
è¾“å…¥å›¾åƒ [B, 3, 224, 224]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Swin Transformer Backbone (Pretrained)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Patch Partition (4Ã—4) â†’ Patches [56Ã—56Ã—128] â”‚  â”‚
â”‚  â”‚                                              â”‚  â”‚
â”‚  â”‚ Stage 0: Swin Blocks Ã— 2                    â”‚  â”‚
â”‚  â”‚   â†’ [B, 128, 56, 56]   â† æå–               â”‚  â”‚
â”‚  â”‚   â†“ Patch Merging                           â”‚  â”‚
â”‚  â”‚                                              â”‚  â”‚
â”‚  â”‚ Stage 1: Swin Blocks Ã— 2                    â”‚  â”‚
â”‚  â”‚   â†’ [B, 256, 28, 28]   â† æå–               â”‚  â”‚
â”‚  â”‚   â†“ Patch Merging                           â”‚  â”‚
â”‚  â”‚                                              â”‚  â”‚
â”‚  â”‚ Stage 2: Swin Blocks Ã— 18                   â”‚  â”‚
â”‚  â”‚   â†’ [B, 512, 14, 14]   â† æå–               â”‚  â”‚
â”‚  â”‚   â†“ Patch Merging                           â”‚  â”‚
â”‚  â”‚                                              â”‚  â”‚
â”‚  â”‚ Stage 3: Swin Blocks Ã— 2                    â”‚  â”‚
â”‚  â”‚   â†’ [B, 1024, 7, 7]    â† æå–               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ â†“ â†“ â†“
  128 256 512 1024 (4ä¸ªå°ºåº¦ç‰¹å¾)
    â†“ â†“ â†“ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-scale Fusion Module                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ AdaptiveAvgPool2d(7Ã—7) å¯¹æ¯ä¸ªç‰¹å¾å›¾          â”‚  â”‚
â”‚  â”‚   Stage 0: [B, 128, 56, 56] â†’ [B, 128, 7, 7] â”‚  â”‚
â”‚  â”‚   Stage 1: [B, 256, 28, 28] â†’ [B, 256, 7, 7] â”‚  â”‚
â”‚  â”‚   Stage 2: [B, 512, 14, 14] â†’ [B, 512, 7, 7] â”‚  â”‚
â”‚  â”‚   Stage 3: [B, 1024, 7, 7]  â†’ [B, 1024, 7, 7]â”‚  â”‚
â”‚  â”‚                                              â”‚  â”‚
â”‚  â”‚ torch.cat(dim=1)  é€šé“æ‹¼æ¥                   â”‚  â”‚
â”‚  â”‚   â†’ [B, 1920, 7, 7]                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Channel Attention Module                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ GlobalAvgPool: [B, 1920, 7, 7] â†’ [B, 1920]  â”‚  â”‚
â”‚  â”‚   â†“                                          â”‚  â”‚
â”‚  â”‚ FC: 1920 â†’ 480 (é™ç»´)                        â”‚  â”‚
â”‚  â”‚   â†“ ReLU                                     â”‚  â”‚
â”‚  â”‚ FC: 480 â†’ 1920 (å‡ç»´)                        â”‚  â”‚
â”‚  â”‚   â†“ Sigmoid                                  â”‚  â”‚
â”‚  â”‚ Attention Weights: [B, 1920]                â”‚  â”‚
â”‚  â”‚   â†“                                          â”‚  â”‚
â”‚  â”‚ Reweight: feat Ã— attention                  â”‚  â”‚
â”‚  â”‚   â†’ [B, 1920, 7, 7]                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HyperNetwork (ä¿æŒåŸå§‹ç»“æ„)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ è¾“å…¥: [B, 1920, 7, 7]                        â”‚  â”‚
â”‚  â”‚   â†“                                          â”‚  â”‚
â”‚  â”‚ Convé™ç»´: 1920 â†’ 256                         â”‚  â”‚
â”‚  â”‚   â†“                                          â”‚  â”‚
â”‚  â”‚ ä¸ºTargetNetç”ŸæˆåŠ¨æ€å‚æ•°:                     â”‚  â”‚
â”‚  â”‚   - target_fc1_weight: [B, 112, 112, ...]   â”‚  â”‚
â”‚  â”‚   - target_fc1_bias:   [B, 112]             â”‚  â”‚
â”‚  â”‚   - target_fc2_weight, fc2_bias             â”‚  â”‚
â”‚  â”‚   - target_fc3_weight, fc3_bias             â”‚  â”‚
â”‚  â”‚   - target_fc4_weight, fc4_bias             â”‚  â”‚
â”‚  â”‚   - target_fc5_weight, fc5_bias             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (dynamic parameters)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TargetNetwork (ä¿æŒåŸå§‹ç»“æ„)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ è¾“å…¥: target_in_vec [B, 112]                 â”‚  â”‚
â”‚  â”‚   â†“                                          â”‚  â”‚
â”‚  â”‚ FC1 (ç”¨HyperNetç”Ÿæˆçš„æƒé‡) â†’ [B, 112]        â”‚  â”‚
â”‚  â”‚   â†“ Dropout(0.4)                            â”‚  â”‚
â”‚  â”‚ FC2 (åŠ¨æ€æƒé‡) â†’ [B, 56]                     â”‚  â”‚
â”‚  â”‚   â†“ Dropout(0.4)                            â”‚  â”‚
â”‚  â”‚ FC3 (åŠ¨æ€æƒé‡) â†’ [B, 28]                     â”‚  â”‚
â”‚  â”‚   â†“ Dropout(0.4)                            â”‚  â”‚
â”‚  â”‚ FC4 (åŠ¨æ€æƒé‡) â†’ [B, 14]                     â”‚  â”‚
â”‚  â”‚   â†“                                          â”‚  â”‚
â”‚  â”‚ FC5 â†’ [B, 1]                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è´¨é‡åˆ†æ•° [B, 1] (0-100)
```

---

## ğŸ”¬ æ¶ˆèå®éªŒéªŒè¯å„ç»„ä»¶è´¡çŒ®

### å®éªŒè®¾è®¡ï¼ˆæ­£å‘ï¼‰

| å®éªŒID | é…ç½® | SRCC | PLCC | Î” SRCC | ç»„ä»¶è´¡çŒ® |
|--------|------|------|------|--------|---------|
| **C0** | ResNet50 (åŸå§‹HyperIQA) | 0.907 | ~0.918 | - | Baseline |
| **A2** | Swin-Base (å•å°ºåº¦, æ— Attention) | 0.9338 | 0.9438 | **+0.0268** | **Backbone: +2.68%** â­ |
| **A1** | Swin-Base + Multi-scale (æ— Attention) | 0.9353 | 0.9458 | **+0.0015** | **Multi-scale: +0.15%** |
| **E6** | Swin-Base + Multi-scale + Attention | **0.9378** | **0.9485** | **+0.0025** | **Attention: +0.25%** |

### å„ç»„ä»¶è´¡çŒ®å æ¯”

```
æ€»æå‡: +3.08% SRCC

è´¡çŒ®åˆ†è§£:
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Swin Transformer: +2.68% (87%)
â–ˆâ–ˆâ–ˆ Multi-scale Fusion: +0.15% (5%)
â–ˆâ–ˆâ–ˆâ–ˆ Attention Mechanism: +0.25% (8%)
```

### å…³é”®å‘ç°

1. **Backboneæ˜¯æ ¸å¿ƒ** (87%è´¡çŒ®):
   - Swin Transformerçš„å±‚çº§ç»“æ„å’Œå…¨å±€å»ºæ¨¡èƒ½åŠ›æ˜¯æœ€å¤§æ”¹è¿›
   - é¢„è®­ç»ƒæƒé‡ï¼ˆImageNet-21Kï¼‰æä¾›äº†å¼ºå¤§çš„ç‰¹å¾è¡¨ç¤º

2. **Attentionæ•ˆæœæœ€æ˜æ˜¾** (8%è´¡çŒ®ï¼Œä½†ROIæœ€é«˜):
   - ç›¸å¯¹äºå¼•å…¥çš„å‚æ•°é‡ï¼ˆ~1.8Mï¼‰ï¼Œæ€§èƒ½æå‡æ˜¾è‘—
   - æ¯ç™¾ä¸‡å‚æ•°å¸¦æ¥ +0.014% SRCC

3. **Multi-scaleè´¡çŒ®ç¨³å®š** (5%è´¡çŒ®):
   - æ— é¢å¤–å‚æ•°ï¼ˆä»…æ‹¼æ¥æ“ä½œï¼‰
   - çº¯æ¶æ„æ”¹è¿›å¸¦æ¥ç¨³å®šæå‡

---

## ğŸ’¾ æ¨¡å‹å¤æ‚åº¦åˆ†æ

### Baseæ¨¡å‹ (æœ€ä½³æ€§èƒ½)

| æŒ‡æ ‡ | æ•°å€¼ | å¯¹æ¯”ResNet50 |
|------|------|------------|
| **å‚æ•°é‡** | 88.0M | +252% (25M â†’ 88M) |
| **FLOPs** | 18.2G | +355% (4G â†’ 18.2G) |
| **æ¨ç†æ—¶é—´** | 45.2ms/image | +166% (17ms â†’ 45ms) |
| **å†…å­˜å ç”¨** | ~2.5GB | +150% (1GB â†’ 2.5GB) |

**å‚æ•°åˆ†å¸ƒ**:
```
Swin Backbone:       87.5M (99.4%)
  - Stage 0-1:        3.8M
  - Stage 2:         39.7M  â† æœ€å¤§
  - Stage 3:         44.0M
Attention Module:     1.8M (2.0%)
HyperNet:            ~0.5M (0.6%)
TargetNet:           <0.1M (negligible)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:               88.0M
```

### ä¸åŒæ¨¡å‹å°ºå¯¸å¯¹æ¯”

| Model | å‚æ•°é‡ | FLOPs | SRCC | PLCC | æ¨ç†æ—¶é—´ | æ•ˆç‡æ¯” |
|-------|--------|-------|------|------|---------|--------|
| **Tiny** | 28M | ~5G | 0.9249 | 0.9360 | 25ms | **1.0x** (baseline) |
| **Small** | 50M | ~11G | 0.9338 | 0.9455 | 35ms | 0.71x |
| **Base** | 88M | 18.2G | **0.9378** | **0.9485** | 45ms | 0.56x |

**æ€§èƒ½-æ•ˆç‡æƒè¡¡**:
- **Base**: æœ€é«˜ç²¾åº¦ï¼Œé€‚åˆå­¦æœ¯ç ”ç©¶å’Œbenchmark
- **Small**: ä»…æŸå¤±0.4% SRCCï¼Œå‚æ•°å‡å°‘43%ï¼Œæ¨èéƒ¨ç½²
- **Tiny**: æŸå¤±1.29% SRCCï¼Œä½†å‚æ•°å‡å°‘68%ï¼Œèµ„æºå—é™åœºæ™¯

---

## ğŸŒ è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›

### æµ‹è¯•è®¾ç½®
- è®­ç»ƒ: KonIQ-10k
- æµ‹è¯•: SPAQ, KADID-10K, AGIQA-3K (zero-shot)

### ç»“æœ

| æ•°æ®é›† | ç±»å‹ | SRCC | PLCC | vs è®­ç»ƒé›† |
|--------|------|------|------|----------|
| **KonIQ-10k** | è®­ç»ƒé›† (In-domain) | **0.9378** | **0.9485** | - |
| **SPAQ** | æ™ºèƒ½æ‰‹æœºç…§ç‰‡ | **0.8698** | 0.8709 | -7.2% |
| **KADID-10K** | åˆæˆå¤±çœŸ | 0.5412 | 0.5591 | -42.3% âš ï¸ |
| **AGIQA-3K** | AIç”Ÿæˆå›¾åƒ | 0.6484 | 0.6830 | -30.9% |

**åˆ†æ**:
1. âœ… **SPAQæ³›åŒ–è‰¯å¥½** (-7%): è‡ªç„¶åœºæ™¯è´¨é‡è¯„ä¼°èƒ½åŠ›å¼º
2. âš ï¸ **KADIDæ³›åŒ–è¾ƒå·®** (-42%): åˆæˆå¤±çœŸï¼ˆGaussian blur, JPEGå‹ç¼©ç­‰ï¼‰ä¸è‡ªç„¶å¤±çœŸå·®å¼‚å¤§
3. ğŸ¤” **AGIQAä¸­ç­‰** (-31%): AIç”Ÿæˆå›¾åƒçš„å¤±çœŸæ¨¡å¼ç‹¬ç‰¹ï¼Œéœ€è¦ç‰¹å®šè®­ç»ƒ

---

## ğŸ¯ è®¾è®¡å“²å­¦æ€»ç»“

### ä¸ºä»€ä¹ˆè¿™äº›æ”¹è¿›æœ‰æ•ˆ?

#### 1. **æ›´å¼ºçš„Backbone = æ›´å¥½çš„è¡¨ç¤º**
```
ResNet50 (25M, ImageNet-1K) 
    â†’ CNNå±€éƒ¨æ„Ÿå—é‡ï¼Œæœ‰é™çš„å…¨å±€å»ºæ¨¡
    
Swin Transformer (88M, ImageNet-21K)
    â†’ Transformerå…¨å±€å»ºæ¨¡ï¼Œå±‚çº§å¼ç‰¹å¾æå–
    â†’ é¢„è®­ç»ƒæ•°æ®10å€äºResNet
    â†’ +2.68% SRCC â­
```

#### 2. **å¤šå°ºåº¦ = å…¨é¢æ„ŸçŸ¥**
```
è´¨é‡é€€åŒ–æ˜¯å¤šå°ºåº¦çš„:
- å™ªå£°ã€é”¯é½¿ (é«˜é¢‘)     â† éœ€è¦ä½å±‚ç‰¹å¾
- æ¨¡ç³Šã€å‹ç¼© (ä¸­é¢‘)     â† éœ€è¦ä¸­å±‚ç‰¹å¾  
- é¢œè‰²ã€å¯¹æ¯”åº¦ (ä½é¢‘)   â† éœ€è¦é«˜å±‚ç‰¹å¾

å•ä¸€å°ºåº¦ â†’ ç›²äººæ‘¸è±¡
å¤šå°ºåº¦èåˆ â†’ å…¨å±€è§†è§’
    â†’ +0.15% SRCC
```

#### 3. **æ³¨æ„åŠ› = è‡ªé€‚åº”**
```
ä¸åŒå›¾åƒéœ€è¦ä¸åŒçš„ç‰¹å¾æƒé‡:
- å™ªå£°å›¾: å¼ºè°ƒä½å±‚çº¹ç†ç‰¹å¾
- æ¨¡ç³Šå›¾: å¼ºè°ƒä¸­å±‚è¾¹ç¼˜ç‰¹å¾
- å‹ç¼©å›¾: å¼ºè°ƒé«˜å±‚ç»“æ„ç‰¹å¾

Channel AttentionåŠ¨æ€è°ƒæ•´æƒé‡
    â†’ +0.25% SRCC
```

#### 4. **ä¿ç•™åŸå§‹è®¾è®¡çš„ä¼˜åŠ¿**
```
HyperNetåŠ¨æ€æƒé‡ç”Ÿæˆ (åŸåˆ›)
    â†’ ä¸åŒå›¾åƒç”¨ä¸åŒçš„é¢„æµ‹ç½‘ç»œ
    â†’ å†…å®¹è‡ªé€‚åº”çš„è´¨é‡è¯„ä¼°
    â†’ ä¿æŒè¿™ä¸ªæ ¸å¿ƒä¼˜åŠ¿ï¼âœ…
```

---

## ğŸ“ è®ºæ–‡å†™ä½œè¦ç‚¹

### Method Sectionç»“æ„å»ºè®®

```markdown
3. Methodology

3.1 Overview of HyperIQA Framework
    - ç®€è¿°åŸå§‹HyperIQAçš„åŒç½‘ç»œç»“æ„
    - æŒ‡å‡ºæ”¹è¿›æ–¹å‘ï¼šbackbone + feature fusion

3.2 Swin Transformer Backbone
    - ä»‹ç»Swin Transformerçš„å±‚çº§ç»“æ„
    - å¯¹æ¯”ResNet50çš„ä¼˜åŠ¿ï¼ˆå›¾è¡¨ï¼‰
    - ç½‘ç»œé…ç½®ï¼šBase/Small/Tiny

3.3 Multi-scale Feature Extraction
    - ä»4ä¸ªstageæå–ç‰¹å¾
    - è‡ªé€‚åº”æ± åŒ–ç»Ÿä¸€å°ºå¯¸
    - é€šé“æ‹¼æ¥ç­–ç•¥
    - (å¯é€‰) æ¶ˆèå®éªŒè¡¨æ ¼

3.4 Channel Attention Fusion
    - Attentionæœºåˆ¶è®¾è®¡
    - å…¬å¼æ¨å¯¼
    - å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰

3.5 HyperNet and TargetNet
    - è¾“å…¥ç‰¹å¾å¢å¼º
    - æƒé‡ç”Ÿæˆè¿‡ç¨‹
    - ä¿æŒåŸå§‹åŠ¨æ€é¢„æµ‹è®¾è®¡

3.6 Training Strategy (å¦‚æœéœ€è¦)
    - Loss function
    - Optimizer and LR
    - Regularization
```

### é‡ç‚¹å›¾è¡¨

1. **æ¶æ„å¯¹æ¯”å›¾**: ResNet50-HyperIQA vs Swin-HyperIQA
2. **æ¶ˆèå®éªŒè¡¨**: C0 â†’ A2 â†’ A1 â†’ E6
3. **æ¨¡å‹å°ºå¯¸å¯¹æ¯”**: Tiny vs Small vs Base (scatter plot)
4. **è·¨æ•°æ®é›†æ³›åŒ–**: 4ä¸ªæ•°æ®é›†çš„SRCCæŸ±çŠ¶å›¾

### å…³é”®å¥å¼

```
- "We replace the ResNet50 backbone with Swin Transformer, achieving 87% of the total improvement."

- "Multi-scale feature fusion extracts features from all four stages, preserving both low-level textures and high-level semantics."

- "Channel attention mechanism dynamically weights feature channels, adapting to different distortion types."

- "Our method achieves 0.9378 SRCC on KonIQ-10k, outperforming the original HyperIQA by 3.08%."
```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

### ä»£ç æ–‡ä»¶
- `models_swin.py`: å®Œæ•´çš„ç½‘ç»œæ¶æ„å®ç°
- `HyperIQASolver_swin.py`: è®­ç»ƒå’Œæµ‹è¯•é€»è¾‘
- `train_swin.py`: è®­ç»ƒè„šæœ¬

### å®éªŒè®°å½•
- `EXPERIMENTS_LOG_TRACKER.md`: æ‰€æœ‰å®éªŒè¯¦ç»†è®°å½•
- `PAPER_CORE_RESULTS.md`: è®ºæ–‡æ ¸å¿ƒç»“æœ
- `VALIDATION_AND_ABLATION_LOG.md`: æ¶ˆèå’ŒéªŒè¯å®éªŒ

### å¯è§†åŒ–
- `training_curves_best_model.png`: è®­ç»ƒæ›²çº¿
- `generate_paper_figures.py`: è®ºæ–‡å›¾è¡¨ç”Ÿæˆè„šæœ¬

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025-12-23  
**çŠ¶æ€**: âœ… å®Œæ•´ï¼Œå¯ç”¨äºè®ºæ–‡å†™ä½œ


