# Abstract
Blind image quality assessment (BIQA)for authentically distorted images has always been a challenging problem, since images captured in the wild include varies contents and diverse types of distortions. The vast majority of prior BIQA methods focus on how to predict synthetic image quality, but fail when applied to real-world distorted im- ages. To deal with the challenge, we propose a self-adaptive hyper network architecture to blind assess image quality in the wild.  We separate the IQA procedure into three stages including content understanding, perception rule learning and quality predicting.  After extracting image semantics, perception rule is established adaptively by a hyper net- work, and then adopted by a quality prediction network. In our model, image quality can be estimated in a self-adaptive manner, thus generalizes well on diverse images captured in the wild.  Experimental results verify that our approach not only outperforms the state-of-the-art methods on challeng- ing authentic image databases but also achieves competing performances on synthetic image databases, though it is not explicitly designed for the synthetic task.

# 1. Introduction

The goal of image quality assessment (IQA) is to enable computers to perceive image quality like humans.  In the past decades, huge efforts have been devoted and a variety of IQA methods have been proposed.  Despite the success they have achieved for assessing laboratory generated syn- thetically distorted images, IQA for authentically distorted images remains a challenge.  The challenge lies mainly in three aspects:
Firstly, IQA in the wild is limited to the ﬁeld of blind IQA (BIQA) since there exists no access to a reference im-age.  As widely accepted, the limitation of reference im- age has made BIQA the most difﬁcult problem among the three IQA categories, i.e.  full-reference IQA (FR-IQA), reduced-reference IQA (RR-IQA) and BIQA, also known as non-reference IQA (NR-IQA). Secondly, different from the common synthetic distortions (e.g. Gaussian blur, JPEG compression) added to the whole area of image, authen- tic distortions are more complicated. The captured images not only suffer from global uniform distortions (e.g.  out of focus, low illumination), but also contain other kinds of non-uniform distortions (e.g. object moving, over light- ing, ghosting) in local areas.  As a result, algorithms are challenged to accurately capture both global and local dis- tortions to merge them into a proper quality prediction. Thirdly, compared to synthetic IQA databases, image con- tent variation, which is a typical challenge in IQA task, presents even more difﬁculty to authentic IQA databases. Existing synthetic IQA databases LIVE [34], TID2013 [32] and CSIQ [21] only include no more than 30 reference im- ages as limited in the sense of image contents, while au- thentic IQA database LIVE Challenge [8] and KonIQ-10k ent contents respectively.  This great content variation has raised a big challenge to the generalization ability of exist- ing IQA methods.
Due to distortion diversity and content variation, IQA for authentically distorted images still has not been well solved. As shown in Figure 1, the extracted features vary when images vary, leading to inconsistent quality predic- tions with mean opinion score (MOS). In previous work, neither handcrafted feature based approaches nor networks with shallow architectures, which both solve synthetic IQA tasks well, are able in handling realistic distortions.  This indicates low level features are not powerful enough in rep- resenting complicated distortion in real world. As a result, attempts have been made to use deep semantic features as quality descriptors:  deep models which are pretrained on classiﬁcation tasks are adopted to predict real world distor- tions. The hypothesis lying behind is that authentic distor- tions actually exist in photographically generated classiﬁca- tion databases such as ImageNet [7], and these pre-trained features are already, to some extent, quality aware.
Although these attempts achieved promising improve- ments, further efforts are still lacked. Speciﬁcally, there are two drawbacks exist by simply adopting network architec- tures, which are initially designed for learning how to rec- ognize objects, to the task of IQA. First, current deep mod- els only learn global features for classiﬁcation. For authen- tic IQA, however, distortions diverse in many ways, most of which exist in local areas.  Ignoring local patterns may lead to an inconsistency between predicted quality and hu- man visual perception, since human visual system (HVS) is sensitive to local distortions when the rest part of the im- age exhibits fairly good quality [21].  Secondly, as image content varies, the way human percepts quality of different objects varies. As illustrated in [22], an image of clear blue sky will be considered of high quality by human inspectors while mistaken by most of IQA methods to be a blurry im- age due to large ﬁattened area the image contains.  There- fore, directly predicting image quality before recognizing image content does not conform to the rule how humans perceive the world. In HVS, the top-down perception model indicates that human tries to understand the image before paying attention to other relevant sub-tasks such as qual- ity assessment.  However, in current models, fusing IQA task into semantic recognition network forces the network to learn image content and quality simultaneously, while it is more properly to let the network learn how to judge image quality after it has recognized the image content.
In this paper, we aim at developing an authentic IQA approach by considering the above two challenges which often appear at real world images: distortion diversity and content variation. We propose a local distortion aware mod- ule to extract local features from multi-scale to handle dis- tortion diversity, and we introduce a hyper network archi-tecture which dynamically generates weights for a quality prediction network to cover wide content variation.  In our method, the proposed hyper network can adaptively learn the rule for perceiving quality according to its recognized content, and the target network follows this manner to give a ﬁnal quality prediction.  By judging quality based upon image content, the network is supposed to give predictions which are more consistent with human perception. In gen- eral, the main contributions of the proposed method can be summarized into three-folds:
•  To enhance the ability of assessing image in the wild, we propose a novel IQA model based on hyper net- work which adaptively adjusts the quality prediction parameters.   The  proposed  network predicts image quality in a content-aware manner, and the perception after recognition procedure is more consistent with the way how human realizes the world.
•  Since  local  features  are  beneﬁcial  to  handle  non- uniform distortions in the image, we introduce a lo- cal distortion aware module to further capture image quality.  We aggregate both local distortion features and global semantic features for gathering ﬁne-grained details and holistic information, image quality is then predicted upon this multi-scale representation.
•  Experimental results demonstrate that our approach not only outperforms the other competitors on authen- tic IQA databases, but also achieves competing results on synthetic IQA databases, despite we did not specif- ically design our model to extract synthetic features. This indicates the powerfulness and generalizability of our proposed model.

# 2. Related Work
2.1. IQA for Synthetically Distorted Images
In the past decades, great efforts have been put into the ﬁeld of synthetic IQA, the approaches follow either of the two categories:  hand-crafted feature based IQA  and learning feature based IQA. Hand-crafted feature based ap- proaches generally utilize NSS models to capture distor- tion.   By modeling scene statistics which is sensitive to the appearance of distortion,  degradation level of qual- ity can be detected and quantiﬁed.  These quality aware natural scene parameters include discrete wavelet coefﬁ- cients [30], the correlation coefﬁcients across subbands [1], DCT coefﬁcients [33], locally normalized luminance co- efﬁcients with their pairwise products [29], image gradi- ent, log-Gabor responses and color statistics [3].  Distribu- tion models used to capture the statistics from synthetically distorted image include generalized Gaussian distribution (GGD) [29,30], asymmetric generalized Gaussian distribu- tion (AGGD) [3, 29], Weibull distribution [3], third order polynomial tting [33] and histogram counting [38].  These hand-crafted features, however, require expertly design and are time-consuming.  In addition, scene statistic features represent image quality from a global view, thus are not able to measure local distortions which commonly appear in authentically distorted images.
Inspired by the successes of machine learning in many computer vision tasks [9, 10, 39, 40], some learning based approaches are also proposed. In the early stage, codebook based learning approaches are introduced [37, 42, 43, 45]. Due to their strong learning power, CNN based methods are then proposed and achieved signiﬁcant progress in synthetic IQA. In [14], a simple CNN with pooling strategy inherited from [43] is used for quality prediction. Ma et al. [27] pro- posed a deeper network to learn distortion type and image quality simultaneously. In [16,23,31], error map of the dis- torted image is learned to guide quality prediction, the ap- proaches learning error map include training with residual error [16], with quality map calculated from FR-IQA meth- ods [31] and with GAN generated image references [23]. Noticing the limited size of training data from existing IQA databases, [24] and [26] proposed to generate vast train- ing samples by labeling their quality rank instead of quality score. Siamese network [5] and RankNet [4] architectures are used respectively to learn the rank of images.
Although these IQA methods have achieved great per- formance improvement on synthetic databases, challenge exists when facing large scale data [25, 28], indicating the problem of content variation still not well managed. It has also been shown that IQA models perform well on syn- thetic databases give inaccurate predictions on authentic IQA databases, suggesting the features of diverse distortion types exist in the wild can not be easily captured by archi- tectures designed for extracting synthetic distortions.
2.2. IQA for Authentically Distorted Images
While most of the IQA models concentrate on synthet- ically distorted images, there are relatively few works fo- cusing on the more challenging problem of authentic IQA. With the assistant of deep learning, deep semantic features are shown effective in representing image quality.  In [17], Kim et al. showed that deep features from AlexNet [20] and ResNet [12] pretrained on classiﬁcation databases such as ImageNet exhibit strong relationship with perceived quality and achieved standout accuracies. In [13], more pretrained baseline networks are tested, results conﬁrmed the power of semantic features in solving IQA problem in the wild. In [46], a two stream network architecture is introduced to both predict synthetic and authentic image distortions.  In their work, the authentic quality prediction stream adopted VGG-16 [35] for feature extraction. In [22], Li et al. pro- posed to use statistics from ResNet50 features of multi- patches for quality prediction.  Recently, Zhang et al. [47]


proposed to use image pairs both in synthetic and authentic databases for training IQA model, and the backbone used for feature extraction is ResNet-34. As can be seen, current models directly use output features from semantic learning networks for quality prediction, there are, however, mainly two drawbacks lying behind: ﬁrst, mixing semantic learn- ing and quality prediction in one network ignores how im- age semantics inﬁuence the way of quality perception, yet in HVS, image quality is judged after image content is rec- ognized.  Second, as deep semantic features are extracted from global scale, local distortions, which commonly exists in graphically obtained images, are ignored.  As a result, networks are not able to capture detailed quality in an im- age, leading to inaccurate predictions.
In this work, we propose a novel multi-scale feature fused hyper network architecture to predict image quality in the wild.  While previous models mix semantic under- standing and quality prediction in one task, we divide the quality prediction procedure into two steps: image semantic features are learned ﬁrst and quality is predicted based upon what content the image delivers. This procedure follows the top-down perception ﬁow of humans, and we design a hy- per network connection to mimic this mapping from image content to the manner of perceiving quality.  In addition, instead of simply using global semantic features for con- tent understanding, we also propose to fuse local distortion features from multi-scale to better represent image quality. In this way, our quality prediction procedure becomes self- adaptive, content-aware and capable of capture both detail and holistic information from the image.
# 3. Proposed Method
In this study, we aim at developing a quality assessment network which adaptively predicts image quality according to image content. The architecture of our network is shown in Figure 2. The proposed network consists of three parts: a backbone network which extracts semantic features, a tar- get network which predicts image quality and a hyper net- work which generates a series of self-adaptive parameters for the target network. We will introduce our self-adaptive IQA model ﬁrst and then present details of the three sub- networks in the following.
3.1. Self-Adaptive IQA Model
Traditional deep learning based quality prediction mod- els receive an input image and directly map it to a quality score, the procedure can be described as follows:
ϕ(x,θ) = q,                              (1)
where ϕ denotes the network model, x is the input image, θ represents the weight parameters. Note that once the train- ing stage completes, weight parameters are ﬁxed for all test images. This prediction model implies that the same kind of quality features are extracted for predicting diverse im- ages. In practical, however, as image contents vary, using the same rule for predicting varies images’ quality is not thorough to cover their differently exhibited structures. As illustrated in [22], humans will take an image of clear blue sky as high quality, while for quality prediction models, this picture is most likely to be regarded as a blur contaminated one due to large ﬁatten area it contains. The reason for this mistaken prediction is the ignorance of image semantic. For humans, under the condition of understanding image con- tent, corresponding rules are then used to judge image qual- ity.  Therefore, to mimic the perception procedure of hu- mans, we model the task of IQA as follows:
ϕ(x,θx ) = q,                             (2)
where network parameters θx  are dependent on the image itself instead of being ﬁxed for all inputs. For easier under- standing, parameters θx can be regarded as quality perceiv- ing rules.  As image content varies, the way of perceiving image quality varies. In this way, our IQA model becomes self-adaptive as it extracts different quality indicators with respect to different contents.  Ideally, one can train images of the same content with an individual network for quality prediction with more ﬁexibility, however, training a set of networks covering such widely spread contents is computa- tion inefﬁcient and not practical.  Therefore, we introduce hyper network to simplify this problem:
θx  = H(S(x),γ),                          (3)
where H stands for a hyper network mapping function and γ represents hyper network parameters.  We deﬁne the in-


put of hyper network as S(x), meaning semantic features extracted from the input image x. Thus the function of hy- per network is to learn the mapping from image content to the rule of how to judge image quality.  The learned per- ception rule will further guide our target network to extract self-adaptive quality features for prediction.
By introducing the intermediate variable θx  and hyper network, we actually divide the task ofIQA into three steps: semantic feature extraction, perception rule establish- ment and quality prediction. We use a backbone network to extract image semantic features S(x), a hyper network to learn the quality perception rule θx and a quality prediction target network to obtain the ﬁnal quality score q. Unlike the quality prediction model in Equation (1), where image qual- ity is directly estimated without semantic understanding or content recognition, our proposed model follows the top- down perception mechanism as it tries to understand image in the ﬁrst place, until when it executes the task of qual- ity judgement.  This designation makes our network more ﬁexible in extracting quality inﬁuential factors when facing content varying images.  In addition, the proposed quality prediction procedure is also more consistent with the way how human perceives image quality.
In order to reduce the amount of target network parame- ters θx and also for easier training, we simplify the input of the target network to a content aware vector vx  = Sms (x), where Sms  stands for the meaning that the content aware vector is also extracted by the backbone semantic extrac- tion network, but fuses multi-scale features to capture local distortions in the image.  Under this alteration, the whole hyper network based IQA model can be described as:
ϕ(vx , H(S(x),γ)) = q.                     (4)
Based on the quality prediction model, we then present the architecture of the three sub-networks in the following.
3.2. Semantic Feature Extraction Network
As shown in Figure 2, the front part ofour network archi- tecture is a common semantic feature extraction network. The semantic extraction network focuses on understanding image content, and outputs two streams of features for qual- ity prediction. The semantic feature S(x) is directly fed to hyper network for weight generation, and the multi-scale content feature stream Sms (x) is treated as the input of the target network. The reason why we extract multi-scale con- tent features is that semantic features extracted from the last layer merely represent holistic image content.  In order to capture local distortions in real world, we propose to extract multi-scale features through a local distortion aware mod- ule.  As illustrated in Figure 3, our designed local distor- tion aware module consists of a series of operations includ- ing dividing multi-scale feature maps into non-overlapping patches, stacking the patches along the channel dimension, conducting 1×1 convolution and globally average pooling them into vectors. The proposed module can be regarded to serve as an attention based patch extractor, which is aware of feature patches corresponding to local distortions for bet- ter capturing its quality.
Speciﬁcally,  we use ResNet50  [12]  as the backbone model for  semantic  feature  extraction.    The  pretrained model on ImageNet [7] is used for network initialization. In our network, the last two layers of the origin ResNet50, i.e. an average pooling layer and a fully connected layer are removed to output feature stream.  We extract multi-scale features from conv2   10, conv3   12, conv4   18 layers as the input to the local distortion aware module, which outputs multi-scale content vector vx.


3.3. Hyper Network for Learning Perception Rule
Inspired by [19], our hyper network consists of three 1×1  convolution  layers  and  several weight  generating branches.  Since in the proposed network, fully connected layers are used as basic target network component (see Sec- tion 3.4), two types of network parameters, i.e.  fully con- nected layer weights and biases, should be generated.  For different types of parameters, we use different weight gen- erating approaches. Fully connected layer weights are gen- erated from convolution followed by reshape operation of extracted features, while fully connected layer biases are generated by simply average pooling and fully connection, as bias weights have much less amount of parameters. The output channels of convolution and fully connected layers are decided based upon dimensions of corresponding layers in target network for size match. The generated weights are regarded as the rule of perceiving image quality and will further instruct target network for predicting image quality.
3.4. Target Network for Quality Prediction
As multi-scale features extracted by the semantic extrac- tion network are content-aware, the function of the target network is simply mapping learned image contents to a quality score. Therefore, we use a small and simple network for quality prediction.  As shown in Figure 2, our target network consists of four fully connected layers, it receives multi-scale content feature vector as input, and propagates through weight determined layers to get the ﬁnal quality score.  In the target network, we choose sigmoid function as the activation function.
3.5. Implementation Details
We implemented our model by PyTorch and conducted training and testing on the NVIDIA 1080Ti GPUs.  Fol- lowing the training strategy from [17], we randomly sam- ple and horizontally ﬁipping 25 patches with size 224×224 pixels from each training image for augmentation. Training patches inherited quality scores from the source image, and we minimize L1 loss over the training set:

where pi   and Qi  refers to the i-th training patch and the ground truth score, respectively. We used Adam [18] opti- mizer with weight decay 5  × 10 —4  to train our model for 15 epochs, with mini-batch size of 96. Learning rate is ﬁrst set to 2 × 10 —5, and reduced by 10 after every 5 epochs. For faster convergence, un-pretrained layers of our model, which are Xavier initialized, applied learning rate 10 times larger. During testing stage, 25 patches with 224×224 pix- els from test image are randomly sampled and their cor-responding prediction scores are average pooled to get the ﬁnal quality score.

# Experiments
4.1. Datasets
We used three authentically distorted image databases including LIVE Challenge (LIVEC) [8], KonIQ-10k [13] and BID [6] for evaluation.  LIVEC contains 1162 images taken from different photographers with varies camera de- vices in the real world, hence these images contain complex and composite distortions.  KonIQ-10k consists of 10073 images which are selected from the large public multime- dia database YFCC100m [36], the sampled images try to cover a wide and uniform quality distribution in the sense of brightness, colorfulness, contrast and sharpness. BID is a blur image database containing 586 images with realistic blur distortions such as motion blur and out of focus, etc.
Except for authentic image databases, we also tested our model on synthetic image databases LIVE [34] and CSIQ [21]. There are 779 and 866 synthetically distorted images included in each database.
4.2. Evaluation Metrics
Two commonly used criteria, Spearman’s rank order cor- relation coefﬁcient (SRCC) and Pearson’s linear correla- tion coefﬁcient (PLCC) are adopted to measure prediction monotonicity and prediction accuracy. The two criteria both range from 0 to 1 and a higher value indicates better perfor- mance. Before calculating PLCC, logistic regression is ﬁrst applied to remove nonlinear rating caused by human visual observation, as suggested in the report from Video Quality Expert Group (VQEG) [11].
For each database, 80% images are used for training and the rest 20% are used for testing.  For synthetic image databases LIVE and CSIQ, the split is implemented accord- ing to reference images to avoid content overlapping.  We run 10 times of this random train-test splitting operation and the median SRCC and PLCC values are reported.
4.3. Comparison with the State-of-the-art Methods
Eight state-of-the-art BIQA methods are selected for per- formance comparison. The comparison methods including hand-crafted based approaches [3, 29, 37],  deep learning based synthetic IQA approaches [2, 15] and deep learning
based authentic IQA approaches [22,44,46].
Single database evaluations.  We ﬁrst analyze experi- mental results on single databases. As shown in Table 1, our approach outperforms all the state-of-the-art methods on all three authentic image databases (LIVEC, BID and KonIQ- 10k) for both SRCC and PLCC evaluations. This suggests that learning image content ﬁrstly assists in perceiving im- age quality, when image data covers a wide range of variety.
Though we did not especially add modules for synthetic image feature extraction, our approach achieved compet- ing performance with the state-of-the-art methods on two synthetic image databases LIVE and CSIQ. Note that com- pared with PQR and SFA, which also utilize backbone clas- siﬁcation networks to extract deep semantic features, our approach signiﬁcantly outperforms PQR on CSIQ database and outperforms SFA on both LIVE and CSIQ dataset.
We further present performance comparison of our ap- proach on individual distortion types. Since distortion types are of high diversity on authentic image databases,  we only evaluate the performance on synthetic image databases LIVE and CSIQ, as shown in Table 2. Compared with other methods which introduce speciﬁc module to handle syn- thetic IQA task, our proposed method uses a simply net- work to obtain competing performances on individual dis- tortion types.  This proved that the effectiveness of image content understanding based IQA method.
Generalization ability test. We ﬁrst run cross database tests for performance comparison, the tests are conducted on intra databases belonging to either authentic or syn- thetic distortions.  We select the most competing two ap- proaches PQR and DBCNN for comparison, and the re- sults are shown in Table 3.   Among six authentic cross database tests, our approach achieves four times of top per- formance. For synthetic cross database tests, our approach still performs competitively to other algorithms, indicating the strong generalization power of our approach.
To further evaluate the generalization ability of our ap- proach, we train competing models on the whole LIVE

Dataset and test them on the large scale synthetic database Waterloo Exploration Database [25].  Firstly, three testing criteria, D-Test, L-Test and P-Test are calculated, which re- spectively measure pristine-distortion discriminability, con- sistency with distortion levels and pairwise quality discrim- inability. As shown in Table 4, our approach achieves com- peting performance, though it is not speciﬁcally designed for synthetically distorted IQA.
Then, we conducted gMAD competition [28] on the Wa- terloo Database for a direct visualization. gMAD efﬁciently selects image pairs with maximum quality difference pre- dicted by an attacking IQA model to challenge an other de-fending model which considers them are of the same level of quality. The selected pairs are shown to the observer to determine whether the attacker or the defender is robust. In Figure 4, we ﬁx our model as a defender in the ﬁrst two columns, image pairs selected from a bad quality level and a good quality level are presented respectively.  In the last two columns, our model attacks other competing methods where each column represents images selected from a bad and a good quality level predicted by the defender.
As can be seen from Figure 4, when our model plays as defender, image pairs selected by the attacker do not vary much in perceived quality, while our model successively selects image pairs with huge quality difference when acts as attacker.  This indicates our model is both powerful in defending and attacking. In addition, it is worth mention- ing that our model successfully recognizes high quality im-ages with ﬁatten contents, despite they deceive the defend- ing models to have low quality (the image “sky” and the image “sunset” on the third column). These results further demonstrate that our proposed model has a strong general- ization ability regarding the challenge of content variation. gMAD results against more IQA models can be found in our supplementary material.
4.4. Visualization of Self-Adaptive Weights
In order to verify the effectiveness of the weight generat- ing procedure, we extract generated weights of the quality prediction network from several images of varies contents. We then do PCA transform to the weights and plot them in a 3D space for visualization. In Figure 5, we plot transformed weights from the ﬁrst layer of the target network, weights from other layers also show similar characteristics.  From Figure 5, several interesting ﬁndings can be discovered:
First, for images of different contents, the generated weights vary.  This indicates our network adopts distinct weights for evaluating image quality in a self-adaptive man- ner.  Whereas for traditional IQA models, weights of the model are ﬁxed for all input images, which will coincide in the same location in the weight space if we plot them.
Second,  images  of the  same  object  generate  similar weights despite they exhibit distinct levels of quality.  As can be seen from Figure 5, though their quality varies, images of the same class “dog” or “vase” generate  sim- ilar weights for quality prediction.  This veriﬁes that our model successfully learns image contents to instruct quality prediction.  We believe this predicting after understanding scheme makes our model self-adaptive, thus is able to eval- uate image quality more ﬁexibly and more precisely when facing the challenge from a large diversity of images.
Third, for ﬁatten images “snow footprint” and “sky”, the corresponding weights distinguish from each other.  This suggests our network indeed learns to understand high-level


Table 5. Ablation results on LIVE Challenge and LIVE databases.

Components	LIVE Challenge	LIVE
	SRCC	PLCC	SRCC	PLCC
Res50	0.827	0.852	0.923	0.947
Res50+MS	0.836	0.859	0.954	0.963
Res50+Hyp	0.854	0.879	0.944	0.959
Res50+MS+Hyp	0.859	0.882	0.962	0.966


image content though they exhibit similar low-level quality indicators such as smoothness. Therefore, our model is pre- vented from mistaking image quality due to content vari- ation, such as confusing a ﬁatten image with blurriness or mistaking an image abundant of textures to a noisy one.
4.5. Ablation Study
To evaluate the efﬁciency of our proposed components, we conduct several ablation experiments on the LIVEC and LIVE database.  We ﬁrst use a pretrained ResNet50 with ﬁne-tuning as our backbone model and analyze the effect of each individual component by comparing both SRCCs and PLCCs. The results are shown in Table 5.
We ﬁrst examine the effectiveness of our proposed lo- cal distortion aware module by concatenating them with ResNet50 output features (Res50+MS). The SRCC slightly improved on the LIVE Challenge Database and obviously improved on the LIVE Database with around 1.6% and 3%.
Then, we add hyper network and target network mod- ule to the backbone network.  The input and weights of the target network are both from the last feature layer of ResNet50.  By modifying the hyper network to our pro- posed architecture, we can see major SRCC and PLCC im- provements on both LIVE Challenge and LIVE databases. On LIVE Challenge, SRCC and PLCC increased both 2.7% and on LIVE, they increase 2.1% and 1.2% respectively.
At last, we add multi-scale features to the target net- work’s input, and SRCC and PLCC further improved to the highest value of 85.9%, 88.2% on the LIVE Challenge Database and 96.2% and 96.6% on the LIVE Database.
# 5. Conclusion
In this paper, we propose a novel network to overcome two challenging problems that appear in the task of au- thentic IQA: distortion diversity and content variation. The proposed network separates quality prediction from content understanding to mimic how humans perceive image qual- ity. We employ hyper network architecture to accomplish this perception ﬁow, and further introduce a multi-scale local distortion aware module to capture complex distor- tions.  Experimental results showed that our proposed ap- proach possesses strong generalization ability which offers the prospect of more extensive applications of IQA task.
