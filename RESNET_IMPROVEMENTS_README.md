# ResNet50 + Improvements å®éªŒ

## ğŸ¯ **å®éªŒç›®çš„**

éªŒè¯æˆ‘ä»¬çš„æ”¹è¿›ï¼ˆMulti-scale Feature Fusion + Channel Attentionï¼‰æ˜¯å¦å¯¹CNN backboneï¼ˆResNet50ï¼‰ä¹Ÿæœ‰æ•ˆï¼Œä»è€Œè¯æ˜æ–¹æ³•çš„æ™®é€‚æ€§ã€‚

---

## ğŸ“ **æ–‡ä»¶è¯´æ˜**

### **æ ¸å¿ƒä»£ç **ï¼š
- **`models_resnet_improved.py`** - ResNet50æ”¹è¿›ç‰ˆæ¨¡å‹
  - åŒ…å«3ä¸ªé…ç½®ï¼šBaseline, +Multi-scale, +Multi-scale+Attention
  - å®Œå…¨å…¼å®¹åŸå§‹HyperIQAçš„TargetNetè®¾è®¡
  
- **`train_resnet_improved.py`** - è®­ç»ƒè„šæœ¬
  - æ”¯æŒæ‰€æœ‰3ä¸ªé…ç½®
  - ä½¿ç”¨ä¸SMART-IQAç›¸åŒçš„è®­ç»ƒç­–ç•¥
  
- **`run_resnet_ablation.sh`** - ä¸€é”®è¿è¡Œ3ä¸ªæ¶ˆèå®éªŒ
  - è‡ªåŠ¨åŒ–è¿è¡Œæ‰€æœ‰å®éªŒ
  - è‡ªåŠ¨æå–ç»“æœ
  - çº¦4.5å°æ—¶å®Œæˆ

---

## ğŸš€ **å¿«é€Ÿå¼€å§‹**

### **æ–¹æ³•1ï¼šè¿è¡Œå®Œæ•´æ¶ˆèå®éªŒï¼ˆæ¨èï¼‰**

```bash
# ä¸€é”®è¿è¡Œ3ä¸ªå®éªŒ
bash run_resnet_ablation.sh
```

è¿™å°†ä¾æ¬¡è¿è¡Œï¼š
1. ResNet50 Baseline (Single-scale, No attention)
2. ResNet50 + Multi-scale
3. ResNet50 + Multi-scale + Attention

### **æ–¹æ³•2ï¼šå•ç‹¬è¿è¡ŒæŸä¸ªå®éªŒ**

```bash
# Baseline
python3 train_resnet_improved.py \
    --dataset koniq-10k \
    --data_path ./koniq-10k \
    --epochs 10 \
    --lr 1e-4 \
    --batch_size 32 \
    --no_color_jitter \
    --test_random_crop \
    --save_model

# + Multi-scale
python3 train_resnet_improved.py \
    --dataset koniq-10k \
    --use_multiscale \
    --epochs 10 \
    --lr 1e-4 \
    --save_model

# + Multi-scale + Attention
python3 train_resnet_improved.py \
    --dataset koniq-10k \
    --use_multiscale \
    --use_attention \
    --epochs 10 \
    --lr 1e-4 \
    --save_model
```

---

## ğŸ“Š **é¢„æœŸç»“æœ**

### **å‡è®¾1ï¼šæ”¹è¿›æœ‰æ•ˆï¼ˆä¹è§‚ï¼‰**
```
ResNet50 Baseline            0.8998  (å·²æµ‹å¾—)
ResNet50 + Multi-scale       0.9050  (+0.52%)
ResNet50 + Multi + Attention 0.9120  (+1.35%)
```
**ç»“è®º**ï¼šæ”¹è¿›å…·æœ‰æ™®é€‚æ€§ âœ…

### **å‡è®¾2ï¼šæ”¹è¿›æœ‰é™ï¼ˆä¸­æ€§ï¼‰**
```
ResNet50 Baseline            0.8998
ResNet50 + Multi-scale       0.9010  (+0.13%)
ResNet50 + Multi + Attention 0.9040  (+0.47%)
```
**ç»“è®º**ï¼šSwinçš„å±‚æ¬¡åŒ–ç‰¹å¾æ›´é€‚åˆæˆ‘ä»¬çš„æ–¹æ³• âœ…

### **å‡è®¾3ï¼šæ”¹è¿›æ— æ•ˆï¼ˆæ‚²è§‚ï¼‰**
```
ResNet50 Baseline            0.8998
ResNet50 + Multi-scale       0.8995  (-0.03%)
ResNet50 + Multi + Attention 0.9000  (+0.02%)
```
**ç»“è®º**ï¼šæ”¹è¿›ä¸“ä¸ºTransformerè®¾è®¡ï¼Œéœ€è¦hierarchical features âœ…

**ä¸‰ç§ç»“æœéƒ½æœ‰è®ºæ–‡ä»·å€¼ï¼**

---

## ğŸ”§ **æ¨¡å‹æ¶æ„**

### **é…ç½®1ï¼šBaseline**
```
ResNet50 (pretrained) â†’ Stage 4 only â†’ HyperNet â†’ TargetNet â†’ Score
```
- å‚æ•°é‡ï¼š25.62M
- ä¸åŸå§‹HyperIQAç›¸åŒ

### **é…ç½®2ï¼š+ Multi-scale**
```
ResNet50 â†’ [Stage 1,2,3,4] 
         â†’ Adaptive Pool (7Ã—7)
         â†’ Conv 1Ã—1 + BN + ReLU
         â†’ Concatenate
         â†’ HyperNet â†’ TargetNet â†’ Score
```
- å‚æ•°é‡ï¼š28.12M
- èåˆ4ä¸ªstageçš„features

### **é…ç½®3ï¼š+ Multi-scale + Attention**
```
ResNet50 â†’ [Stage 1,2,3,4] 
         â†’ Adaptive Pool (7Ã—7)
         â†’ Conv 1Ã—1 + BN + ReLU
         â†’ Channel Attention (weights: w1,w2,w3,w4)
         â†’ Weighted Concatenate
         â†’ HyperNet â†’ TargetNet â†’ Score
```
- å‚æ•°é‡ï¼š28.65M
- åŠ¨æ€åŠ æƒfusion

---

## ğŸ“ **å®éªŒè®¾ç½®**

### **è¶…å‚æ•°**ï¼ˆä¸åŸå§‹ResNet50 baselineä¸€è‡´ï¼‰ï¼š
- Learning Rate: 1e-4
- Batch Size: 32
- Epochs: 10
- Train Patches: 25
- Test Patches: 25
- ColorJitter: âŒ Disabled
- Test Crop: RandomCrop âœ…
- Weight Decay: 1e-4
- Dropout: 0.3

### **æ•°æ®é›†**ï¼š
- KonIQ-10k
- Train: 7,046 images
- Test: 2,010 images

---

## ğŸ“‚ **è¾“å‡ºæ–‡ä»¶**

### **æ—¥å¿—æ–‡ä»¶**ï¼š
```
logs/resnet_ablation_YYYYMMDD_HHMMSS/
â”œâ”€â”€ exp1_baseline.log               # Baselineå®éªŒæ—¥å¿—
â”œâ”€â”€ exp2_multiscale.log             # Multi-scaleå®éªŒæ—¥å¿—
â””â”€â”€ exp3_multiscale_attention.log   # å®Œæ•´æ”¹è¿›å®éªŒæ—¥å¿—
```

### **æ¨¡å‹æƒé‡**ï¼š
```
checkpoints/
â”œâ”€â”€ resnet_improved_ss_noatt_best.pth   # Baseline
â”œâ”€â”€ resnet_improved_ms_noatt_best.pth   # + Multi-scale
â””â”€â”€ resnet_improved_ms_att_best.pth     # + Multi-scale + Attention
```

---

## ğŸ§ª **æµ‹è¯•æ¨¡å‹**

```bash
# æµ‹è¯•æ¨¡å‹forward pass
python3 models_resnet_improved.py
```

è¿™å°†æµ‹è¯•æ‰€æœ‰3ä¸ªé…ç½®ï¼Œè¾“å‡ºï¼š
- å‚æ•°é‡
- Forward passæˆåŠŸä¸å¦
- Attention weightsï¼ˆå¦‚æœæœ‰ï¼‰

---

## ğŸ“Š **ä¸SMART-IQAçš„å¯¹æ¯”**

| æ¨¡å‹ | Backbone | Multi-scale | Attention | Params | SRCC (é¢„æœŸ) |
|-----|---------|------------|-----------|--------|------------|
| **HyperIQA** | ResNet50 | âŒ | âŒ | 25M | 0.8998 |
| **ResNet+Ours** | ResNet50 | âœ… | âœ… | 28.7M | **0.90-0.91?** |
| **SMART-IQA** | Swin-Base | âœ… | âœ… | 88M | **0.9378** âœ… |

**å…³é”®é—®é¢˜**ï¼šResNet+æ”¹è¿› vs SMART-IQAçš„å·®è·æ˜¯å¤šå°‘ï¼Ÿ
- å¦‚æœå·®è·å¤§ï¼ˆ0.91 vs 0.94ï¼‰â†’ Swinçš„å±‚æ¬¡åŒ–ç‰¹å¾æ˜¯å…³é”®
- å¦‚æœå·®è·å°ï¼ˆ0.92 vs 0.94ï¼‰â†’ æ”¹è¿›æœ¬èº«è´¡çŒ®æ›´å¤§

---

## ğŸ’¡ **è®ºæ–‡ä¸­çš„å‘ˆç°**

### **å¦‚æœç»“æœå¥½ï¼ˆ+1-2%ï¼‰**ï¼š
```latex
We verify the generality of our improvements by applying them to ResNet50.
Results show +1.35% improvement, demonstrating that our method benefits
CNN backbones. However, Swin Transformer achieves +3.80% improvement,
suggesting that hierarchical vision features are more suitable for
quality-aware multi-scale fusion.
```

### **å¦‚æœç»“æœä¸€èˆ¬ï¼ˆ+0.3-0.5%ï¼‰**ï¼š
```latex
To understand the contribution of backbone architecture, we apply our
improvements to ResNet50. The gain (+0.47%) is much smaller than with
Swin Transformer (+3.80%), indicating that hierarchical, self-attention
based features are crucial for our method's success.
```

### **å¦‚æœç»“æœä¸å¥½ï¼ˆ<0.3%ï¼‰**ï¼š
```latex
Interestingly, applying the same improvements to ResNet50 shows minimal
gains (<0.3%), while Swin Transformer benefits significantly (+3.80%).
This suggests that our multi-scale attention mechanism specifically
leverages the hierarchical, window-based features of vision transformers.
```

---

## â±ï¸ **é¢„è®¡æ—¶é—´**

- **å•ä¸ªå®éªŒ**: ~1.5å°æ—¶
- **å®Œæ•´æ¶ˆè**: ~4.5å°æ—¶
- **æµ‹è¯•æ¨¡å‹**: <1åˆ†é’Ÿ

---

## ğŸ¯ **å»ºè®®**

1. **å…ˆè¿è¡Œå®Œæ•´æ¶ˆè**ï¼š`bash run_resnet_ablation.sh`
2. **ç­‰å¾…ç»“æœ**ï¼šçº¦4.5å°æ—¶
3. **åˆ†æç»“æœ**ï¼šå¯¹æ¯”3ä¸ªé…ç½®çš„SRCC
4. **æ›´æ–°è®ºæ–‡**ï¼šæ ¹æ®ç»“æœé€‰æ‹©åˆé€‚çš„å‘ˆç°æ–¹å¼

---

## ğŸ“§ **é—®é¢˜æ’æŸ¥**

### **CUDA Out of Memory**ï¼š
```bash
# å‡å°batch size
python3 train_resnet_improved.py --batch_size 16 ...
```

### **DataLoaderé”™è¯¯**ï¼š
```bash
# ç¡®ä¿data_loader.pyå·²æ›´æ–°
# ç¡®ä¿koniq-10kæ•°æ®é›†è·¯å¾„æ­£ç¡®
```

### **æ¨¡å‹æµ‹è¯•å¤±è´¥**ï¼š
```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python3 models_resnet_improved.py
```

---

## âœ… **æ£€æŸ¥æ¸…å•**

- [x] æ¨¡å‹ä»£ç å®Œæˆï¼ˆ`models_resnet_improved.py`ï¼‰
- [x] è®­ç»ƒè„šæœ¬å®Œæˆï¼ˆ`train_resnet_improved.py`ï¼‰
- [x] æ‰¹å¤„ç†è„šæœ¬å®Œæˆï¼ˆ`run_resnet_ablation.sh`ï¼‰
- [x] æ¨¡å‹æµ‹è¯•é€šè¿‡
- [ ] è¿è¡Œå®éªŒ
- [ ] æå–ç»“æœ
- [ ] æ›´æ–°è®ºæ–‡

---

**åˆ›å»ºæ—¥æœŸ**: 2024-12-24  
**çŠ¶æ€**: âœ… ä»£ç å®Œæˆï¼Œç­‰å¾…è¿è¡Œå®éªŒ

