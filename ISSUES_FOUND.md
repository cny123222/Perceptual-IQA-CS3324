# åœ¨å½“å‰åˆ†æ”¯å‘ç°çš„é—®é¢˜

## åˆ†æ”¯ä¿¡æ¯
- **åˆ†æ”¯**: `fix-training-issue` (åŸºäº `a4d1eda017d8ac9a8a04c62d73593ae6e6f77b92`)
- **çŠ¶æ€**: å·²ç¡®è®¤SRCC/PLCCè®¡ç®—é€»è¾‘ä¸åŸå§‹è®ºæ–‡ä¸€è‡´

---

## å‘ç°çš„é—®é¢˜

### ğŸ”´ é—®é¢˜1: Backboneå­¦ä¹ ç‡ä¸è¡°å‡

**ä½ç½®**: `HyerIQASolver.py:122-129`

**ä»£ç **:
```python
lr = self.lr / pow(10, (t // 6))  # åªæœ‰hypernetçš„lrä¼šè¡°å‡
if t > 8:
    self.lrratio = 1
self.paras = [
    {'params': self.hypernet_params, 'lr': lr * self.lrratio},  # ä¼šè¡°å‡
    {'params': self.model_hyper.res.parameters(), 'lr': self.lr}  # âŒ æ°¸è¿œä¸ä¼šè¡°å‡ï¼
]
self.solver = torch.optim.Adam(self.paras, weight_decay=self.weight_decay)
```

**é—®é¢˜**:
- HyperNetworkçš„å­¦ä¹ ç‡ä¼šè¡°å‡ï¼ˆæ¯6ä¸ªepochè¡°å‡10å€ï¼‰
- **Backbone (ResNet) çš„å­¦ä¹ ç‡å§‹ç»ˆä¿æŒä¸ºåˆå§‹å€¼ `self.lr`**
- è¿™æ„å‘³ç€Backboneåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¸€ç›´ç”¨**é«˜å­¦ä¹ ç‡**æ›´æ–°
- å¯¼è‡´Backboneè¿‡åº¦æ›´æ–°ï¼Œç ´åäº†é¢„è®­ç»ƒç‰¹å¾ï¼Œæ³›åŒ–èƒ½åŠ›ä¸‹é™

**å½±å“**:
- Epoch 1: Backboneç‰¹å¾è¿˜æ¯”è¾ƒæ¥è¿‘é¢„è®­ç»ƒæƒé‡ï¼Œæ³›åŒ–å¥½ âœ…
- Epoch 2+: BackboneæŒç»­è¢«é«˜å­¦ä¹ ç‡æ›´æ–°ï¼Œåç¦»é¢„è®­ç»ƒæƒé‡ï¼Œæ³›åŒ–å˜å·® âŒ

---

### ğŸ”´ é—®é¢˜2: ä¼˜åŒ–å™¨çŠ¶æ€è¢«é‡ç½®

**ä½ç½®**: `HyerIQASolver.py:129`

**ä»£ç **:
```python
self.solver = torch.optim.Adam(self.paras, weight_decay=self.weight_decay)
```

**é—®é¢˜**:
- æ¯ä¸ªepochç»“æŸåï¼Œéƒ½é‡æ–°åˆ›å»ºoptimizer
- **Adamçš„momentum buffersï¼ˆä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡ï¼‰è¢«æ¸…ç©º**
- è¿™æ„å‘³ç€æ¯ä¸ªepochéƒ½æ˜¯"ä»å¤´å¼€å§‹"çš„Adamä¼˜åŒ–ï¼Œå¤±å»äº†å†å²æ¢¯åº¦ä¿¡æ¯
- å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š

**æ­£ç¡®åšæ³•**:
- åº”è¯¥åªæ›´æ–°learning rateï¼Œè€Œä¸æ˜¯é‡æ–°åˆ›å»ºoptimizer
- ä½¿ç”¨ `optimizer.param_groups[i]['lr'] = new_lr` æ¥æ›´æ–°å­¦ä¹ ç‡

---

### âš ï¸ é—®é¢˜3: filter() è¿­ä»£å™¨è€—å°½bug

**ä½ç½®**: `HyerIQASolver.py:40`

**ä»£ç **:
```python
self.hypernet_params = filter(lambda p: id(p) not in backbone_params, self.model_hyper.parameters())
```

**é—®é¢˜**:
- `filter()` è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œåªèƒ½ä½¿ç”¨ä¸€æ¬¡
- åœ¨ç¬¬ä¸€æ¬¡åˆ›å»ºoptimizeråï¼Œè¿­ä»£å™¨è¢«è€—å°½
- åç»­epoché‡æ–°åˆ›å»ºoptimizeræ—¶ï¼Œ`self.hypernet_params` æ˜¯ç©ºçš„
- è¿™ä¼šå¯¼è‡´åªæœ‰backboneå‚æ•°è¢«ä¼˜åŒ–ï¼Œhypernetworkå‚æ•°ä¸è¢«æ›´æ–°

**ä¿®å¤**:
```python
self.hypernet_params = list(filter(lambda p: id(p) not in backbone_params, self.model_hyper.parameters()))
```

---

## ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤1: Backboneå­¦ä¹ ç‡ä¹Ÿè¡°å‡

```python
# ä¿®å¤å
backbone_lr = self.lr / pow(10, (t // 6))  # Backbone LRä¹Ÿè¡°å‡
hypernet_lr = backbone_lr * self.lrratio
if t > 8:
    self.lrratio = 1
    hypernet_lr = backbone_lr
```

### ä¿®å¤2: ä¿æŒä¼˜åŒ–å™¨çŠ¶æ€

```python
# ä¿®å¤å
if t == 0:
    # First epoch: create optimizer
    self.paras = [
        {'params': self.hypernet_params, 'lr': hypernet_lr},
        {'params': self.model_hyper.res.parameters(), 'lr': backbone_lr}
    ]
    self.solver = torch.optim.Adam(self.paras, weight_decay=self.weight_decay)
else:
    # Subsequent epochs: only update learning rates
    self.solver.param_groups[0]['lr'] = hypernet_lr
    self.solver.param_groups[1]['lr'] = backbone_lr
```

### ä¿®å¤3: ä¿®å¤filter() bug

```python
# ä¿®å¤å
self.hypernet_params = list(filter(lambda p: id(p) not in backbone_params, self.model_hyper.parameters()))
```

---

## é¢„æœŸæ•ˆæœ

ä¿®å¤è¿™äº›é—®é¢˜åï¼Œé¢„æœŸï¼š
1. âœ… Backboneç‰¹å¾ä¸ä¼šè¿‡åº¦åç¦»é¢„è®­ç»ƒæƒé‡
2. âœ… ä¿æŒæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
3. âœ… è®­ç»ƒæ›´ç¨³å®š
4. âœ… æµ‹è¯•SRCCå¯èƒ½åœ¨åç»­epochä¿æŒç¨³å®šæˆ–ç»§ç»­æå‡

---

## ä¸‹ä¸€æ­¥

1. åº”ç”¨æ‰€æœ‰ä¸‰ä¸ªä¿®å¤
2. è¿è¡Œè®­ç»ƒï¼Œè§‚å¯Ÿæµ‹è¯•SRCCæ˜¯å¦åœ¨åç»­epochä¿æŒæˆ–æå‡
3. å¦‚æœä»æœ‰é—®é¢˜ï¼Œè€ƒè™‘æ·»åŠ Early Stopping

