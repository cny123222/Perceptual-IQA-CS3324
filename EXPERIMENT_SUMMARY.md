# å®éªŒæ€»ç»“æŠ¥å‘Š - Perceptual Image Quality Assessment

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

**ä»»åŠ¡**ï¼šæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNo-Reference Image Quality Assessmentï¼‰

**æ•°æ®é›†**ï¼šKonIQ-10k
- è®­ç»ƒé›†ï¼š7,046 å¼ å›¾åƒ
- æµ‹è¯•é›†ï¼š2,010 å¼ å›¾åƒ
- æ¯å¼ å›¾åƒåœ¨è®­ç»ƒæ—¶é‡‡æ · 20 ä¸ª patches (224Ã—224)

**ç›®æ ‡**ï¼šé¢„æµ‹å›¾åƒçš„ä¸»è§‚è´¨é‡è¯„åˆ†ï¼Œä½¿ç”¨ SRCC å’Œ PLCC ä½œä¸ºè¯„ä¼°æŒ‡æ ‡

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### **æ ¸å¿ƒæ¶æ„ï¼šSwin Transformer + HyperNet**

```
Input Image (224Ã—224)
    â†“
Swin Transformer Tiny (Backbone)
    â†“ (Multi-scale features)
[Stage 0: 96 channels]
[Stage 1: 192 channels]  â†’ Multi-scale Feature Fusion
[Stage 2: 384 channels]      (Concatenation + Conv)
[Stage 3: 768 channels]
    â†“
HyperNet (Meta-learner)
    â†“ (Dynamically generates)
Target Network Weights (FC layers)
    â†“
Quality Score
```

### **å…³é”®ç‰¹æ€§**

1. **Multi-scale Feature Fusion** âœ…
   - èåˆ Swin Transformer çš„ 4 ä¸ª stage ç‰¹å¾
   - æ–¹æ³•ï¼šAdaptive pooling (7Ã—7) â†’ Concatenation â†’ Conv (é™ç»´)
   - æ€»é€šé“æ•°ï¼š96 + 192 + 384 + 768 = 1440 â†’ 112 (hyper_in_channels)

2. **HyperNet Architecture**
   - åŠ¨æ€ç”Ÿæˆ Target Network çš„æƒé‡å’Œåç½®
   - Target Network: 4 å±‚å…¨è¿æ¥ç½‘ç»œ
   - å‚æ•°é‡ï¼š~28.8M (Swin) + ~0.5M (HyperNet) = **29.3M total**

3. **Training Configuration**
   ```python
   batch_size = 96
   train_patch_num = 20  # per image
   test_patch_num = 20
   optimizer = Adam
   lr_hypernet = 2e-4
   lr_backbone = 2e-5
   scheduler = Step Decay (Ã·10 every 6 epochs)
   loss = L1 Loss (MAE)
   early_stopping = patience 7
   random_seed = 42  # å¯å¤ç°
   ```

---

## ğŸ“Š å®éªŒç»“æœ

### **ä¸»è¦ç»“æœï¼ˆBaseline - Multi-scale Concatï¼‰**

| Epoch | Train Loss | Train SRCC | Test SRCC | Test PLCC | çŠ¶æ€ |
|-------|------------|------------|-----------|-----------|------|
| **1** | 4.997 | 0.8758 | **0.9195** | **0.9342** | â­ **æœ€ä½³** |
| 2 | 3.073 | 0.9527 | 0.9185 | 0.9320 | å¼€å§‹è¿‡æ‹Ÿåˆ |
| 3 | 2.527 | 0.9674 | 0.9145 | 0.9275 | æ€§èƒ½ä¸‹é™ |
| 4 | 2.218 | 0.9747 | 0.9174 | 0.9287 | æŒç»­è¿‡æ‹Ÿåˆ |
| 5+ | ... | ... | ... | ... | ç»§ç»­ä¸‹é™ |

**æœ€ä½³æ€§èƒ½**ï¼š
- **SRCC**: 0.9195
- **PLCC**: 0.9342
- **å‡ºç°æ—¶æœº**ï¼šç¬¬ 1 ä¸ª epoch

---

### **æ¶ˆèå®éªŒ 1ï¼šRanking Loss vs. L1 Loss**

**ç›®æ ‡**ï¼šæµ‹è¯• Ranking Loss (pairwise) å¯¹æ€§èƒ½çš„å½±å“

| Method | Loss Function | Test SRCC | Test PLCC |
|--------|---------------|-----------|-----------|
| Baseline | L1 only | **0.9195** | **0.9342** |
| Ranking Î±=0.5 | L1 + 0.5Ã—Ranking | 0.9092 | 0.9289 |
| Ranking Î±=1.0 | L1 + 1.0Ã—Ranking | ~0.90 | ~0.93 |

**ç»“è®º**ï¼š
- âŒ Ranking Loss é™ä½äº†æ€§èƒ½
- âœ… çº¯ L1 Loss æ›´é€‚åˆè¿™ä¸ªä»»åŠ¡
- **åŸå› **ï¼šMAE å·²ç»è¶³å¤Ÿå¼ºï¼ŒRanking Loss å¯èƒ½å¼•å…¥å™ªå£°

---

### **æ¶ˆèå®éªŒ 2ï¼šAttention-based Multi-scale Fusion**

**åŠ¨æœº**ï¼šç”¨æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€è°ƒæ•´ä¸åŒå°ºåº¦ç‰¹å¾çš„æƒé‡

**å®ç°**ï¼š
```python
class MultiScaleAttention(nn.Module):
    def __init__(self, in_channels=[96, 192, 384, 768]):
        self.attention_net = nn.Sequential(
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 4),
            nn.Softmax(dim=1)
        )
    
    def forward(self, feat_list):
        # Stage 3 (global) â†’ attention weights
        feat3_global = AdaptiveAvgPool2d(feat_list[-1], (1,1))
        weights = self.attention_net(feat3_global)  # [B, 4]
        
        # Weighted fusion
        fused = Î£ (weights[i] * feat_list[i])
        return fused
```

**ç»“æœ**ï¼š

| Method | Epoch 1 SRCC | Epoch 1 PLCC | Epoch 2 SRCC | çŠ¶æ€ |
|--------|--------------|--------------|--------------|------|
| **Concat (Baseline)** | **0.9195** | **0.9342** | 0.9185 | âœ… æœ€ä½³ PLCC |
| Attention (std=0.05) | 0.9198 | 0.9317 | 0.9145 | âŒ è¿‡æ‹Ÿåˆä¸¥é‡ |
| Attention (std=0.01) | 0.9196 | 0.9317 | 0.9159 | âŒ ä»è¿‡æ‹Ÿåˆ |
| Attention (dropout=0.5) | 0.9196 | 0.9317 | 0.9159 | âŒ æ— æ”¹å–„ |

**è§‚å¯Ÿåˆ°çš„æ³¨æ„åŠ›æƒé‡**ï¼š
```
åˆå§‹ï¼ˆæ”¹è¿›å‰ï¼‰ï¼š[0.0000, 0.0000, 0.0001, 0.9999]  # æåº¦ä¸å¹³è¡¡
åˆå§‹ï¼ˆæ”¹è¿›åï¼‰ï¼š[0.2316, 0.2153, 0.2261, 0.3270]  # è¾ƒå¹³è¡¡
```

**ç»“è®º**ï¼š
- âŒ æ³¨æ„åŠ›æœºåˆ¶æ²¡æœ‰å¸¦æ¥æ€§èƒ½æå‡
- âŒ PLCC åè€Œä¸‹é™ï¼ˆ0.9342 â†’ 0.9317ï¼‰
- âŒ è¿‡æ‹Ÿåˆé—®é¢˜æ›´ä¸¥é‡
- **å¯èƒ½åŸå› **ï¼š
  1. ç®€å• concat å·²ç»è¶³å¤Ÿæœ‰æ•ˆ
  2. æ³¨æ„åŠ›å¢åŠ äº† ~0.2M å‚æ•°ï¼ŒåŠ å‰§è¿‡æ‹Ÿåˆ
  3. HyperNet æœ¬èº«å°±å¾ˆå¤æ‚ï¼Œå†åŠ æ³¨æ„åŠ›ä¼˜åŒ–å›°éš¾

---

### **æ¶ˆèå®éªŒ 3ï¼šTest Augmentation**

| Method | Test SRCC | å¯å¤ç°æ€§ |
|--------|-----------|---------|
| RandomCrop | 0.9195 | âŒ ä½ï¼ˆéšæœºï¼‰ |
| CenterCrop | 0.9182 | âœ… é«˜ï¼ˆå›ºå®šï¼‰ |

**ç»“è®º**ï¼š
- RandomCrop æ€§èƒ½ç•¥å¥½ï¼ˆ+0.0013ï¼‰
- ä½† CenterCrop æ›´é€‚åˆè®ºæ–‡å‘è¡¨ï¼ˆå¯å¤ç°ï¼‰
- **å½“å‰é€‰æ‹©**ï¼šRandomCropï¼ˆåŸè®ºæ–‡æ–¹æ³•ï¼Œ`--test_random_crop` å‚æ•°ï¼‰

---

## â— å½“å‰æ ¸å¿ƒé—®é¢˜ï¼šä¸¥é‡è¿‡æ‹Ÿåˆ

### **é—®é¢˜è¡¨ç°**

1. **ç¬¬ 1 ä¸ª epoch åæ€§èƒ½æŒç»­ä¸‹é™**
   - Test SRCC: 0.9195 â†’ 0.9185 â†’ 0.9145 â†’ ...
   - Train SRCC: 0.8758 â†’ 0.9527 â†’ 0.9674 â†’ 0.9747

2. **è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ€§èƒ½å·®è·æ‰©å¤§**
   - Epoch 1: Train 0.8758 vs Test 0.9195 (gap = -0.0437)
   - Epoch 4: Train 0.9747 vs Test 0.9174 (gap = +0.0573)

3. **Early stopping åœ¨ç¬¬ 1 ä¸ª epoch åœæ­¢**
   - è¯´æ˜æ¨¡å‹å¾ˆå¿«å°±è¾¾åˆ°æ³›åŒ–èƒ½åŠ›å³°å€¼
   - ä¹‹åçš„è®­ç»ƒéƒ½æ˜¯åœ¨"è®°å¿†"è®­ç»ƒé›†

### **å¯èƒ½çš„åŸå› åˆ†æ**

1. **æ¨¡å‹å®¹é‡è¿‡å¤§**
   - Swin Transformer Tiny: 28.8M å‚æ•°
   - HyperNet: 0.5M å‚æ•°
   - å¯¹äº 7k è®­ç»ƒå›¾åƒå¯èƒ½è¿‡äºå¤æ‚

2. **æ•°æ®å¢å¼ºä¸è¶³**
   - å½“å‰åªæœ‰ RandomCrop (224Ã—224)
   - æ²¡æœ‰é¢œè‰²æ‰°åŠ¨ã€æ—‹è½¬ã€å™ªå£°ç­‰
   - Patch ä¹‹é—´ä¿¡æ¯å†—ä½™é«˜ï¼ˆæ¥è‡ªåŒä¸€å›¾åƒï¼‰

3. **æ­£åˆ™åŒ–ä¸è¶³**
   - æ—  Dropoutï¼ˆé™¤äº†æ³¨æ„åŠ›å®éªŒä¸­çš„å°è¯•ï¼‰
   - Weight decay = 0ï¼ˆæœªä½¿ç”¨ï¼‰
   - æ—  Label smoothing

4. **å­¦ä¹ ç‡å¯èƒ½è¿‡å¤§**
   - HyperNet: 2e-4
   - Backbone: 2e-5
   - ç¬¬ 1 ä¸ª epoch å°±å­¦åˆ°ä½äº†ï¼Œä¹‹åå¼€å§‹è¿‡æ‹Ÿåˆ

---

## ğŸ”§ å·²å°è¯•çš„è§£å†³æ–¹æ¡ˆ

| æ–¹æ³• | æ•ˆæœ | è¯´æ˜ |
|------|------|------|
| Ranking Loss | âŒ | æ€§èƒ½ä¸‹é™ |
| Attention Fusion | âŒ | è¿‡æ‹Ÿåˆæ›´ä¸¥é‡ |
| Attention Dropout (0.5) | âŒ | æ— æ”¹å–„ |
| RandomCrop vs CenterCrop | âœ… | RandomCrop ç•¥å¥½ |
| Early Stopping (patience=7) | âœ… | è‡ªåŠ¨ä¿å­˜ç¬¬ 1 epoch |

---

## ğŸ’­ æœªå°è¯•çš„å¯èƒ½æ”¹è¿›æ–¹å‘

### **1. æ›´å¼ºçš„æ•°æ®å¢å¼º**

```python
# é¢œè‰²æ‰°åŠ¨
transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)

# å‡ ä½•å˜æ¢
transforms.RandomRotation(10)
transforms.RandomHorizontalFlip(0.5)

# å›¾åƒè´¨é‡ç›¸å…³çš„å¢å¼º
- Random JPEG compression
- Gaussian blur
- Gaussian noise
```

**æŒ‘æˆ˜**ï¼šIQA ä»»åŠ¡ä¸­ï¼ŒæŸäº›å¢å¼ºå¯èƒ½æ”¹å˜å›¾åƒçš„çœŸå®è´¨é‡

---

### **2. æ­£åˆ™åŒ–æŠ€æœ¯**

```python
# Dropout in HyperNet
nn.Dropout(0.3) in fc layers

# Weight decay
optimizer = Adam(..., weight_decay=1e-4)

# Label smoothing
# å°† MOS score åšè½»å¾®å¹³æ»‘

# Stochastic depth (Swin Transformer)
drop_path_rate = 0.2
```

---

### **3. è®­ç»ƒç­–ç•¥è°ƒæ•´**

```python
# æ›´å°çš„å­¦ä¹ ç‡
lr_hypernet = 1e-4  # åŸæ¥ 2e-4
lr_backbone = 1e-5  # åŸæ¥ 2e-5

# Warmup
# å‰ 1-2 ä¸ª epoch ç”¨æ›´å°çš„å­¦ä¹ ç‡

# æ›´æ¿€è¿›çš„ early stopping
patience = 3  # åŸæ¥ 7

# Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

### **4. æ¨¡å‹æ¶æ„è°ƒæ•´**

```python
# ä½¿ç”¨æ›´å°çš„ backbone
swin_transformer_nano  # ä»£æ›¿ tiny

# å‡å°‘ patch æ•°é‡
train_patch_num = 10  # åŸæ¥ 20
# é™ä½æ•°æ®å†—ä½™

# Freeze early layers
# å†»ç»“ Swin çš„å‰ 1-2 ä¸ª stage

# åœ¨ HyperNet ä¸­åŠ å…¥ BatchNorm
# å¯èƒ½æé«˜è®­ç»ƒç¨³å®šæ€§
```

---

### **5. æ•°æ®å±‚é¢**

```python
# æ··åˆå¤šä¸ªæ•°æ®é›†
# KonIQ-10k + LIVE-itW + SPAQ
# å¢åŠ æ•°æ®å¤šæ ·æ€§

# æ›´å¼ºçš„ patch é‡‡æ ·ç­–ç•¥
# å½“å‰ï¼šéšæœºé‡‡æ · 20 ä¸ª patches
# æ”¹è¿›ï¼šç¡®ä¿ patches è¦†ç›–ä¸åŒåŒºåŸŸï¼ˆgrid samplingï¼‰
```

---

### **6. Loss Function æ”¹è¿›**

```python
# Huber Lossï¼ˆæ›´é²æ£’ï¼‰
loss = nn.SmoothL1Loss()

# Weighted MAEï¼ˆå…³æ³¨éš¾æ ·æœ¬ï¼‰
# å¯¹é¢„æµ‹è¯¯å·®å¤§çš„æ ·æœ¬åŠ æƒ

# Contrastive Learning
# å­¦ä¹ è´¨é‡åˆ†æ•°ç›¸è¿‘çš„å›¾åƒåº”è¯¥æœ‰ç›¸ä¼¼çš„ç‰¹å¾
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ï¼ˆKonIQ-10k SOTAï¼‰

| Method | Backbone | SRCC | PLCC | å‚æ•°é‡ |
|--------|----------|------|------|--------|
| DBCNN | Custom CNN | 0.875 | 0.884 | ~10M |
| HyperIQA (åŸè®ºæ–‡) | ResNet50 | 0.906 | 0.917 | ~25M |
| MANIQA | ViT-Base | 0.920 | 0.937 | ~86M |
| **Ours (Baseline)** | **Swin-Tiny** | **0.9195** | **0.9342** | **29.3M** |

**ä¼˜åŠ¿**ï¼š
- âœ… å‚æ•°æ•ˆç‡é«˜ï¼ˆ29.3M vs 86M ViTï¼‰
- âœ… æ€§èƒ½æ¥è¿‘ SOTA
- âœ… ä½¿ç”¨ multi-scale ç‰¹å¾èåˆ

**åŠ£åŠ¿**ï¼š
- âŒ ä¸¥é‡è¿‡æ‹Ÿåˆï¼ˆåªèƒ½è®­ç»ƒ 1 ä¸ª epochï¼‰
- âŒ æ³›åŒ–èƒ½åŠ›å¯èƒ½ä¸å¦‚å¤§æ¨¡å‹

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜æ€»ç»“

### **æœ€éœ€è¦è§£å†³çš„é—®é¢˜**

1. **å¦‚ä½•è®©æ¨¡å‹åœ¨ç¬¬ 2+ ä¸ª epoch ä¸æ‰ç‚¹ï¼Ÿ**
   - å½“å‰ï¼šEpoch 1 (0.9195) â†’ Epoch 2 (0.9185) â†’ Epoch 3 (0.9145)
   - æœŸæœ›ï¼šEpoch 1 (0.9195) â†’ Epoch 2 (0.9200+) â†’ ...

2. **å¦‚ä½•åœ¨ä¸æŸå¤±æ€§èƒ½çš„å‰æä¸‹å‡å°‘è¿‡æ‹Ÿåˆï¼Ÿ**
   - æ•°æ®å¢å¼º vs ä»»åŠ¡ç‰¹æ®Šæ€§ï¼ˆä¸èƒ½ç ´åå›¾åƒè´¨é‡ï¼‰
   - æ­£åˆ™åŒ– vs æ¨¡å‹å®¹é‡éœ€æ±‚

3. **æ˜¯å¦åº”è¯¥æ¢ç”¨æ›´å°çš„ backboneï¼Ÿ**
   - Swin-Tiny (28.8M) å¯èƒ½å¤ªå¤§
   - Swin-Nano / ResNet50 / MobileNet?

---

## ğŸ” éœ€è¦ AI ä¸“å®¶å¸®åŠ©è§£ç­”çš„é—®é¢˜

1. **ä¸ºä»€ä¹ˆç¬¬ 1 ä¸ª epoch å°±è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Ÿ**
   - æ˜¯å­¦ä¹ ç‡å¤ªå¤§ï¼Ÿ
   - è¿˜æ˜¯æ¨¡å‹å®¹é‡å¤ªå¼ºï¼Ÿ
   - è¿˜æ˜¯æ•°æ®å¤ªç®€å•ï¼Ÿ

2. **IQA ä»»åŠ¡çš„æ•°æ®å¢å¼ºæœ‰å“ªäº›æœ€ä½³å®è·µï¼Ÿ**
   - å“ªäº›å¢å¼ºä¸ä¼šæ”¹å˜å›¾åƒçš„çœŸå®è´¨é‡ï¼Ÿ
   - å¦‚ä½•å¹³è¡¡å¢å¼ºå¼ºåº¦å’Œä»»åŠ¡ç‰¹æ€§ï¼Ÿ

3. **HyperNet + Swin Transformer è¿™ä¸ªç»„åˆæ˜¯å¦åˆç†ï¼Ÿ**
   - ä¸¤ä¸ªéƒ½æ˜¯"å…ƒå­¦ä¹ "æ¶æ„
   - ä¼šä¸ä¼šäº’ç›¸å¹²æ‰°ï¼Ÿ
   - æœ‰æ²¡æœ‰æ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆï¼Ÿ

4. **Multi-scale fusion çš„æœ€ä½³æ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿ**
   - Concatï¼ˆå½“å‰ï¼‰vs Attentionï¼ˆå¤±è´¥ï¼‰vs å…¶ä»–ï¼Ÿ
   - æ˜¯å¦éœ€è¦å¯å­¦ä¹ çš„èåˆæƒé‡ï¼Ÿ

5. **å¦‚ä½•è®¾è®¡å®éªŒæ¥è¯Šæ–­è¿‡æ‹Ÿåˆçš„æ ¹æœ¬åŸå› ï¼Ÿ**
   - åº”è¯¥å…ˆå°è¯•å“ªä¸ªæ–¹å‘ï¼Ÿ
   - å¦‚ä½•ç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸åŒå‡è®¾ï¼Ÿ

---

## ğŸ“ é‡è¦æ–‡ä»¶

- **ä»£ç **ï¼š`train_swin.py`, `models_swin.py`, `HyperIQASolver_swin.py`
- **æ—¥å¿—**ï¼š`logs/swin_multiscale_ranking_alpha0_20251218_161547.log`
- **æ¨¡å‹**ï¼š`checkpoints/koniq-10k-swin_20251218_161547/best_model_srcc_0.9195_plcc_0.9342.pkl`
- **æ–‡æ¡£**ï¼š`WORK_SUMMARY.md`, `LR_SCHEDULER_GUIDE.md`, `ARCHITECTURE_COMPARISON_GUIDE.md`

---

## âœ… å·²å®ç°çš„åŠŸèƒ½

- [x] Multi-scale feature fusion
- [x] Learning rate schedulers (Step, Cosine, Constant)
- [x] Early stopping
- [x] Automatic logging
- [x] Reproducibility (random seed)
- [x] Test augmentation options (RandomCrop / CenterCrop)
- [x] SPAQ cross-dataset testing (optional)
- [x] Ranking Loss (ä½†æ•ˆæœä¸ä½³)
- [x] Attention-based fusion (ä½†æ•ˆæœä¸ä½³)

---

## ğŸ“ è®ºæ–‡ä»·å€¼

å³ä½¿å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œè¿™ä¸ªå·¥ä½œä»æœ‰è®ºæ–‡ä»·å€¼ï¼š

1. **Swin Transformer åœ¨ IQA çš„é¦–æ¬¡ç³»ç»Ÿåº”ç”¨**
2. **Multi-scale ç‰¹å¾èåˆçš„æ¶ˆèå®éªŒ**
3. **å‚æ•°é«˜æ•ˆçš„è®¾è®¡**ï¼ˆ29.3M vs 86M ViTï¼‰
4. **è¯šå®çš„ ablation studies**ï¼ˆRanking Loss å’Œ Attention å¤±è´¥ä¹Ÿæ˜¯å‘ç°ï¼‰

---

## ğŸ™ è¯· AI ä¸“å®¶æä¾›å»ºè®®

**æ ¸å¿ƒç›®æ ‡**ï¼šåœ¨ä¸é™ä½ Epoch 1 æ€§èƒ½çš„å‰æä¸‹ï¼Œè®© Epoch 2-5 çš„æ€§èƒ½ä¹Ÿèƒ½ä¿æŒåœ¨ 0.919+ SRCC

**çº¦æŸæ¡ä»¶**ï¼š
- è®¡ç®—èµ„æºæœ‰é™ï¼ˆå• GPUï¼‰
- æ•°æ®é›†å›ºå®šï¼ˆKonIQ-10kï¼‰
- æ¶æ„å¤§æ¡†æ¶ä¸å˜ï¼ˆSwin + HyperNetï¼‰

**å¸Œæœ›å¾—åˆ°**ï¼š
1. æœ€æœ‰å¯èƒ½æœ‰æ•ˆçš„æ”¹è¿›æ–¹å‘ï¼ˆæ’åºï¼‰
2. å®éªŒè®¾è®¡å»ºè®®ï¼ˆå¦‚ä½•æµ‹è¯•å‡è®¾ï¼‰
3. æ˜¯å¦æœ‰ç±»ä¼¼é—®é¢˜çš„æˆåŠŸæ¡ˆä¾‹
4. æ˜¯å¦éœ€è¦é‡æ–°è€ƒè™‘æ¶æ„é€‰æ‹©

æ„Ÿè°¢ï¼ğŸ™

