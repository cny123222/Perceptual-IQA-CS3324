# ResNet + æ”¹è¿›ç»„ä»¶æ¶ˆèå®éªŒå¯è¡Œæ€§åˆ†æ

## ğŸ¯ **å®éªŒç›®æ ‡**

éªŒè¯æˆ‘ä»¬çš„æ”¹è¿›ï¼ˆMulti-scale + Attentionï¼‰æ˜¯å¦ä¹Ÿèƒ½æå‡ResNet50çš„æ€§èƒ½ï¼Œä»è€Œè¯æ˜æ”¹è¿›çš„æ™®é€‚æ€§ã€‚

---

## âœ… **å¯è¡Œæ€§ï¼šå®Œå…¨å¯ä»¥åšï¼**

### **å®éªŒè®¾è®¡**ï¼š

```
åŸºå‡†ï¼šResNet50 (HyperIQAåŸå§‹)     â†’ SRCC 0.8998
å®éªŒ1ï¼šResNet50 + Multi-scale     â†’ SRCC ?
å®éªŒ2ï¼šResNet50 + Attention       â†’ SRCC ?  
å®éªŒ3ï¼šResNet50 + Multi + Atten   â†’ SRCC ?
```

---

## ğŸ”§ **æŠ€æœ¯å®ç°æ–¹æ¡ˆ**

### **æ–¹æ¡ˆAï¼šåŸºäºç°æœ‰models_swin.pyä¿®æ”¹**

```python
# åœ¨models_swin.pyä¸­æ·»åŠ ResNetç‰ˆæœ¬

class HyperNet_ResNet_Improved(nn.Module):
    def __init__(self, use_multiscale=False, use_attention=False):
        super().__init__()
        
        # ResNet50 backbone
        resnet = models.resnet50(pretrained=True)
        
        # æå–4ä¸ªstageçš„features
        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)
        self.layer1 = resnet.layer1  # Stage 1: 256 channels
        self.layer2 = resnet.layer2  # Stage 2: 512 channels
        self.layer3 = resnet.layer3  # Stage 3: 1024 channels
        self.layer4 = resnet.layer4  # Stage 4: 2048 channels
        
        if use_multiscale:
            # Multi-scale feature aggregation
            self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))
            self.conv1_stage1 = nn.Conv2d(256, 256, 1)
            self.conv1_stage2 = nn.Conv2d(512, 512, 1)
            self.conv1_stage3 = nn.Conv2d(1024, 1024, 1)
            
            if use_attention:
                # Channel attention (ç±»ä¼¼Swinç‰ˆæœ¬)
                self.attention_net = MultiScaleAttention([256, 512, 1024, 2048])
                input_channels = 256 + 512 + 1024 + 2048  # 3840
            else:
                input_channels = 256 + 512 + 1024 + 2048  # 3840
        else:
            # å•å°ºåº¦ï¼ˆåªç”¨Stage 4ï¼‰
            input_channels = 2048
        
        # HyperNetå’ŒTargetNetï¼ˆä¿æŒä¸å˜ï¼‰
        # ...
```

---

## ğŸ“Š **é¢„æœŸç»“æœ**

### **å‡è®¾1ï¼šæ”¹è¿›æœ‰æ•ˆï¼ˆä¹è§‚ï¼‰**

```
ResNet50 (åŸå§‹)              0.8998  (baseline)
ResNet50 + Multi-scale       0.9050  (+0.0052, +0.58%)
ResNet50 + Attention         0.9080  (+0.0082, +0.91%)
ResNet50 + Multi + Atten     0.9120  (+0.0122, +1.35%)
```

**æ„ä¹‰**ï¼šè¯æ˜æ”¹è¿›å…·æœ‰æ™®é€‚æ€§ï¼Œä¸ä¾èµ–äºSwin Transformer

---

### **å‡è®¾2ï¼šæ”¹è¿›æœ‰é™ï¼ˆä¸­æ€§ï¼‰**

```
ResNet50 (åŸå§‹)              0.8998  (baseline)
ResNet50 + Multi-scale       0.9010  (+0.0012, +0.13%)
ResNet50 + Attention         0.9025  (+0.0027, +0.30%)
ResNet50 + Multi + Atten     0.9040  (+0.0042, +0.47%)
```

**æ„ä¹‰**ï¼šæ”¹è¿›å¯¹ResNetå¸®åŠ©æœ‰é™ï¼Œè¯´æ˜Swinçš„å±‚æ¬¡åŒ–ç‰¹å¾æ›´å…³é”®

---

### **å‡è®¾3ï¼šæ”¹è¿›æ— æ•ˆï¼ˆæ‚²è§‚ï¼‰**

```
ResNet50 (åŸå§‹)              0.8998  (baseline)
ResNet50 + Multi-scale       0.8995  (-0.0003)
ResNet50 + Attention         0.9005  (+0.0007)
ResNet50 + Multi + Atten     0.9000  (+0.0002)
```

**æ„ä¹‰**ï¼šæ”¹è¿›ä¸“é—¨ä¸ºSwinè®¾è®¡ï¼Œéœ€è¦hierarchical featuresæ‰èƒ½å‘æŒ¥ä½œç”¨

---

## ğŸ¯ **å®éªŒä»·å€¼åˆ†æ**

### **ä¼˜ç‚¹**ï¼š

1. âœ… **è¯æ˜æ”¹è¿›çš„æ™®é€‚æ€§**
   - å¦‚æœResNet+æ”¹è¿›ä¹Ÿæœ‰æå‡ï¼Œè¯´æ˜æ–¹æ³•ä¸ä¾èµ–backbone
   
2. âœ… **æ›´å…¬å¹³çš„å¯¹æ¯”**
   - å¯ä»¥åˆ†ç¦»"Swinæœ¬èº«"å’Œ"æ”¹è¿›æ–¹æ³•"çš„è´¡çŒ®
   
3. âœ… **è®ºæ–‡æ›´å®Œæ•´**
   - Ablation studyæ›´å…¨é¢
   
4. âœ… **æŠ€æœ¯ä¸Šå¯è¡Œ**
   - ä»£ç æ”¹åŠ¨ä¸å¤§ï¼ˆ~200è¡Œï¼‰
   - è®­ç»ƒæ—¶é—´ï¼š~1-2å°æ—¶/å®éªŒ

### **ç¼ºç‚¹**ï¼š

1. âš ï¸ **éœ€è¦é¢å¤–å®éªŒæ—¶é—´**
   - 3ä¸ªå®éªŒ Ã— 1.5å°æ—¶ = ~4.5å°æ—¶
   
2. âš ï¸ **å¯èƒ½ç»“æœä¸ç†æƒ³**
   - å¦‚æœResNet+æ”¹è¿›æå‡å¾ˆå°ï¼Œåè€Œæ˜¾å¾—æˆ‘ä»¬çš„æ–¹æ³•ä¸å¤Ÿé€šç”¨
   
3. âš ï¸ **è®ºæ–‡ç¯‡å¹…**
   - éœ€è¦é¢å¤–1-2é¡µæ¥è®¨è®ºè¿™äº›å®éªŒ

---

## ğŸ’¡ **å»ºè®®**

### **æ¨èæ–¹æ¡ˆï¼šåš1ä¸ªå…³é”®å®éªŒ**

```
ResNet50 + Multi-scale + Attention (å®Œæ•´æ”¹è¿›)
```

**åŸå› **ï¼š
1. åªéœ€è¦1ä¸ªå®éªŒï¼ˆ~1.5å°æ—¶ï¼‰
2. å¦‚æœæœ‰æ˜æ˜¾æå‡ï¼ˆ+1-2%ï¼‰ï¼Œè¯´æ˜æ”¹è¿›æœ‰æ•ˆ
3. å¦‚æœæå‡å¾ˆå°ï¼ˆ<0.5%ï¼‰ï¼Œè¯´æ˜Swinçš„å±‚æ¬¡åŒ–ç‰¹å¾æ˜¯å…³é”®
4. ä¸¤ç§ç»“æœéƒ½æœ‰è®ºæ–‡ä»·å€¼

---

## ğŸ“‹ **å®ç°æ­¥éª¤**

### **Step 1: ä»£ç å®ç°ï¼ˆ30åˆ†é’Ÿï¼‰**

```bash
# åˆ›å»ºmodels_resnet_improved.py
cp models_swin.py models_resnet_improved.py
# ä¿®æ”¹ä¸ºResNet backbone
```

### **Step 2: è®­ç»ƒè„šæœ¬ï¼ˆ10åˆ†é’Ÿï¼‰**

```bash
# å¤åˆ¶è®­ç»ƒè„šæœ¬
cp train_test_IQA_swin.py train_test_IQA_resnet_improved.py
# ä¿®æ”¹model import
```

### **Step 3: è¿è¡Œå®éªŒï¼ˆ1.5å°æ—¶ï¼‰**

```bash
python3 train_test_IQA_resnet_improved.py \
    --dataset koniq-10k \
    --epochs 10 \
    --lr 1e-4 \
    --batch_size 32 \
    --use_multiscale \
    --use_attention \
    --backbone resnet50
```

### **Step 4: åˆ†æç»“æœï¼ˆ20åˆ†é’Ÿï¼‰**

- æå–SRCC/PLCC
- ä¸ResNet baselineå¯¹æ¯”
- å†™å…¥è®ºæ–‡

---

## ğŸ“ **è®ºæ–‡ä¸­å¦‚ä½•å‘ˆç°**

### **å¦‚æœç»“æœå¥½ï¼ˆ+1-2%ï¼‰**ï¼š

```latex
\subsection{Generalization to CNN Backbones}

To verify the generality of our proposed improvements (multi-scale fusion 
and channel attention), we apply them to the original ResNet50 backbone. 
As shown in Table X, ResNet50 with our improvements achieves 0.9120 SRCC, 
significantly outperforming the original HyperIQA (0.8998) by 1.35%. 
This demonstrates that our method is not limited to Transformer architectures 
and can benefit CNN-based models as well. However, the improvement is smaller 
than that achieved with Swin Transformer (+0.0122 vs +0.0380), suggesting 
that hierarchical vision transformers provide more suitable features for 
quality-aware multi-scale fusion.
```

### **å¦‚æœç»“æœä¸€èˆ¬ï¼ˆ+0.3-0.5%ï¼‰**ï¼š

```latex
\subsection{Importance of Hierarchical Features}

We investigate whether our improvements (multi-scale fusion and attention) 
can benefit CNN backbones by applying them to ResNet50. The improved 
ResNet50 achieves 0.9040 SRCC, slightly better than the original (0.8998), 
but the gain (+0.0042) is much smaller than with Swin Transformer (+0.0380). 
This indicates that our multi-scale attention mechanism specifically benefits 
from the hierarchical, window-based features of Swin Transformer, which 
preserve more fine-grained spatial information than conventional CNN features.
```

### **å¦‚æœç»“æœä¸å¥½ï¼ˆ<0.3%ï¼‰**ï¼š

```latex
\subsection{Role of Backbone Architecture}

To understand the source of our performance gains, we apply the same 
improvements to ResNet50. Interestingly, ResNet50 with multi-scale attention 
shows minimal improvement (0.9000 vs 0.8998), while Swin Transformer benefits 
significantly (+0.0380). This suggests that the hierarchical, self-attention 
based features of Swin Transformer are crucial for our method's success, 
and our improvements are specifically designed to leverage these characteristics.
```

---

## âœ… **æœ€ç»ˆå»ºè®®**

### **å»ºè®®åšè¿™ä¸ªå®éªŒï¼Œç†ç”±ï¼š**

1. **æ—¶é—´æˆæœ¬å¯æ¥å—**ï¼šåªéœ€1.5å°æ—¶
2. **è®ºæ–‡æ›´å®Œæ•´**ï¼šæä¾›äº†æ–¹æ³•æ™®é€‚æ€§çš„åˆ†æ
3. **ä¸‰ç§ç»“æœéƒ½æœ‰ä»·å€¼**ï¼š
   - å¥½ç»“æœï¼šè¯æ˜æ–¹æ³•é€šç”¨
   - ä¸€èˆ¬ç»“æœï¼šè¯´æ˜Swinæ›´é€‚åˆ
   - å·®ç»“æœï¼šå¼ºè°ƒå±‚æ¬¡åŒ–ç‰¹å¾çš„é‡è¦æ€§
4. **æŠ€æœ¯ä¸Šç®€å•**ï¼šä»£ç æ”¹åŠ¨å°ï¼Œé£é™©ä½

### **ä½•æ—¶åšï¼Ÿ**

- **ç°åœ¨å°±å¯ä»¥åš**ï¼Œåœ¨è®ºæ–‡å®šç¨¿å‰
- æˆ–è€…ä½œä¸º**Rebuttalå®éªŒ**ï¼ˆå¦‚æœå®¡ç¨¿äººæé—®ï¼‰

---

## ğŸ”§ **ä»£ç æ¡†æ¶**

```python
# models_resnet_improved.py

import torch.nn as nn
import torchvision.models as models

class ResNetImproved(nn.Module):
    def __init__(self, use_multiscale=True, use_attention=True):
        super().__init__()
        
        # Load pretrained ResNet50
        resnet = models.resnet50(pretrained=True)
        
        # Extract stages
        self.stage0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)
        self.stage1 = resnet.layer1  # 56x56, 256 channels
        self.stage2 = resnet.layer2  # 28x28, 512 channels
        self.stage3 = resnet.layer3  # 14x14, 1024 channels
        self.stage4 = resnet.layer4  # 7x7,  2048 channels
        
        self.use_multiscale = use_multiscale
        self.use_attention = use_attention
        
        if use_multiscale:
            # Adaptive pooling to 7x7
            self.pool = nn.AdaptiveAvgPool2d((7, 7))
            
            # Conv 1x1 for each stage
            self.conv1 = nn.Sequential(
                nn.Conv2d(256, 256, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            )
            self.conv2 = nn.Sequential(
                nn.Conv2d(512, 512, 1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True)
            )
            self.conv3 = nn.Sequential(
                nn.Conv2d(1024, 1024, 1),
                nn.BatchNorm2d(1024),
                nn.ReLU(inplace=True)
            )
            
            if use_attention:
                # Channel attention
                from models_swin import MultiScaleAttention
                self.attention = MultiScaleAttention([256, 512, 1024, 2048])
                in_channels = 256 + 512 + 1024 + 2048  # 3840
            else:
                in_channels = 256 + 512 + 1024 + 2048  # 3840
        else:
            # Only use stage 4
            in_channels = 2048
        
        # HyperNet (same as original)
        self.hyper_net = nn.Sequential(
            nn.Conv2d(in_channels, 512, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 112*224, 1)
        )
        
        # ... rest of the implementation
    
    def forward(self, x):
        # Extract features
        x = self.stage0(x)
        f1 = self.stage1(x)  # 256 channels
        f2 = self.stage2(f1)  # 512 channels
        f3 = self.stage3(f2)  # 1024 channels
        f4 = self.stage4(f3)  # 2048 channels
        
        if self.use_multiscale:
            # Pool to 7x7
            f1 = self.conv1(self.pool(f1))
            f2 = self.conv2(self.pool(f2))
            f3 = self.conv3(self.pool(f3))
            
            if self.use_attention:
                # Apply attention
                feat_fused, attn_weights = self.attention([f1, f2, f3, f4])
            else:
                # Simple concatenation
                feat_fused = torch.cat([f1, f2, f3, f4], dim=1)
        else:
            feat_fused = f4
        
        # Generate weights and predict score
        # ... (same as original HyperNet)
```

---

**ç»“è®º**ï¼šè¿™ä¸ªå®éªŒå€¼å¾—åšï¼Œå»ºè®®åœ¨è®ºæ–‡å®šç¨¿å‰å®Œæˆã€‚

