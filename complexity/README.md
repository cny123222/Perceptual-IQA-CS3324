# æ¨¡å‹å¤æ‚åº¦åˆ†æ

æœ¬ç›®å½•åŒ…å«ç”¨äºåˆ†ææ¨¡å‹è®¡ç®—å¤æ‚åº¦çš„è„šæœ¬å’Œå·¥å…·ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

- `compute_complexity.py` - å®Œæ•´çš„å¤æ‚åº¦åˆ†æè„šæœ¬ï¼ˆæ¨èï¼‰
- `quick_test.py` - å¿«é€Ÿæµ‹è¯•è„šæœ¬ï¼ˆä¸éœ€è¦é¢å¤–ä¾èµ–ï¼‰
- `example.JPG` - æµ‹è¯•å›¾ç‰‡
- `complexity_method.md` - è®¡ç®—æ–¹æ³•å‚è€ƒæ–‡æ¡£
- `complexity_results.md` - åˆ†æç»“æœæŠ¥å‘Šï¼ˆè¿è¡Œåç”Ÿæˆï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1ï¼šå¿«é€Ÿæµ‹è¯•ï¼ˆæ¨èå…ˆè¿è¡Œï¼‰

ä¸éœ€è¦å®‰è£…é¢å¤–ä¾èµ–ï¼Œå¿«é€Ÿå¾—åˆ°åŸºæœ¬ç»“æœï¼š

```bash
cd /root/Perceptual-IQA-CS3324
python complexity/quick_test.py
```

**è¾“å‡ºå†…å®¹**ï¼š
- æ¨¡å‹å‚æ•°é‡
- ä¼°ç®—çš„ FLOPs
- å•å¼ å›¾ç‰‡æ¨ç†æ—¶é—´
- ååé‡

### æ–¹æ³• 2ï¼šå®Œæ•´åˆ†æï¼ˆéœ€è¦å®‰è£…ä¾èµ–ï¼‰

å®‰è£…ä¾èµ–ï¼š

```bash
pip install ptflops thop fvcore
```

è¿è¡Œå®Œæ•´åˆ†æï¼š

```bash
cd /root/Perceptual-IQA-CS3324
python complexity/compute_complexity.py
```

**è¾“å‡ºå†…å®¹**ï¼š
- è¯¦ç»†çš„ FLOPs è®¡ç®—ï¼ˆä½¿ç”¨ ptflops å’Œ thopï¼‰
- å‚æ•°é‡ç»Ÿè®¡
- æ¨ç†æ—¶é—´ç»Ÿè®¡ï¼ˆå¹³å‡ã€æ ‡å‡†å·®ã€æœ€å°ã€æœ€å¤§ã€ä¸­ä½æ•°ï¼‰
- ä¸åŒ batch size çš„ååé‡æµ‹è¯•
- è‡ªåŠ¨ç”Ÿæˆ `complexity_results.md` æŠ¥å‘Š

## ğŸ“Š è¾“å‡ºç¤ºä¾‹

### å¿«é€Ÿæµ‹è¯•è¾“å‡º

```
============================================================
QUICK COMPLEXITY TEST
============================================================

1. Loading model...
   âœ… Model loaded

2. Model Parameters: 88,123,456 (88.12M)

3. Estimated FLOPs: 352.49 GFLOPs

4. Loading test image...
   âœ… Image loaded: (800, 600) -> torch.Size([1, 3, 224, 224])

5. Measuring inference time...

6. Results:
   Average inference time: 45.23 Â± 2.15 ms
   Throughput: 22.11 images/sec
   Predicted quality score: 0.7845

============================================================
âœ… Quick test completed!
============================================================
```

### å®Œæ•´åˆ†æè¾“å‡º

```
================================================================================
COMPLEXITY ANALYSIS SUMMARY
================================================================================

ğŸ“Š Model Information:
  Model Name: HyperIQA with Swin Transformer
  Model Size: base
  Total Parameters: 88,123,456 (88.12M)
  Trainable Parameters: 88,123,456 (88.12M)

ğŸ’» Computational Complexity:
  FLOPs (ptflops): 352.49G
  Params (ptflops): 88.12M
  FLOPs (thop): 352.47G
  Params (thop): 88.12M

â±ï¸  Inference Time (single image, 224x224):
  Mean: 45.23 ms
  Std:  2.15 ms
  Min:  42.10 ms
  Max:  51.30 ms
  Median: 44.80 ms

ğŸš€ Throughput:
  Batch size  1:  22.11 images/sec
  Batch size  4:  65.32 images/sec
  Batch size  8: 102.45 images/sec
  Batch size 16: 145.67 images/sec
  Batch size 32: OOM

================================================================================
```

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

ç¼–è¾‘ `compute_complexity.py` æˆ– `quick_test.py` ä¸­çš„é…ç½®ï¼š

```python
# æ¨¡å‹é…ç½®
checkpoint_path = "path/to/your/checkpoint.pkl"
model_size = 'base'  # 'tiny', 'small', 'base'

# æµ‹è¯•å›¾ç‰‡
image_path = "path/to/your/image.jpg"

# è¾“å‡ºæ–‡ä»¶
output_file = "path/to/output.md"
```

## ğŸ“ˆ æµ‹è¯•ä¸åŒæ¨¡å‹

### Swin-Tiny

```python
model = models.HyperNet(
    16, 112, 224, 112, 56, 28, 14, 7,
    use_multiscale=True,
    use_attention=False,
    drop_path_rate=0.2,
    dropout_rate=0.3,
    model_size='tiny'
)
```

é¢„æœŸç»“æœï¼š
- å‚æ•°é‡ï¼š~28M
- FLOPsï¼š~120G
- æ¨ç†æ—¶é—´ï¼š~20ms

### Swin-Small

```python
model = models.HyperNet(
    16, 112, 224, 112, 56, 28, 14, 7,
    use_multiscale=True,
    use_attention=False,
    drop_path_rate=0.2,
    dropout_rate=0.3,
    model_size='small'
)
```

é¢„æœŸç»“æœï¼š
- å‚æ•°é‡ï¼š~50M
- FLOPsï¼š~210G
- æ¨ç†æ—¶é—´ï¼š~30ms

### Swin-Base

```python
model = models.HyperNet(
    16, 112, 224, 112, 56, 28, 14, 7,
    use_multiscale=True,
    use_attention=False,
    drop_path_rate=0.3,
    dropout_rate=0.4,
    model_size='base'
)
```

é¢„æœŸç»“æœï¼š
- å‚æ•°é‡ï¼š~88M
- FLOPsï¼š~350G
- æ¨ç†æ—¶é—´ï¼š~45ms

## ğŸ¯ å…³é”®æŒ‡æ ‡è¯´æ˜

### FLOPs (Floating Point Operations)
- è¡¡é‡æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦
- æ•°å€¼è¶Šå¤§ï¼Œè®¡ç®—é‡è¶Šå¤§
- 1 GFLOPs = 10^9 æ¬¡æµ®ç‚¹è¿ç®—

### å‚æ•°é‡ (Parameters)
- æ¨¡å‹åŒ…å«çš„å¯å­¦ä¹ å‚æ•°æ€»æ•°
- å½±å“æ¨¡å‹å¤§å°å’Œå†…å­˜å ç”¨
- 1M = 1,000,000 ä¸ªå‚æ•°

### æ¨ç†æ—¶é—´ (Inference Time)
- å¤„ç†å•å¼ å›¾ç‰‡æ‰€éœ€çš„æ—¶é—´
- åŒ…æ‹¬å‰å‘ä¼ æ’­çš„æ‰€æœ‰è®¡ç®—
- é€šå¸¸ä»¥æ¯«ç§’ (ms) ä¸ºå•ä½

### ååé‡ (Throughput)
- å•ä½æ—¶é—´å†…å¯ä»¥å¤„ç†çš„å›¾ç‰‡æ•°é‡
- ä»¥ images/sec ä¸ºå•ä½
- ä¸ batch size ç›¸å…³

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **GPU æ€§èƒ½å½±å“**ï¼šæ¨ç†æ—¶é—´å’Œååé‡ä¼šå— GPU æ€§èƒ½å½±å“
2. **Warmup é‡è¦æ€§**ï¼šå‰å‡ æ¬¡æ¨ç†å¯èƒ½è¾ƒæ…¢ï¼Œéœ€è¦ warmup
3. **Batch Size**ï¼šå¢å¤§ batch size å¯æé«˜ååé‡ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜
4. **å›¾ç‰‡å°ºå¯¸**ï¼šæ‰€æœ‰æµ‹è¯•ä½¿ç”¨ 224Ã—224 è¾“å…¥å°ºå¯¸
5. **FLOPs å·®å¼‚**ï¼šä¸åŒå·¥å…·æµ‹é‡çš„ FLOPs å¯èƒ½ç•¥æœ‰å·®å¼‚ï¼ˆé€šå¸¸åœ¨ 1% ä»¥å†…ï¼‰

## ğŸ› å¸¸è§é—®é¢˜

### 1. CUDA out of memory

```bash
# é™ä½ batch size æˆ–ä½¿ç”¨ CPU
device = 'cpu'
```

### 2. ä¾èµ–å®‰è£…å¤±è´¥

```bash
# ä½¿ç”¨å¿«é€Ÿæµ‹è¯•è„šæœ¬ï¼ˆä¸éœ€è¦é¢å¤–ä¾èµ–ï¼‰
python complexity/quick_test.py
```

### 3. æ‰¾ä¸åˆ° checkpoint

```bash
# æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®
ls -lh checkpoints/
```

## ğŸ“š å‚è€ƒæ–‡æ¡£

- `complexity_method.md` - åŒ…å«å¤šç§ FLOPs è®¡ç®—æ–¹æ³•
- PyTorch å®˜æ–¹æ–‡æ¡£ï¼šhttps://pytorch.org/docs/stable/
- ptflops GitHubï¼šhttps://github.com/sovrasov/flops-counter.pytorch
- thop GitHubï¼šhttps://github.com/Lyken17/pytorch-OpCounter

