Using device: cuda

Creating ResNet50-based HyperIQA model...
Total parameters: 27,375,369 (27.38M)
Trainable parameters: 27,375,369 (27.38M)
FLOPs: 4.33 GFLOPs

Measuring inference time...
Mean inference time: 3.12 ± 0.33 ms
Throughput: 320.48 images/sec

Measuring throughput for different batch sizes...
  Batch size  1: 329.73 images/sec
  Batch size  4: 1321.37 images/sec
  Batch size  8: 2765.86 images/sec
  Batch size 16: 3711.53 images/sec
  Batch size 32: 3790.58 images/sec

✅ 报告已保存: /root/Perceptual-IQA-CS3324/complexity/complexity_results_resnet50.md
usage: compute_complexity.py [-h] [--checkpoint CHECKPOINT] [--image IMAGE]
                             [--output OUTPUT]
                             [--model_size {tiny,small,base}]
                             [--use_attention] [--no_attention]
compute_complexity.py: error: unrecognized arguments: --model-size base
================================================================================
批量运行所有模型的复杂度分析
================================================================================

================================================================================
分析 HyperIQA (ResNet50) 复杂度
================================================================================
创建 ResNet 复杂度分析脚本: /root/Perceptual-IQA-CS3324/complexity/compute_complexity_resnet.py

================================================================================
分析 SMART-Tiny 复杂度
================================================================================
⚠️  未找到 SMART-Tiny 的 checkpoint
   请手动指定 checkpoint 路径或训练模型
创建无checkpoint的基本分析: tiny
Loading Swin Transformer TINY (~28M parameters)
Using attention-based multi-scale feature fusion for TINY
HyperNet input channels: 1440 (multi_scale=True, model=tiny)
✅ 报告已保存: /root/Perceptual-IQA-CS3324/complexity/complexity_results_swin_tiny.md
   参数量: 29.52M
   FLOPs: 4.47 GFLOPs

================================================================================
分析 SMART-Small 复杂度
================================================================================
⚠️  未找到 SMART-Small 的 checkpoint
   请手动指定 checkpoint 路径或训练模型
创建无checkpoint的基本分析: small
Loading Swin Transformer SMALL (~50M parameters)
Using attention-based multi-scale feature fusion for SMALL
HyperNet input channels: 1440 (multi_scale=True, model=small)
✅ 报告已保存: /root/Perceptual-IQA-CS3324/complexity/complexity_results_swin_small.md
   参数量: 50.84M
   FLOPs: 8.65 GFLOPs

================================================================================
分析 SMART-Base 复杂度
================================================================================

================================================================================
✅ 所有模型复杂度分析完成！
================================================================================

查看结果:
  - complexity/complexity_results_resnet50.md
  - complexity/complexity_results_swin_tiny.md
  - complexity/complexity_results_swin_small.md
  - complexity/complexity_results_swin_base.md
