================================================================================
MODEL COMPLEXITY ANALYSIS
================================================================================
Checkpoint: ../checkpoints/koniq-10k-swin-ranking-alpha0.5_20251221_155013/best_model_srcc_0.9343_plcc_0.9463.pkl
Model size: base
Device: cuda
GPU: NVIDIA GeForce RTX 5090
GPU Memory: 33.67 GB
Loading model from: ../checkpoints/koniq-10k-swin-ranking-alpha0.5_20251221_155013/best_model_srcc_0.9343_plcc_0.9463.pkl
Auto-detected attention: True
Loading Swin Transformer BASE (~88M parameters)
Using attention-based multi-scale feature fusion for BASE
HyperNet input channels: 1920 (multi_scale=True, model=base)
Model loaded successfully (model_size=base, attention=True)

Loading image: /root/Perceptual-IQA-CS3324/complexity/example.JPG
Original image size: (2048, 1536)
Preprocessed tensor shape: torch.Size([1, 3, 224, 224])

üìä Parameters:
  Total: 89,111,941 (89.11M)
  Trainable: 89,111,941 (89.11M)

================================================================================
Computing FLOPs using ptflops...
================================================================================

================================================================================
Computing FLOPs using thop...
================================================================================

================================================================================
Measuring inference time (warmup=10, iterations=100)...
================================================================================
Warming up...
Measuring...

================================================================================
Measuring throughput for different batch sizes (duration=10s)...
================================================================================
  Batch size  1:  88.30 images/sec (884 images in 10.01s)
  Batch size  4: 283.15 images/sec (2832 images in 10.00s)
  Batch size  8: 435.09 images/sec (4352 images in 10.00s)
  Batch size 16: 571.12 images/sec (5712 images in 10.00s)
  Batch size 32: 607.35 images/sec (6080 images in 10.01s)

================================================================================
COMPLEXITY ANALYSIS SUMMARY
================================================================================

üìä Model Information:
  Model Name: HyperIQA with Swin Transformer
  Model Size: base
  Total Parameters: 89,111,941 (89.11M)
  Trainable Parameters: 89,111,941 (89.11M)

üíª Computational Complexity:
  FLOPs (ptflops): 15.3 GMac
  Params (ptflops): 89.11 M
  FLOPs (thop): 15.278G
  Params (thop): 89.048M

‚è±Ô∏è  Inference Time (single image, 224x224):
  Mean: 11.15 ms
  Std:  0.81 ms
  Min:  10.36 ms
  Max:  15.33 ms
  Median: 10.97 ms

üöÄ Throughput:
  Batch size  1:  88.30 images/sec
  Batch size  4: 283.15 images/sec
  Batch size  8: 435.09 images/sec
  Batch size 16: 571.12 images/sec
  Batch size 32: 607.35 images/sec

================================================================================

‚úÖ Results saved to: complexity_results_base_attention.md

‚úÖ Complexity analysis completed!
