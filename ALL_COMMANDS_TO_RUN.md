# ğŸ” æ‰€æœ‰å®éªŒå‘½ä»¤æ¸…å• - è¯·ä»”ç»†æ£€æŸ¥

**æ€»å®éªŒæ•°**: 6ä¸ªå®éªŒï¼Œåˆ†3ä¸ªæ‰¹æ¬¡  
**é¢„è®¡æ€»æ—¶é—´**: ~10å°æ—¶  
**ä½¿ç”¨GPU**: 2å—ï¼ˆæ¯æ‰¹æ¬¡å¹¶è¡Œ2ä¸ªå®éªŒï¼‰

---

## ğŸ“‹ Batch 1: Learning Rate Comparison (Phase 1)

### GPU 0: LR = 1e-6
```bash
CUDA_VISIBLE_DEVICES=0 python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 10 \
  --patience 3 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --train_test_num 10 \
  --lr 1e-6 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --attention_fusion \
  --ranking_loss_alpha 0 \
  --test_random_crop \
  --no_spaq \
  --no_color_jitter
```
**è¾“å‡º**: `logs/batch1_gpu0_lr1e6.log`

---

### GPU 1: LR = 5e-7
```bash
CUDA_VISIBLE_DEVICES=1 python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 10 \
  --patience 3 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --train_test_num 10 \
  --lr 5e-7 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --attention_fusion \
  --ranking_loss_alpha 0 \
  --test_random_crop \
  --no_spaq \
  --no_color_jitter
```
**è¾“å‡º**: `logs/batch1_gpu1_lr5e7.log`

**é¢„è®¡æ—¶é—´**: ~3.4å°æ—¶

---

## ğŸ“‹ Batch 2: Ablation Studies (Phase 2)

### GPU 0: A1 - Remove Attention Fusion
```bash
CUDA_VISIBLE_DEVICES=0 python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 10 \
  --patience 3 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --train_test_num 10 \
  --lr 1e-6 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --ranking_loss_alpha 0 \
  --test_random_crop \
  --no_spaq \
  --no_color_jitter
```
**æ³¨æ„**: æ²¡æœ‰ `--attention_fusion` å‚æ•°  
**è¾“å‡º**: `logs/batch2_gpu0_A1_no_attention.log`

---

### GPU 1: A2 - Remove Multi-scale Fusion
```bash
CUDA_VISIBLE_DEVICES=1 python train_swin.py \
  --dataset koniq-10k \
  --model_size base \
  --batch_size 32 \
  --epochs 10 \
  --patience 3 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --train_test_num 10 \
  --lr 1e-6 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.3 \
  --dropout_rate 0.4 \
  --lr_scheduler cosine \
  --no_multi_scale \
  --ranking_loss_alpha 0 \
  --test_random_crop \
  --no_spaq \
  --no_color_jitter
```
**æ³¨æ„**: æœ‰ `--no_multi_scale` å‚æ•°  
**è¾“å‡º**: `logs/batch2_gpu1_A2_no_multiscale.log`

**é¢„è®¡æ—¶é—´**: ~3.4å°æ—¶

---

## ğŸ“‹ Batch 3: Model Size Comparison (Phase 3)

### GPU 0: B1 - Swin-Tiny
```bash
CUDA_VISIBLE_DEVICES=0 python train_swin.py \
  --dataset koniq-10k \
  --model_size tiny \
  --batch_size 32 \
  --epochs 10 \
  --patience 3 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --train_test_num 10 \
  --lr 1e-6 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.2 \
  --dropout_rate 0.3 \
  --lr_scheduler cosine \
  --attention_fusion \
  --ranking_loss_alpha 0 \
  --test_random_crop \
  --no_spaq \
  --no_color_jitter
```
**æ³¨æ„**: `model_size=tiny`, `drop_path_rate=0.2`, `dropout_rate=0.3` (æ›´ä½çš„æ­£åˆ™åŒ–)  
**è¾“å‡º**: `logs/batch3_gpu0_B1_tiny.log`

---

### GPU 1: B2 - Swin-Small
```bash
CUDA_VISIBLE_DEVICES=1 python train_swin.py \
  --dataset koniq-10k \
  --model_size small \
  --batch_size 32 \
  --epochs 10 \
  --patience 3 \
  --train_patch_num 20 \
  --test_patch_num 20 \
  --train_test_num 10 \
  --lr 1e-6 \
  --weight_decay 2e-4 \
  --drop_path_rate 0.25 \
  --dropout_rate 0.35 \
  --lr_scheduler cosine \
  --attention_fusion \
  --ranking_loss_alpha 0 \
  --test_random_crop \
  --no_spaq \
  --no_color_jitter
```
**æ³¨æ„**: `model_size=small`, `drop_path_rate=0.25`, `dropout_rate=0.35` (ä¸­ç­‰æ­£åˆ™åŒ–)  
**è¾“å‡º**: `logs/batch3_gpu1_B2_small.log`

**é¢„è®¡æ—¶é—´**: ~3.2å°æ—¶

---

## âœ… å…³é”®å‚æ•°ç¡®è®¤

### æ‰€æœ‰å®éªŒå…±åŒå‚æ•°:
- âœ… `--dataset koniq-10k`
- âœ… `--batch_size 32`
- âœ… `--epochs 10`
- âœ… `--patience 3`
- âœ… `--train_patch_num 20`
- âœ… `--test_patch_num 20`
- âœ… `--train_test_num 10` (10è½®)
- âœ… `--lr_scheduler cosine`
- âœ… `--ranking_loss_alpha 0` (ä¸ç”¨ranking loss)
- âœ… `--test_random_crop`
- âœ… `--no_spaq`
- âœ… `--no_color_jitter`

### å˜åŒ–çš„å‚æ•°:

| Batch | GPU 0 | GPU 1 |
|-------|-------|-------|
| **Batch 1** | LR=1e-6, base, attention+multiscale | LR=5e-7, base, attention+multiscale |
| **Batch 2** | LR=1e-6, base, NO attention | LR=1e-6, base, NO multiscale |
| **Batch 3** | LR=1e-6, tiny, drop_path=0.2, dropout=0.3 | LR=1e-6, small, drop_path=0.25, dropout=0.35 |

---

## ğŸ“Š é¢„æœŸç»“æœ

| å®éªŒ | é¢„æœŸSRCC | è¯´æ˜ |
|------|---------|------|
| **Batch 1 - LR 1e-6** | **0.937** | æœ€ä½³æ¨¡å‹ |
| Batch 1 - LR 5e-7 | 0.935 | å¯¹æ¯”å®éªŒ |
| Batch 2 - A1 (No Attention) | 0.932 | é‡åŒ–Attentionè´¡çŒ® |
| Batch 2 - A2 (No Multi-scale) | 0.930 | é‡åŒ–Multi-scaleè´¡çŒ® |
| Batch 3 - B1 (Tiny) | 0.921 | å°æ¨¡å‹ |
| Batch 3 - B2 (Small) | 0.933 | ä¸­ç­‰æ¨¡å‹ |

---

## âš ï¸ è¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹

1. **å‚æ•°æ­£ç¡®æ€§**:
   - [ ] Batch 2 çš„ A1 ç¡®å®æ²¡æœ‰ `--attention_fusion`
   - [ ] Batch 2 çš„ A2 ç¡®å®æœ‰ `--no_multi_scale`
   - [ ] Batch 3 çš„ Tiny ç”¨çš„æ˜¯ drop_path=0.2, dropout=0.3
   - [ ] Batch 3 çš„ Small ç”¨çš„æ˜¯ drop_path=0.25, dropout=0.35
   - [ ] æ‰€æœ‰å®éªŒéƒ½ç”¨ `--lr 1e-6` (é™¤äº†Batch 1çš„GPU 1ç”¨5e-7å¯¹æ¯”)

2. **æ•°æ®é›†è·¯å¾„**:
   - [ ] `/root/Perceptual-IQA-CS3324/koniq-10k/` å­˜åœ¨
   - [ ] æ•°æ®é›†å®Œæ•´

3. **ç£ç›˜ç©ºé—´**:
   - [ ] è‡³å°‘æœ‰ 30GB ç©ºé—²ç©ºé—´ï¼ˆ6ä¸ªå®éªŒ Ã— ~2.7GB checkpointï¼‰

4. **è®­ç»ƒæ—¶é—´**:
   - [ ] Batch 1: ~3.4å°æ—¶ (10 rounds Ã— 10 epochs Ã— 2 min/epoch)
   - [ ] Batch 2: ~3.4å°æ—¶
   - [ ] Batch 3: ~3.2å°æ—¶
   - [ ] **æ€»è®¡**: ~10å°æ—¶

---

## ğŸ“ æ—¥å¿—æ–‡ä»¶ä½ç½®

æ‰€æœ‰æ—¥å¿—å°†ä¿å­˜åœ¨ `logs/` ç›®å½•ä¸‹:
```
logs/
â”œâ”€â”€ batch1_gpu0_lr1e6.log           # Phase 1, GPU 0
â”œâ”€â”€ batch1_gpu1_lr5e7.log           # Phase 1, GPU 1
â”œâ”€â”€ batch2_gpu0_A1_no_attention.log # Phase 2, GPU 0
â”œâ”€â”€ batch2_gpu1_A2_no_multiscale.log# Phase 2, GPU 1
â”œâ”€â”€ batch3_gpu0_B1_tiny.log         # Phase 3, GPU 0
â””â”€â”€ batch3_gpu1_B2_small.log        # Phase 3, GPU 1
```

---

## âœ… ç¡®è®¤æ— è¯¯å

è¯·æ£€æŸ¥å®Œæ‰€æœ‰å‘½ä»¤ï¼Œå¦‚æœç¡®è®¤æ— è¯¯ï¼Œæˆ‘å°†åˆ›å»ºå®Œæ•´çš„tmuxè‡ªåŠ¨åŒ–è„šæœ¬ã€‚

**è„šæœ¬å°†åŒ…å«**:
1. è‡ªåŠ¨åˆ›å»ºtmuxä¼šè¯
2. é¡ºåºæ‰§è¡Œ3ä¸ªbatch
3. æ¯ä¸ªbatchåœ¨2ä¸ªtmuxçª—å£ä¸­å¹¶è¡Œ
4. è‡ªåŠ¨ç­‰å¾…batchå®Œæˆå†å¯åŠ¨ä¸‹ä¸€ä¸ª
5. å¤±è´¥é‡è¯•æœºåˆ¶
6. å®Œæˆåè‡ªåŠ¨æå–ç»“æœ
7. å‘é€å®Œæˆé€šçŸ¥

**æ˜¯å¦ç»§ç»­åˆ›å»ºè‡ªåŠ¨åŒ–è„šæœ¬ï¼Ÿ**

