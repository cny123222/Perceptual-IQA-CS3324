# å®Œæ•´æ”¹è¿›åˆ†æï¼šä»ResNet50 HyperIQAåˆ°Swin Transformerç‰ˆæœ¬

## ğŸ“‹ å¯¹æ¯”æ€»ç»“

| æ–¹é¢ | åŸå§‹HyperIQA (ResNet50) | æˆ‘ä»¬çš„Swinç‰ˆæœ¬ | æ”¹è¿›ç±»å‹ |
|------|------------------------|---------------|---------|
| **SRCC** | 0.907 | **0.9378** | **+3.08%** |
| **PLCC** | ~0.918 | **0.9485** | **+3.05%** |

---

## ğŸ—ï¸ æ¶æ„æ”¹è¿› (Architecture Improvements)

### 1. **Backboneæ›¿æ¢ï¼šResNet50 â†’ Swin Transformer**

#### ResNet50 (åŸå§‹)
- å‚æ•°é‡ï¼š~25.6M
- è¾“å‡ºç‰¹å¾ï¼š2048ç»´ @ 7Ã—7
- ä»…ä½¿ç”¨layer4çš„è¾“å‡ºï¼ˆå•å°ºåº¦ï¼‰
- å·ç§¯ç¥ç»ç½‘ç»œï¼Œå±€éƒ¨æ„Ÿå—é‡

#### Swin Transformer Base (æˆ‘ä»¬çš„)
- å‚æ•°é‡ï¼š~88Mï¼ˆ+3.4å€ï¼‰
- è¾“å‡ºç‰¹å¾ï¼š4ä¸ªé˜¶æ®µ [128, 256, 512, 1024]ç»´
- ä½¿ç”¨æ‰€æœ‰4ä¸ªé˜¶æ®µçš„ç‰¹å¾ï¼ˆå¤šå°ºåº¦ï¼‰
- Transformeræ¶æ„ï¼Œå…¨å±€å»ºæ¨¡èƒ½åŠ›
- **é¢„æœŸè´¡çŒ®ï¼š+2.84% SRCCï¼ˆæœ€å¤§è´¡çŒ®ï¼‰**

---

### 2. **å¤šå°ºåº¦ç‰¹å¾èåˆ (Multi-scale Feature Fusion)**

#### ResNet50 (åŸå§‹)
```python
# ä»…ä½¿ç”¨æœ€åä¸€å±‚ç‰¹å¾
out['hyper_in_feat'] = x  # [B, 2048, 7, 7]
```

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
```python
# ä½¿ç”¨æ‰€æœ‰4ä¸ªé˜¶æ®µçš„ç‰¹å¾
out['hyper_in_feat_multi'] = [feat0, feat1, feat2, feat3]
# Tiny/Small: [96, 192, 384, 768] â†’ 1440ç»´
# Base: [128, 256, 512, 1024] â†’ 1920ç»´

# ç»Ÿä¸€ç©ºé—´å°ºå¯¸åˆ°7Ã—7
feat0_pooled = F.adaptive_avg_pool2d(feat0, (7, 7))  # 56Ã—56 â†’ 7Ã—7
feat1_pooled = F.adaptive_avg_pool2d(feat1, (7, 7))  # 28Ã—28 â†’ 7Ã—7
feat2_pooled = F.adaptive_avg_pool2d(feat2, (7, 7))  # 14Ã—14 â†’ 7Ã—7
feat3_pooled = feat3  # 7Ã—7 (already)

# é€šé“ç»´åº¦æ‹¼æ¥
hyper_in_feat_raw = torch.cat([feat0, feat1, feat2, feat3], dim=1)
```

**ç‰¹ç‚¹**ï¼š
- æ•è·ä»ä½å±‚åˆ°é«˜å±‚çš„å¤šå°ºåº¦è¯­ä¹‰ä¿¡æ¯
- ä½å±‚ç‰¹å¾ï¼šçº¹ç†ã€è¾¹ç¼˜ç»†èŠ‚
- é«˜å±‚ç‰¹å¾ï¼šå…¨å±€è¯­ä¹‰ã€å†…å®¹ç†è§£
- **é¢„æœŸè´¡çŒ®ï¼š~0.5-0.8% SRCC**

---

### 3. **æ³¨æ„åŠ›æœºåˆ¶èåˆ (Attention-based Fusion)**

#### ResNet50 (åŸå§‹)
- æ— æ³¨æ„åŠ›æœºåˆ¶
- ç®€å•çš„ç‰¹å¾ä¼ é€’

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
```python
class MultiScaleAttention(nn.Module):
    def __init__(self, in_channels_list):
        # ä½¿ç”¨æœ€é«˜å±‚ç‰¹å¾ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        self.attention_net = nn.Sequential(
            nn.Linear(in_channels_list[-1], 256),  # 1024 â†’ 256
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),  # å¼ºæ­£åˆ™åŒ–
            nn.Linear(256, 4),  # â†’ 4ä¸ªå°ºåº¦çš„æƒé‡
            nn.Softmax(dim=1)  # å½’ä¸€åŒ–
        )
    
    def forward(self, feat_list):
        # æå–å…¨å±€è¡¨ç¤º
        feat3_global = F.adaptive_avg_pool2d(feat_list[-1], (1, 1))
        
        # ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention_net(feat3_global)  # [B, 4]
        
        # åŠ¨æ€åŠ æƒèåˆ
        weighted_feats = []
        for i, feat in enumerate(feats_pooled):
            weight = attention_weights[:, i].view(B, 1, 1, 1)
            weighted_feat = feat * weight
            weighted_feats.append(weighted_feat)
        
        fused_feat = torch.cat(weighted_feats, dim=1)
        return fused_feat, attention_weights
```

**ç‰¹ç‚¹**ï¼š
- **åŠ¨æ€åŠ æƒ**ï¼šæ ¹æ®è¾“å…¥å›¾åƒè‡ªé€‚åº”è°ƒæ•´å„å°ºåº¦çš„é‡è¦æ€§
- **æ•°æ®é©±åŠ¨**ï¼šä¸åŒå›¾åƒå¯èƒ½éœ€è¦ä¸åŒå°ºåº¦çš„ä¿¡æ¯
- **å¯è§£é‡Šæ€§**ï¼šæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
- **é¢„æœŸè´¡çŒ®ï¼š~0.3-0.5% SRCC**

---

## ğŸ¯ è®­ç»ƒç­–ç•¥æ”¹è¿› (Training Strategy Improvements)

### 4. **å­¦ä¹ ç‡ä¼˜åŒ–**

#### ResNet50 (åŸå§‹)
- Learning Rate: ~1e-4 (è¾ƒé«˜)
- LR Scheduler: Step decay (æ¯10 epoch Ã— 0.1)
- å•ä¸€å­¦ä¹ ç‡ç­–ç•¥

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
- **æœ€ä¼˜Learning Rate: 5e-7** (ä½200å€ï¼)
- **LR Scheduler: Cosine Annealing** (å¹³æ»‘è¡°å‡)
- **Backbone LR Ratio: 10** (backboneç”¨æ›´ä½çš„LR)
  ```python
  backbone_params: lr = 5e-7
  hypernet_params: lr = 5e-6
  ```

**å…³é”®å‘ç°**ï¼š
- Swin Transformerå¯¹å­¦ä¹ ç‡æå…¶æ•æ„Ÿ
- éœ€è¦éå¸¸æ…¢ã€ç¨³å®šçš„è®­ç»ƒ
- **è´¡çŒ®ï¼š+0.24% SRCC** (5e-6 â†’ 5e-7)

**å­¦ä¹ ç‡å®éªŒç»“æœ**ï¼š
```
5e-6 (baseline): 0.9354
3e-6:            0.9364  (+0.10%)
1e-6:            0.9374  (+0.20%)
5e-7:            0.9378  (+0.24%) ğŸ†
```

---

### 5. **æ­£åˆ™åŒ–å¢å¼º (Enhanced Regularization)**

#### ResNet50 (åŸå§‹)
```python
# ä»…æœ‰åŸºç¡€æ­£åˆ™åŒ–
weight_decay = 1e-4
# æ— Dropout
# æ— Stochastic Depth
```

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
```python
# 1. Weight Decay (æ›´å¼º)
weight_decay = 2e-4  # æé«˜2å€

# 2. Stochastic Depth (Drop Path)
drop_path_rate = 0.3  # Swin Transformerå†…éƒ¨
# éšæœºä¸¢å¼ƒæ•´ä¸ªæ®‹å·®åˆ†æ”¯ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

# 3. Dropout in HyperNet
self.dropout = nn.Dropout(0.4)  # åœ¨HyperNetä¸­æ·»åŠ 
hyper_in_feat = self.dropout(hyper_in_feat)

# 4. Dropout in TargetNet
class TargetNet(nn.Module):
    def __init__(self, paras, dropout_rate=0.4):
        self.dropout = nn.Dropout(dropout_rate)
    
    def forward(self, x):
        q = self.l1(x)
        q = self.dropout(q)  # æ¯å±‚åéƒ½dropout
        q = self.l2(q)
        q = self.dropout(q)
        q = self.l3(q)
        q = self.dropout(q)
        q = self.l4(q)
        return q
```

**å¤šå±‚æ­£åˆ™åŒ–ç­–ç•¥**ï¼š
1. **Backboneå±‚**ï¼šDrop Path (0.3)
2. **HyperNetå±‚**ï¼šDropout (0.4) + Weight Decay (2e-4)
3. **TargetNetå±‚**ï¼šDropout (0.4)

**æ•ˆæœ**ï¼š
- é˜²æ­¢å¤§æ¨¡å‹è¿‡æ‹Ÿåˆ
- æé«˜æ³›åŒ–èƒ½åŠ›
- **é¢„æœŸè´¡çŒ®ï¼š~0.3-0.5% SRCC**

---

### 6. **Early Stopping with Patience**

#### ResNet50 (åŸå§‹)
- å›ºå®šè®­ç»ƒepochæ•°
- å¯èƒ½è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
```python
patience = 3  # 3ä¸ªepochæ— æå‡åˆ™åœæ­¢
early_stopping_enabled = True

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
if srcc > best_srcc:
    best_srcc = srcc
    patience_counter = 0
    # ä¿å­˜æœ€ä½³æ¨¡å‹
else:
    patience_counter += 1
    if patience_counter >= self.patience:
        print(f"Early stopping at epoch {epoch}")
        break
```

**æ•ˆæœ**ï¼š
- è‡ªåŠ¨æ‰¾åˆ°æœ€ä½³åœæ­¢ç‚¹
- é¿å…è¿‡æ‹Ÿåˆ
- èŠ‚çœè®­ç»ƒæ—¶é—´

---

### 7. **æµ‹è¯•ç­–ç•¥æ”¹è¿›**

#### ResNet50 (åŸå§‹)
- Test Crop: CenterCrop (å›ºå®šä¸­å¿ƒè£å‰ª)
- å•ä¸€è§†è§’è¯„ä¼°

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
```python
test_random_crop = True  # ä½¿ç”¨éšæœºè£å‰ª

# å¤špatchæµ‹è¯•
test_patch_num = 20  # æ¯å¼ å›¾20ä¸ªpatch
# å¯¹æ¯ä¸ªpatchç‹¬ç«‹è¯„åˆ†ï¼Œç„¶åå¹³å‡
```

**æ•ˆæœ**ï¼š
- æ›´å…¨é¢çš„å›¾åƒè´¨é‡è¯„ä¼°
- å‡å°‘ä½ç½®åå·®
- æé«˜æµ‹è¯•é²æ£’æ€§
- **é¢„æœŸè´¡çŒ®ï¼š~0.1-0.2% SRCC**

---

## ğŸ”§ å·¥ç¨‹æ”¹è¿› (Engineering Improvements)

### 8. **æ•°æ®å¢å¼ºè°ƒæ•´**

#### ResNet50 (åŸå§‹)
- ColorJitter: å¯ç”¨
  ```python
  transforms.ColorJitter(
      brightness=0.2,
      contrast=0.2,
      saturation=0.2,
      hue=0.1
  )
  ```

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
- **ColorJitter: ç¦ç”¨** (`--no_color_jitter`)

**åŸå› **ï¼š
- IQAä»»åŠ¡éœ€è¦ä¿æŒå›¾åƒçš„åŸå§‹é¢œè‰²ä¿¡æ¯
- ColorJitterä¼šæ”¹å˜å›¾åƒçš„æ„ŸçŸ¥è´¨é‡
- ç§»é™¤åè®­ç»ƒé€Ÿåº¦æå‡3å€ï¼ˆCPUç“¶é¢ˆæ¶ˆé™¤ï¼‰
- **è´¡çŒ®ï¼š+0.16% SRCC** + è®­ç»ƒåŠ é€Ÿ

---

### 9. **ä¼˜åŒ–å™¨æ”¹è¿›**

#### ResNet50 (åŸå§‹)
```python
optimizer = torch.optim.Adam(
    [
        {'params': hypernet_params},
        {'params': backbone_params}
    ],
    lr=config.lr,
    weight_decay=config.weight_decay
)
```

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
```python
optimizer = torch.optim.AdamW(  # AdamWè€ŒéAdam
    [
        {'params': hypernet_params, 
         'lr': config.lr * config.lr_ratio,  # 5e-6
         'weight_decay': config.weight_decay},
        {'params': backbone_params, 
         'lr': config.lr,  # 5e-7
         'weight_decay': config.weight_decay}
    ]
)
```

**æ”¹è¿›**ï¼š
- **AdamW**ï¼šæ›´å¥½çš„æƒé‡è¡°å‡å®ç°ï¼ˆè§£è€¦ï¼‰
- **å·®å¼‚åŒ–å­¦ä¹ ç‡**ï¼šbackboneç”¨æ›´ä½çš„LRï¼ˆå¾®è°ƒé¢„è®­ç»ƒæƒé‡ï¼‰

---

### 10. **æ¨¡å‹å°ºå¯¸å¯é€‰ (Model Size Options)**

#### ResNet50 (åŸå§‹)
- å›ºå®šä½¿ç”¨ResNet50
- æ— æ¨¡å‹å°ºå¯¸é€‰æ‹©

#### Swinç‰ˆæœ¬ (æˆ‘ä»¬çš„)
```python
model_size = 'base'  # å¯é€‰: 'tiny', 'small', 'base'

# Swin-Tiny: ~28M params, channels=[96, 192, 384, 768]
# Swin-Small: ~50M params, channels=[96, 192, 384, 768]
# Swin-Base: ~88M params, channels=[128, 256, 512, 1024]
```

**çµæ´»æ€§**ï¼š
- æ ¹æ®è®¡ç®—èµ„æºé€‰æ‹©æ¨¡å‹
- ç²¾åº¦-æ•ˆç‡æƒè¡¡
- **Baseç›¸æ¯”Tinyæå‡ï¼š~0.2% SRCC**

---

## ğŸ“Š å®Œæ•´è´¡çŒ®åˆ†è§£

### æ€»æå‡ï¼š0.907 â†’ 0.9378 = **+3.08% SRCC**

| ç»„ä»¶ | è´¡çŒ® | å æ¯” | ç±»å‹ |
|------|------|------|------|
| **1. Swin Transformer Backbone** | +2.84% | 92% | æ¶æ„ |
| **2. å­¦ä¹ ç‡ä¼˜åŒ– (5e-7)** | +0.24% | 8% | è®­ç»ƒç­–ç•¥ |
| **3. å¤šå°ºåº¦èåˆ** | ~+0.5% | - | æ¶æ„ |
| **4. æ³¨æ„åŠ›æœºåˆ¶** | ~+0.3% | - | æ¶æ„ |
| **5. æ­£åˆ™åŒ–å¢å¼º** | ~+0.4% | - | è®­ç»ƒç­–ç•¥ |
| **6. ç§»é™¤ColorJitter** | +0.16% | - | æ•°æ®å¢å¼º |
| **7. æµ‹è¯•ç­–ç•¥æ”¹è¿›** | ~+0.1% | - | è¯„ä¼° |

**æ³¨æ„**ï¼šç»„ä»¶3-7çš„è´¡çŒ®æœ‰é‡å ï¼Œä¸èƒ½ç®€å•ç›¸åŠ ã€‚

---

## ğŸ¯ æ¶ˆèå®éªŒè®¾è®¡ï¼ˆæ­£å‘ï¼‰

ä¸ºäº†é‡åŒ–æ¯ä¸ªç»„ä»¶çš„ç‹¬ç«‹è´¡çŒ®ï¼Œåº”è¯¥è¿›è¡Œ**æ­£å‘æ¶ˆèå®éªŒ**ï¼ˆä»ç®€å•åˆ°å¤æ‚ï¼‰ï¼š

### C0: ResNet50 Baseline
```bash
# åŸå§‹HyperIQA
SRCC = 0.907
```

### C1: ä»…æ¢Backbone (Swin-Base, å•å°ºåº¦, æ— æ³¨æ„åŠ›)
```bash
python train_swin.py \
    --model_size base \
    --lr 5e-7 \
    --no_multiscale \
    --no_color_jitter \
    ...
```
**é¢„æœŸ**ï¼š~0.930-0.932ï¼ˆ+2.3-2.5%ï¼‰

### C2: æ·»åŠ å¤šå°ºåº¦èåˆ (Swin-Base + Multi-scale, æ— æ³¨æ„åŠ›)
```bash
python train_swin.py \
    --model_size base \
    --lr 5e-7 \
    --no_color_jitter \
    ...
    # é»˜è®¤å¯ç”¨multi-scaleï¼Œä¸åŠ --attention_fusion
```
**é¢„æœŸ**ï¼š~0.934-0.936ï¼ˆ+2.7-2.9%ï¼‰

### C3: æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶ (å®Œæ•´ç‰ˆæœ¬)
```bash
python train_swin.py \
    --model_size base \
    --lr 5e-7 \
    --attention_fusion \
    --no_color_jitter \
    ...
```
**å®é™…**ï¼š0.9378ï¼ˆ+3.08%ï¼‰âœ…

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

### 1. **Backboneæ˜¯æœ€å¤§è´¡çŒ®è€…**
- Swin Transformerç›¸æ¯”ResNet50æä¾›äº†92%çš„æ€§èƒ½æå‡
- è¯´æ˜**é¢„è®­ç»ƒçš„Transformeræ¶æ„å¯¹IQAä»»åŠ¡éå¸¸æœ‰æ•ˆ**

### 2. **å­¦ä¹ ç‡è‡³å…³é‡è¦**
- Swinéœ€è¦æ¯”ResNetä½200å€çš„å­¦ä¹ ç‡
- è¯´æ˜**å¤§æ¨¡å‹éœ€è¦æ›´ç¨³å®šã€ç¼“æ…¢çš„å¾®è°ƒ**

### 3. **å¤šå°ºåº¦ä¿¡æ¯å¾ˆé‡è¦**
- å¤šå°ºåº¦èåˆæä¾›äº†é¢å¤–çš„æ€§èƒ½å¢ç›Š
- è¯´æ˜**IQAéœ€è¦ä»ä½å±‚çº¹ç†åˆ°é«˜å±‚è¯­ä¹‰çš„å…¨æ–¹ä½ä¿¡æ¯**

### 4. **æ³¨æ„åŠ›æœºåˆ¶é”¦ä¸Šæ·»èŠ±**
- åŠ¨æ€åŠ æƒæ¯”ç®€å•æ‹¼æ¥æ›´å¥½
- è¯´æ˜**ä¸åŒå›¾åƒéœ€è¦ä¸åŒå°ºåº¦çš„ä¿¡æ¯**

### 5. **æ­£åˆ™åŒ–å¿…ä¸å¯å°‘**
- å¤§æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦å¤šå±‚æ­£åˆ™åŒ–
- Drop Path + Dropout + Weight Decayçš„ç»„åˆæ•ˆæœæœ€å¥½

---

## ğŸ“ è®ºæ–‡å†™ä½œå»ºè®®

### Abstract
> "We propose an improved HyperIQA model by replacing the ResNet50 backbone with Swin Transformer and introducing multi-scale attention-based feature fusion. Our method achieves 0.9378 SRCC on KonIQ-10k, outperforming the original HyperIQA by 3.08%."

### Method Section
1. **Backbone Replacement** (Section 3.1)
   - ä¸ºä»€ä¹ˆé€‰æ‹©Swin Transformer
   - æ¶æ„ç»†èŠ‚
   
2. **Multi-scale Feature Fusion** (Section 3.2)
   - 4ä¸ªé˜¶æ®µçš„ç‰¹å¾æå–
   - ç©ºé—´å°ºå¯¸ç»Ÿä¸€
   
3. **Attention-based Fusion** (Section 3.3)
   - åŠ¨æ€æƒé‡ç”Ÿæˆ
   - å¯è§£é‡Šæ€§
   
4. **Training Strategy** (Section 3.4)
   - å­¦ä¹ ç‡è°ƒä¼˜
   - æ­£åˆ™åŒ–ç­–ç•¥
   - Early stopping

### Ablation Study (Section 4.2)
- Table: æ­£å‘æ¶ˆèå®éªŒç»“æœ (C0 â†’ C1 â†’ C2 â†’ C3)
- Analysis: æ¯ä¸ªç»„ä»¶çš„ç‹¬ç«‹è´¡çŒ®

### Model Size Comparison (Section 4.3)
- Table: Tiny vs Small vs Base
- Analysis: å‚æ•°é‡-æ€§èƒ½æƒè¡¡

---

## ğŸ” å½“å‰å®éªŒçŠ¶æ€

### âœ… å·²å®Œæˆ
- C0 (ResNet50): 0.907
- C3 (å®Œæ•´ç‰ˆæœ¬): 0.9378
- E6 (LR 5e-7): 0.9378 (æœ€ä½³)

### â³ æ­£åœ¨è¿è¡Œ
- A1 (Remove Attention) â‰ˆ C2
- A2 (Remove Multi-scale) â‰ˆ C1
- B1 (Swin-Small)
- B2 (Swin-Tiny)
- E7 (LR 1e-7)

### ğŸ“Œ å»ºè®®è¡¥å……
- å¯èƒ½éœ€è¦é‡æ–°è®¾è®¡A1å’ŒA2ä¸ºæ­£å‘å®éªŒ
- æˆ–è€…ç›´æ¥ä½¿ç”¨å½“å‰ç»“æœåæ¨ï¼š
  - C1 â‰ˆ A2çš„ç»“æœ
  - C2 â‰ˆ A1çš„ç»“æœ

---

**æ€»ç»“**ï¼šæˆ‘ä»¬çš„æ”¹è¿›ä¸ä»…ä»…æ˜¯æ¢äº†ä¸ªbackboneï¼Œè€Œæ˜¯ä¸€ä¸ª**ç³»ç»Ÿæ€§çš„æ¶æ„å‡çº§ + è®­ç»ƒç­–ç•¥ä¼˜åŒ–**çš„ç»„åˆæ‹³ï¼ğŸ¯

