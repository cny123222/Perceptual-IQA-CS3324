#!/usr/bin/env python3
"""
è·¨æ•°æ®é›†æµ‹è¯•è„šæœ¬ - Cross-Dataset Testing Script

ç”¨äºåœ¨å¤šä¸ªIQAæ•°æ®é›†ä¸Šè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹
Supports: KonIQ-10k, SPAQ, KADID-10K, AGIQA-3K

Usage:
    python cross_dataset_test.py --checkpoint path/to/model.pkl --model_size tiny
    python cross_dataset_test.py --checkpoint path/to/model.pkl --model_size small --test_patch_num 20
"""

import os
import sys
import argparse
import torch
import torchvision
import models_swin as models
from scipy import stats
import numpy as np
import json
from PIL import Image
from datetime import datetime
from tqdm import tqdm
import glob


def get_device():
    """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")


def pil_loader(path):
    """åŠ è½½PILå›¾åƒ"""
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGB')


class JSONTestDataset(torch.utils.data.Dataset):
    """
    é€šç”¨JSONæ ¼å¼æµ‹è¯•æ•°æ®é›†åŠ è½½å™¨
    ä¼˜åŒ–ï¼šé¢„åŠ è½½å’Œç¼“å­˜resizeåçš„å›¾åƒï¼ˆä¸è®­ç»ƒæ—¶çš„SPAQæµ‹è¯•å®Œå…¨ä¸€è‡´ï¼‰
    """
    def __init__(self, root, json_file, patch_num, test_random_crop=True):
        self.root = root
        self.patch_num = patch_num
        
        # åŠ è½½JSONæ–‡ä»¶
        json_path = os.path.join(root, json_file)
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"JSON file not found: {json_path}")
            
        with open(json_path) as f:
            data = json.load(f)
        
        # ç”Ÿæˆæ ·æœ¬åˆ—è¡¨ï¼šæ¯ä¸ªå›¾åƒç”Ÿæˆpatch_numä¸ªæ ·æœ¬
        self.samples = []
        for item in data:
            img_path = os.path.join(root, os.path.basename(item['image']))
            if not os.path.exists(img_path):
                continue
            score = float(item['score'])
            for _ in range(patch_num):
                self.samples.append((img_path, score))
        
        # è®¡ç®—å®é™…å›¾åƒæ•°é‡
        unique_paths = list(set([s[0] for s in self.samples]))
        print(f"  Found {len(unique_paths)} images, {len(self.samples)} total patches")
        
        # å…³é”®ä¼˜åŒ–ï¼šé¢„åŠ è½½å’Œç¼“å­˜æ‰€æœ‰resizeåçš„å›¾åƒï¼ˆä¸è®­ç»ƒæ—¶çš„SPAQDatasetå®Œå…¨ä¸€è‡´ï¼‰
        self.resize_transform = torchvision.transforms.Resize((512, 384))
        
        # æ ¹æ®cropæ–¹æ³•è®¾ç½®crop transform
        if test_random_crop:
            self.crop_transform = torchvision.transforms.Compose([
                torchvision.transforms.RandomCrop(size=224),
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),
                                               std=(0.229, 0.224, 0.225))])
        else:
            self.crop_transform = torchvision.transforms.Compose([
                torchvision.transforms.CenterCrop(size=224),
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),
                                               std=(0.229, 0.224, 0.225))])
        
        # é¢„åŠ è½½å’Œç¼“å­˜æ‰€æœ‰resizeåçš„å›¾åƒ
        self._resized_cache = {}
        print(f"  Pre-loading and caching {len(unique_paths)} images (this may take a moment)...")
        for path in tqdm(unique_paths, desc='  Caching images', unit='img'):
            if os.path.exists(path):
                try:
                    img = pil_loader(path)
                    self._resized_cache[path] = self.resize_transform(img)
                except Exception as e:
                    print(f"  Warning: Failed to load {path}: {e}")
                    continue
        print(f"  âœ“ Cached {len(self._resized_cache)} images in memory")
    
    def __getitem__(self, index):
        path, target = self.samples[index]
        # ä»ç¼“å­˜ä¸­è·å–é¢„å…ˆresizeçš„å›¾åƒ
        resized_img = self._resized_cache.get(path)
        if resized_img is None:
            # Fallbackï¼šå¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œé‡æ–°åŠ è½½
            img = pil_loader(path)
            resized_img = self.resize_transform(img)
            self._resized_cache[path] = resized_img
        
        # åªéœ€è¦åšCenterCrop + ToTensor + Normalizeï¼ˆå¾ˆå¿«ï¼‰
        sample = self.crop_transform(resized_img)
        return sample, target
    
    def __len__(self):
        return len(self.samples)


def test_on_dataset(dataset_name, dataset_path, json_file, device, model_hyper, 
                   patch_num, dropout_rate, test_random_crop=True):
    """
    åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæµ‹è¯•æ¨¡å‹
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        dataset_path: æ•°æ®é›†è·¯å¾„
        json_file: JSONæ–‡ä»¶å
        device: è®¡ç®—è®¾å¤‡
        model_hyper: HyperNetæ¨¡å‹
        patch_num: æ¯å¼ å›¾åƒçš„patchæ•°é‡
        dropout_rate: Dropoutç‡
        test_random_crop: æ˜¯å¦ä½¿ç”¨RandomCropï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨CenterCropä»¥ä¿è¯å¯å¤ç°æ€§ï¼‰
    
    Returns:
        srcc, plcc, num_images
    """
    print(f'\n{"="*60}')
    print(f'Testing on {dataset_name.upper()} dataset')
    print(f'{"="*60}')
    print(f'Dataset path: {dataset_path}')
    print(f'JSON file: {json_file}')
    print(f'Patch number: {patch_num}')
    print(f'Crop method: {"RandomCrop" if test_random_crop else "CenterCrop"}')
    
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆtransformåœ¨Datasetå†…éƒ¨å¤„ç†ï¼Œå·²ç»ä¼˜åŒ–ï¼‰
    try:
        dataset = JSONTestDataset(dataset_path, json_file, patch_num, test_random_crop)
    except FileNotFoundError as e:
        print(f"  âŒ Error: {e}")
        return None, None, 0
    
    test_loader = torch.utils.data.DataLoader(
        dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)
    
    # æµ‹è¯•ï¼ˆä¸ HyperIQASolver_swin.py ä¸­çš„ test() æ–¹æ³•å®Œå…¨ä¸€è‡´ï¼‰
    model_hyper.train(False)
    pred_scores = []
    gt_scores = []
    
    print(f'  Total batches: {len(test_loader)}')
    
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥åŠ é€Ÿæ¨ç†
        test_loader_with_progress = tqdm(
            test_loader,
            desc=f'  Testing {dataset_name.upper()}',
            total=len(test_loader),
            unit='batch',
            mininterval=1.0
        )
        
        for img, label in test_loader_with_progress:
            img = img.to(device)
            label = label.float().to(device)
            
            # ç”Ÿæˆç›®æ ‡ç½‘ç»œæƒé‡
            paras = model_hyper(img)
            
            # æ„å»ºç›®æ ‡ç½‘ç»œ
            model_target = models.TargetNet(paras, dropout_rate=dropout_rate).to(device)
            model_target.train(False)
            
            # è´¨é‡é¢„æµ‹
            pred = model_target(paras['target_in_vec'])
            
            pred_scores.append(float(pred.item()))
            gt_scores.extend(label.cpu().tolist())
    
    if len(pred_scores) == 0:
        print(f"  âŒ Error: No predictions generated")
        return None, None, 0
    
    # è®¡ç®—æ¯ä¸ªå›¾åƒçš„å¹³å‡åˆ†æ•°ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
    # å‚è€ƒ HyperIQASolver_swin.py çš„ test() æ–¹æ³•
    pred_scores = np.mean(np.reshape(np.array(pred_scores), (-1, patch_num)), axis=1)
    gt_scores = np.mean(np.reshape(np.array(gt_scores), (-1, patch_num)), axis=1)
    
    num_images = len(pred_scores)
    
    # è®¡ç®—ç›¸å…³ç³»æ•°ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
    # SRCC (Spearman Rank Correlation Coefficient)
    srcc, _ = stats.spearmanr(pred_scores, gt_scores)
    
    # PLCC (Pearson Linear Correlation Coefficient)
    plcc, _ = stats.pearsonr(pred_scores, gt_scores)
    
    print(f'  âœ… {dataset_name.upper()} Results:')
    print(f'     Images: {num_images}')
    print(f'     SRCC: {srcc:.4f}')
    print(f'     PLCC: {plcc:.4f}')
    
    return srcc, plcc, num_images


def load_model(checkpoint_path, model_size, device):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
        model_size: æ¨¡å‹å¤§å° (tiny/small/base)
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        model_hyper: åŠ è½½å¥½çš„HyperNetæ¨¡å‹
        dropout_rate: æ¨¡å‹çš„dropoutç‡
    """
    print(f'\n{"="*60}')
    print(f'Loading Model')
    print(f'{"="*60}')
    print(f'Checkpoint: {checkpoint_path}')
    print(f'Model size: {model_size}')
    print(f'Device: {device}')
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    # ä»checkpointæ–‡ä»¶åä¸­æå–dropout_rateï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # ä¾‹å¦‚: best_model_srcc_0.9236_plcc_0.9406.pkl
    dropout_rate = 0.4  # é»˜è®¤å€¼ï¼ˆBaseæ¨¡å‹ä½¿ç”¨0.4ï¼‰
    
    # åŠ è½½æƒé‡ä»¥æ£€æŸ¥æ˜¯å¦åŒ…å«attention
    print(f'  Loading checkpoint...')
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # æ£€æŸ¥checkpointä¸­æ˜¯å¦åŒ…å«attentionæƒé‡
    has_attention = any('multiscale_attention' in key for key in checkpoint.keys())
    print(f'  Checkpoint contains attention: {has_attention}')
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆä¸è®­ç»ƒæ—¶çš„HyperNetä¸€è‡´ï¼‰
    # å‚è€ƒ HyperIQASolver_swin.py çš„ __init__ æ–¹æ³•
    model_hyper = models.HyperNet(
        16, 112, 224, 112, 56, 28, 14, 7,
        use_multiscale=True,  # é»˜è®¤å¯ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆ
        use_attention=has_attention,  # æ ¹æ®checkpointè‡ªåŠ¨æ£€æµ‹
        drop_path_rate=0.3,   # Baseæ¨¡å‹ä½¿ç”¨0.3
        dropout_rate=dropout_rate,
        model_size=model_size
    ).to(device)
    
    # åŠ è½½æƒé‡
    model_hyper.load_state_dict(checkpoint)
    model_hyper.train(False)  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    print(f'  âœ… Model loaded successfully!')
    
    return model_hyper, dropout_rate


def main():
    parser = argparse.ArgumentParser(
        description='Cross-Dataset Testing for Swin Transformer IQA Model',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Test on all datasets with Swin-Tiny model
  python cross_dataset_test.py --checkpoint checkpoints/best_model.pkl --model_size tiny
  
  # Test on specific datasets with Swin-Small model
  python cross_dataset_test.py --checkpoint checkpoints/best_model.pkl --model_size small --datasets koniq spaq
  
  # Use RandomCrop for testing (less reproducible but matches original paper)
  python cross_dataset_test.py --checkpoint checkpoints/best_model.pkl --model_size tiny --test_random_crop
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', dest='checkpoint', type=str, required=True,
                       help='Path to checkpoint file (.pkl)')
    parser.add_argument('--model_size', dest='model_size', type=str, required=True,
                       choices=['tiny', 'small', 'base'],
                       help='Swin Transformer model size: tiny, small, or base')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--datasets', dest='datasets', type=str, nargs='+',
                       default=['koniq', 'spaq', 'kadid', 'agiqa'],
                       choices=['koniq', 'spaq', 'kadid', 'agiqa'],
                       help='Datasets to test on (default: all)')
    parser.add_argument('--test_patch_num', dest='test_patch_num', type=int, default=20,
                       help='Number of patches per image (default: 20)')
    parser.add_argument('--test_random_crop', dest='test_random_crop', action='store_true',
                       help='Use RandomCrop for testing (default: CenterCrop for reproducibility)')
    parser.add_argument('--base_dir', dest='base_dir', type=str, default=None,
                       help='Base directory for datasets (default: script directory)')
    
    config = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    # ä»checkpointè·¯å¾„æå–ä¿¡æ¯
    checkpoint_name = os.path.basename(config.checkpoint).replace('.pkl', '')
    log_filename = f'cross_dataset_test_{config.model_size}_{timestamp}.log'
    log_path = os.path.join(log_dir, log_filename)
    
    # åˆ›å»ºTeeç±»æ¥åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
    class Tee:
        def __init__(self, *files):
            self.files = files
        
        def write(self, data):
            for f in self.files:
                f.write(data)
                f.flush()
        
        def flush(self):
            for f in self.files:
                f.flush()
    
    # é‡å®šå‘stdoutå’Œstderråˆ°æ—¥å¿—æ–‡ä»¶
    log_file = open(log_path, 'w', buffering=1)
    sys.stdout = Tee(sys.stdout, log_file)
    sys.stderr = Tee(sys.stderr, log_file)
    
    print(f'\nğŸ“ Log file: {log_path}\n')
    
    # è®¾ç½®åŸºç¡€ç›®å½•
    if config.base_dir is None:
        base_dir = os.path.dirname(os.path.abspath(__file__))
    else:
        base_dir = config.base_dir
    
    # æ•°æ®é›†é…ç½®
    datasets_config = {
        'koniq': {
            'path': os.path.join(base_dir, 'koniq-test') + '/',
            'json': 'koniq_test.json',  # ä½¿ç”¨KonIQ-10kçš„æµ‹è¯•é›†
            'name': 'KonIQ-10k Test Set'
        },
        'spaq': {
            'path': os.path.join(base_dir, 'spaq-test') + '/',
            'json': 'spaq_test.json',
            'name': 'SPAQ'
        },
        'kadid': {
            'path': os.path.join(base_dir, 'kadid-test') + '/',
            'json': 'kadid_test.json',
            'name': 'KADID-10K'
        },
        'agiqa': {
            'path': os.path.join(base_dir, 'agiqa-test') + '/',
            'json': 'agiqa_test.json',
            'name': 'AGIQA-3K'
        }
    }
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print('\n' + '='*60)
    print('Cross-Dataset Testing Configuration')
    print('='*60)
    print(f'Checkpoint: {config.checkpoint}')
    print(f'Model size: {config.model_size}')
    print(f'Test patch num: {config.test_patch_num}')
    print(f'Crop method: {"RandomCrop" if config.test_random_crop else "CenterCrop"}')
    print(f'Datasets: {", ".join([d.upper() for d in config.datasets])}')
    print('='*60)
    
    # è·å–è®¾å¤‡
    device = get_device()
    print(f'Using device: {device}')
    
    # åŠ è½½æ¨¡å‹
    try:
        model_hyper, dropout_rate = load_model(config.checkpoint, config.model_size, device)
    except Exception as e:
        print(f'\nâŒ Error loading model: {e}')
        import traceback
        traceback.print_exc()
        return
    
    # åœ¨å„æ•°æ®é›†ä¸Šæµ‹è¯•
    results = {}
    for dataset_name in config.datasets:
        dataset_info = datasets_config[dataset_name]
        try:
            srcc, plcc, num_images = test_on_dataset(
                dataset_info['name'],
                dataset_info['path'],
                dataset_info['json'],
                device,
                model_hyper,
                config.test_patch_num,
                dropout_rate,
                config.test_random_crop
            )
            
            if srcc is not None and plcc is not None:
                results[dataset_info['name']] = {
                    'srcc': srcc,
                    'plcc': plcc,
                    'num_images': num_images
                }
            else:
                results[dataset_info['name']] = None
                
        except Exception as e:
            print(f'\nâŒ Error testing on {dataset_info["name"]}: {e}')
            import traceback
            traceback.print_exc()
            results[dataset_info['name']] = None
    
    # æ‰“å°æœ€ç»ˆç»“æœæ±‡æ€»
    print('\n' + '='*60)
    print('FINAL RESULTS SUMMARY')
    print('='*60)
    print(f'Model: Swin-{config.model_size.capitalize()}')
    print(f'Checkpoint: {os.path.basename(config.checkpoint)}')
    print(f'Test patch num: {config.test_patch_num}')
    print('-'*60)
    
    # è¡¨æ ¼å½¢å¼æ‰“å°
    print(f'{"Dataset":<20} {"Images":<10} {"SRCC":<10} {"PLCC":<10}')
    print('-'*60)
    
    for dataset_name, result in results.items():
        if result is not None:
            print(f'{dataset_name:<20} {result["num_images"]:<10} '
                  f'{result["srcc"]:<10.4f} {result["plcc"]:<10.4f}')
        else:
            print(f'{dataset_name:<20} {"Failed":<10} {"-":<10} {"-":<10}')
    
    print('='*60)
    
    # è®¡ç®—å¹³å‡æ€§èƒ½ï¼ˆä»…é’ˆå¯¹æˆåŠŸçš„æ•°æ®é›†ï¼‰
    successful_results = [r for r in results.values() if r is not None]
    if len(successful_results) > 0:
        avg_srcc = np.mean([r['srcc'] for r in successful_results])
        avg_plcc = np.mean([r['plcc'] for r in successful_results])
        print(f'\nAverage across {len(successful_results)} datasets:')
        print(f'  Average SRCC: {avg_srcc:.4f}')
        print(f'  Average PLCC: {avg_plcc:.4f}')
        print('='*60)
    
    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    output_file = f'cross_dataset_results_{config.model_size}_{os.path.basename(config.checkpoint).replace(".pkl", "")}.json'
    output_path = os.path.join(base_dir, output_file)
    
    output_data = {
        'checkpoint': config.checkpoint,
        'model_size': config.model_size,
        'test_patch_num': config.test_patch_num,
        'test_random_crop': config.test_random_crop,
        'results': {k: v for k, v in results.items()}
    }
    
    with open(output_path, 'w') as f:
        json.dump(output_data, f, indent=2)
    
    print(f'\nâœ… Results saved to: {output_path}')
    print(f'\nğŸ“ Log file saved to: {log_path}')
    
    # æ¢å¤æ ‡å‡†è¾“å‡ºå¹¶å…³é—­æ—¥å¿—æ–‡ä»¶
    sys.stdout = sys.__stdout__
    sys.stderr = sys.__stderr__
    log_file.close()


if __name__ == '__main__':
    main()

