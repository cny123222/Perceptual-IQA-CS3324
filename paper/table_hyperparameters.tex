% Table: Complete Experimental Hyperparameters
\begin{table*}[t]
\centering
\caption{Detailed Experimental Hyperparameters and Training Configuration}
\label{tab:hyperparameters}
\begin{tabular}{llccc}
\hline
\multirow{2}{*}{Category} & \multirow{2}{*}{Hyperparameter} & \multicolumn{3}{c}{SMART-IQA Variants} \\
\cline{3-5}
& & Tiny & Small & Base \\
\hline
\multicolumn{5}{l}{\textbf{Model Architecture}} \\
& Backbone & Swin-T & Swin-S & Swin-B \\
& Pretrained Weights & ImageNet-21K & ImageNet-21K & ImageNet-21K \\
& Input Resolution & $224\times224$ & $224\times224$ & $224\times224$ \\
& Patch Size & $4\times4$ & $4\times4$ & $4\times4$ \\
& Embed Dim & 96 & 96 & 128 \\
& Depths & [2,2,6,2] & [2,2,18,2] & [2,2,18,2] \\
& Num Heads & [3,6,12,24] & [3,6,12,24] & [4,8,16,32] \\
& Window Size & $7\times7$ & $7\times7$ & $7\times7$ \\
& Multi-scale Fusion & \checkmark & \checkmark & \checkmark \\
& Channel Attention & \checkmark & \checkmark & \checkmark \\
& Feature Dimensions & [96,192,384,768] & [96,192,384,768] & [128,256,512,1024] \\
& Target FC Sizes & [112,224,112,56] & [112,224,112,56] & [112,224,112,56] \\
\hline
\multicolumn{5}{l}{\textbf{Training Strategy}} \\
& Optimizer & AdamW & AdamW & AdamW \\
& Learning Rate (Backbone) & $5\times10^{-7}$ & $5\times10^{-7}$ & $5\times10^{-7}$ \\
& Learning Rate (Others) & $5\times10^{-6}$ & $5\times10^{-6}$ & $5\times10^{-6}$ \\
& Weight Decay & 0.0002 & 0.0002 & 0.0002 \\
& Batch Size & 32 & 32 & 32 \\
& Epochs & 10 & 10 & 10 \\
& Loss Function & L1 (MAE) & L1 (MAE) & L1 (MAE) \\
& Drop Path Rate & 0.2 & 0.25 & 0.3 \\
& Dropout Rate & 0.3 & 0.35 & 0.4 \\
& Gradient Clipping & None & None & None \\
\hline
\multicolumn{5}{l}{\textbf{Data Augmentation}} \\
& Training Patches per Image & 20 & 20 & 20 \\
& Test Patches per Image & 20 & 20 & 20 \\
& Random Horizontal Flip & \checkmark & \checkmark & \checkmark \\
& Random Crop & \checkmark & \checkmark & \checkmark \\
& Resize & $(512, 384)$ & $(512, 384)$ & $(512, 384)$ \\
& Crop Size & $224\times224$ & $224\times224$ & $224\times224$ \\
& Color Jitter & $\times$ & $\times$ & $\times$ \\
& Normalization & ImageNet & ImageNet & ImageNet \\
\hline
\multicolumn{5}{l}{\textbf{Dataset Split}} \\
& Training Images & 7,046 & 7,046 & 7,046 \\
& Test Images & 2,010 & 2,010 & 2,010 \\
& Total Images & 10,073 & 10,073 & 10,073 \\
& Dataset & KonIQ-10k & KonIQ-10k & KonIQ-10k \\
\hline
\multicolumn{5}{l}{\textbf{Computational Complexity}} \\
& Parameters (M) & 29.52 & 50.84 & 89.11 \\
& FLOPs (G) & 4.47 & 8.65 & 15.28 \\
& Inference Time (ms) & 6.00 & 10.62 & 10.06 \\
& Throughput (FPS) & 166.7 & 94.2 & 99.4 \\
\hline
\multicolumn{5}{l}{\textbf{Training Resources}} \\
& GPU & NVIDIA A100 & NVIDIA A100 & NVIDIA A100 \\
& Training Time per Epoch & ~8 min & ~9 min & ~10 min \\
& Total Training Time & ~1.3h & ~1.5h & ~1.7h \\
\hline
\end{tabular}
\end{table*}

