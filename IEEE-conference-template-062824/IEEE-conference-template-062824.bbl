% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{su2020hyperiq}
S.~Su, Q.~Yan, Y.~Zhu, C.~Zhang, X.~Ge, J.~Sun, and Y.~Zhang, ``Blindly assess
  image quality in the wild guided by a self-adaptive hyper network,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2020, pp. 3667--3676.

\bibitem{dosovitskiy2021vit}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.},
  ``An image is worth 16x16 words: Transformers for image recognition at
  scale,'' in \emph{International Conference on Learning Representations},
  2021.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2021, pp. 10\,012--10\,022.

\bibitem{talebi2018nima}
H.~Talebi and P.~Milanfar, ``Nima: Neural image assessment,'' \emph{IEEE
  Transactions on Image Processing}, vol.~27, no.~8, pp. 3998--4011, 2018.

\bibitem{ying2020paq2piq}
Z.~Ying, H.~Niu, P.~Gupta, D.~Mahajan, D.~Ghadiyaram, and A.~Bovik, ``From
  patches to pictures (paq-2-piq): Mapping the perceptual space of picture
  quality,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2020, pp. 3575--3585.

\bibitem{ke2021musiq}
J.~Ke, Q.~Wang, Y.~Wang, P.~Milanfar, and F.~Yang, ``Musiq: Multi-scale image
  quality transformer,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2021, pp. 5148--5157.

\bibitem{yang2022maniqa}
S.~Yang, T.~Wu, S.~Shi, S.~Lao, Y.~Gong, M.~Cao, J.~Wang, and Y.~Yang,
  ``Maniqa: Multi-dimension attention network for no-reference image quality
  assessment,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition Workshops}, 2022, pp. 1191--1200.

\bibitem{golestaneh2022tres}
S.~A. Golestaneh, S.~Dadsetan, and K.~M. Kitani, ``No-reference image quality
  assessment via transformers, relative ranking, and self-consistency,'' in
  \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision}, 2022, pp. 1220--1230.

\bibitem{hosu2020koniq}
V.~Hosu, H.~Lin, T.~Sziranyi, and D.~Saupe, ``Koniq-10k: An ecologically valid
  database for deep learning of blind image quality assessment,'' \emph{IEEE
  Transactions on Image Processing}, vol.~29, pp. 4041--4056, 2020.

\bibitem{fang2020perceptual}
Y.~Fang, H.~Zhu, Y.~Zeng, K.~Ma, and Z.~Wang, ``Perceptual quality assessment
  of smartphone photography,'' in \emph{Proceedings of the IEEE/CVF Conference
  on Computer Vision and Pattern Recognition}, 2020, pp. 3677--3686.

\bibitem{lin2019kadid}
H.~Lin, V.~Hosu, and D.~Saupe, ``Kadid-10k: A large-scale artificially
  distorted iqa database,'' in \emph{2019 Eleventh International Conference on
  Quality of Multimedia Experience (QoMEX)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2019, pp. 1--3.

\bibitem{li2023agiqa}
C.~Li, Z.~Zhang, H.~Wu, W.~Sun, X.~Min, X.~Liu, G.~Zhai, and W.~Lin,
  ``Agiqa-3k: An open database for ai-generated image quality assessment,''
  \emph{IEEE Transactions on Circuits and Systems for Video Technology}, 2023.

\bibitem{bosse2017wadiqam}
S.~Bosse, D.~Maniry, K.-R. Muller, T.~Wiegand, and W.~Samek, ``Deep neural
  networks for no-reference and full-reference image quality assessment,'' in
  \emph{IEEE Transactions on Image Processing}, vol.~27, no.~1.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2017, pp. 206--219.

\bibitem{zhang2018dbcnn}
W.~Zhang, K.~Ma, J.~Yan, D.~Deng, and Z.~Wang, ``Blind image quality assessment
  using a deep bilinear convolutional neural network,'' \emph{IEEE Transactions
  on Circuits and Systems for Video Technology}, vol.~30, no.~1, pp. 36--47,
  2018.

\bibitem{zeng2021pqr}
H.~Zeng, L.~Zhang, and A.~C. Bovik, ``Perceptual quality assessment of
  omnidirectional images as moving camera videos,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp.
  3423--3432.

\bibitem{li2022sfa}
D.~Li, T.~Jiang, W.~Lin, and M.~Jiang, ``Blindly assess image quality in the
  wild guided by a self-adaptive hyper network,'' \emph{IEEE Transactions on
  Circuits and Systems for Video Technology}, vol.~32, no.~3, pp. 1287--1299,
  2022.

\bibitem{sun2024stairiqa}
W.~Sun, H.~Zhang, L.~Liao, Y.~Wei, G.~Zhai, and X.~Min, ``Stairiqa: Towards
  staircase-shaped quality scales for blind image quality assessment,''
  \emph{IEEE Transactions on Multimedia}, 2024.

\bibitem{zhang2021unique}
W.~Zhang, K.~Ma, G.~Zhai, and X.~Yang, ``Uncertainty-aware blind image quality
  assessment in the laboratory and wild,'' in \emph{Proceedings of the IEEE/CVF
  International Conference on Computer Vision}, 2021, pp. 3142--3151.

\bibitem{zhang2023liqe}
W.~Zhang, G.~Zhai, Y.~Wei, X.~Yang, and K.~Ma, ``Blind image quality assessment
  via vision-language correspondence: A multitask learning perspective,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2023, pp. 14\,071--14\,081.

\bibitem{wang2023clipiqa}
J.~Wang, K.~C. Chan, and C.~C. Loy, ``Exploring clip for assessing the look and
  feel of images,'' in \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, vol.~37, 2023, pp. 2555--2563.

\end{thebibliography}
