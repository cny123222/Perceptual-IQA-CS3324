\relax 
\citation{su2020hyperiq}
\citation{dosovitskiy2021vit}
\citation{liu2021swin}
\citation{talebi2018nima}
\citation{ying2020paq2piq}
\citation{ke2021musiq}
\citation{yang2022maniqa}
\citation{golestaneh2022tres}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Blind Image Quality Assessment}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Transformer-based IQA}{1}{}\protected@file@percent }
\citation{su2020hyperiq}
\citation{liu2021swin}
\citation{su2020hyperiq}
\citation{hosu2020koniq}
\citation{fang2020perceptual}
\citation{lin2019kadid}
\citation{li2023agiqa}
\citation{talebi2018nima}
\citation{ying2020paq2piq}
\citation{su2020hyperiq}
\citation{ke2021musiq}
\citation{golestaneh2022tres}
\citation{yang2022maniqa}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Hyper Networks for IQA}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Overview}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Swin Transformer Backbone}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Multi-scale Feature Fusion}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Channel Attention Mechanism}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-E}}HyperNet and TargetNet}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-F}}Training Strategy}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiments}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Experimental Setup}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}1}Datasets}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}2}Evaluation Metrics}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}3}Implementation Details}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Comparison with State-of-the-Art}{2}{}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{su2020hyperiq}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training curves of the best model (Swin-Base, LR=$5\times 10^{-7}$). Left: Training loss decreases from 11.64 to 3.66 over 10 epochs. Middle: Validation SRCC with best performance at Epoch 8 (0.9378, marked with gold star). Right: Validation PLCC reaches 0.9485 at Epoch 8. The model shows stable convergence without overfitting.}}{3}{}\protected@file@percent }
\newlabel{fig:training_curves}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Ablation Study}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Cross-Dataset Generalization}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-E}}Model Variants}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{3}{}\protected@file@percent }
\bibcite{dosovitskiy2021vit}{2}
\bibcite{liu2021swin}{3}
\bibcite{talebi2018nima}{4}
\bibcite{ying2020paq2piq}{5}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Performance comparison with state-of-the-art methods on KonIQ-10k dataset}}{4}{}\protected@file@percent }
\newlabel{tab:sota_comparison}{{I}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Ablation study visualization. Left: SRCC comparison showing Swin Transformer contributes 87\% of total improvement. Right: PLCC comparison demonstrating consistent gains across both metrics. The full model achieves SRCC of 0.9378 and PLCC of 0.9485.}}{4}{}\protected@file@percent }
\newlabel{fig:ablation}{{2}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Ablation study on KonIQ-10k: component contribution analysis}}{4}{}\protected@file@percent }
\newlabel{tab:ablation_study}{{II}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Cross-dataset generalization performance (trained on KonIQ-10k)}}{4}{}\protected@file@percent }
\newlabel{tab:cross_dataset}{{III}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Performance-efficiency trade-off across model sizes on KonIQ-10k}}{4}{}\protected@file@percent }
\newlabel{tab:model_size}{{IV}{4}}
\bibcite{ke2021musiq}{6}
\bibcite{yang2022maniqa}{7}
\bibcite{golestaneh2022tres}{8}
\bibcite{hosu2020koniq}{9}
\bibcite{fang2020perceptual}{10}
\bibcite{lin2019kadid}{11}
\bibcite{li2023agiqa}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cross-dataset performance heatmap comparing HyperIQA and SMART-IQA. Our method consistently outperforms HyperIQA across most datasets, demonstrating strong generalization capability.}}{5}{}\protected@file@percent }
\newlabel{fig:cross_dataset}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance vs model size trade-off. Left: SRCC comparison showing all variants outperform HyperIQA baseline. Right: Parameter-performance scatter plot highlighting the evolution path. Small variant offers the best balance for deployment.}}{5}{}\protected@file@percent }
\newlabel{fig:model_size}{{4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Learning rate sensitivity analysis. Left: SRCC vs learning rate showing optimal LR at $5\times 10^{-7}$ (marked with gold star). Right: Training efficiency showing faster convergence with larger learning rates but slightly worse performance. The y-axis range is extended to better visualize the stability of the training process.}}{5}{}\protected@file@percent }
\newlabel{fig:lr_sensitivity}{{5}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Loss function comparison on KonIQ-10k}}{5}{}\protected@file@percent }
\newlabel{tab:loss_comparison}{{V}{5}}
\@writefile{toc}{\contentsline {section}{Appendix}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Learning Rate Sensitivity}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Data Augmentation}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Loss Function Comparison}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Loss function performance comparison. Left: SRCC comparison showing L1 (MAE) achieves the best performance. Right: SRCC vs PLCC scatter plot demonstrating the consistency of L1 loss across both metrics.}}{6}{}\protected@file@percent }
\newlabel{fig:loss_comparison}{{6}{6}}
\gdef \@abspage@last{6}
