\relax 
\citation{mittal2012brisque,mittal2013niqe}
\citation{zhang2018dbcnn,talebi2018nima}
\citation{su2020hyperiq}
\citation{dosovitskiy2021vit,liu2021swin}
\citation{liu2021swin}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\citation{mittal2012brisque,mittal2013niqe}
\citation{bosse2017wadiqam}
\citation{zhang2018dbcnn}
\citation{talebi2018nima}
\citation{ying2020paq2piq}
\citation{su2020hyperiq}
\citation{ke2021musiq}
\citation{yang2022maniqa}
\citation{golestaneh2022tres}
\citation{zhang2023liqe}
\citation{wang2023clipiqa}
\citation{su2020hyperiq}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Overview}{2}{}\protected@file@percent }
\citation{liu2021swin}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture of SMART-IQA. The pipeline consists of: (1) Swin Transformer backbone with $K$ hierarchical stages extracting multi-scale features $\{F^1, F^2, \ldots  , F^K\}$ with progressively decreasing spatial resolutions and increasing channel dimensions, (2) Adaptive Feature Aggregation (AFA) module that unifies spatial dimensions through adaptive pooling and $1\times 1$ convolution, producing aligned features $\{F^1_{\text  {pool}}, F^2_{\text  {pool}}, \ldots  , F^{K-1}_{\text  {pool}}\}$ at target resolution $H_{\text  {target}} \times W_{\text  {target}}$, (3) Channel Attention Fusion module that uses the deepest feature $F^K$ to generate attention weights $\alpha = \{\alpha _1, \alpha _2, \ldots  , \alpha _K\}$ via global pooling and FC layers, dynamically weighting multi-scale features based on content, (4) HyperNet that generates dynamic parameters $\theta _x = \{W_1, b_1, W_2, b_2\}$ for the TargetNet based on $F^K$, and (5) TargetNet that predicts the final quality score $q$ using the attention-weighted feature vector $v_x$ and content-adaptive parameters $\theta _x$. The orange-highlighted attention module enables content-aware feature fusion, while the red dashed arrows indicate dynamic weight generation. In our implementation, we use $K=4$ stages (see Section 3.3 for details).}}{3}{}\protected@file@percent }
\newlabel{fig:architecture}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Hierarchical Feature Extraction via Swin Transformer}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Adaptive Feature Aggregation (AFA) Module}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Channel Attention for Content-Aware Feature Weighting}{4}{}\protected@file@percent }
\citation{su2020hyperiq}
\citation{hosu2020koniq}
\citation{fang2020perceptual}
\citation{lin2019kadid}
\citation{li2023agiqa}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-E}}Content-Adaptive Quality Prediction}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-F}}Training Objective}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiments}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Experimental Setup}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}1}Datasets}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}2}Evaluation Metrics}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}3}Implementation Details}{5}{}\protected@file@percent }
\citation{bosse2017wadiqam}
\citation{li2022sfa}
\citation{zhang2018dbcnn}
\citation{zeng2021pqr}
\citation{su2020hyperiq}
\citation{wang2023clipiqa}
\citation{zhang2021unique}
\citation{sun2024stairiqa}
\citation{ke2021musiq}
\citation{zhang2023liqe}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Comparison with State-of-the-Art}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Ablation Study}{6}{}\protected@file@percent }
\newlabel{sec:ablation}{{\mbox  {IV-C}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training curves of the best model (Swin-Base, LR=$5\times 10^{-7}$). Left: Training loss decreases from 11.64 to 3.66 over 10 epochs. Middle: Validation SRCC with best performance at Epoch 8 (0.9378). Right: Validation PLCC reaches 0.9485 at Epoch 8. The model shows stable convergence without overfitting.}}{7}{}\protected@file@percent }
\newlabel{fig:training_curves}{{2}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Performance comparison with state-of-the-art methods on KonIQ-10k dataset. Best results are in bold.}}{7}{}\protected@file@percent }
\newlabel{tab:sota_comparison}{{I}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Cross-Dataset Generalization}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Ablation study visualization. Left: SRCC comparison showing Swin Transformer contributes 87\% of total improvement (+0.0268). Right: PLCC comparison demonstrating consistent gains across both metrics. The progressive improvements show: Swin-Base backbone (+2.68\% SRCC), AFA module (+0.15\% SRCC), and attention mechanism (+0.25\% SRCC). The full model achieves SRCC of 0.9378 and PLCC of 0.9485.}}{7}{}\protected@file@percent }
\newlabel{fig:ablation}{{3}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Ablation study on KonIQ-10k: component contribution analysis}}{8}{}\protected@file@percent }
\newlabel{tab:ablation_study}{{II}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-E}}Performance-Efficiency Trade-off Analysis}{8}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Cross-dataset generalization performance (trained on KonIQ-10k)}}{8}{}\protected@file@percent }
\newlabel{tab:cross_dataset}{{III}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Performance-efficiency trade-off across model sizes on KonIQ-10k}}{9}{}\protected@file@percent }
\newlabel{tab:model_size}{{IV}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance vs model size trade-off. Left: SRCC comparison showing all variants outperform HyperIQA baseline. Right: Parameter-performance scatter plot highlighting the evolution path. Small variant offers the best balance for deployment.}}{9}{}\protected@file@percent }
\newlabel{fig:model_size}{{4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-F}}Channel Attention Mechanism Analysis}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Channel attention weight distribution for images of different quality levels. Top: Attention weights across four Swin Transformer stages. Low-quality image (left) shows balanced multi-scale attention, while high-quality image (right) concentrates 99.67\% weight on Stage 3. Bottom: Visual examples with ground truth and predicted quality scores. This adaptive attention mechanism enables content-aware feature fusion.}}{9}{}\protected@file@percent }
\newlabel{fig:attention_analysis}{{5}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{9}{}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{mittal2012brisque}{1}
\bibcite{mittal2013niqe}{2}
\bibcite{zhang2018dbcnn}{3}
\bibcite{talebi2018nima}{4}
\bibcite{su2020hyperiq}{5}
\bibcite{dosovitskiy2021vit}{6}
\bibcite{liu2021swin}{7}
\bibcite{bosse2017wadiqam}{8}
\bibcite{ying2020paq2piq}{9}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Scatter plot of predicted vs ground truth MOS scores on KonIQ-10k test set (500 images). Blue dots represent normal predictions, while red dots indicate large errors (top 10\%). The close clustering around the diagonal line demonstrates high prediction accuracy, with SRCC of 0.9374 and PLCC of 0.9479.}}{10}{}\protected@file@percent }
\newlabel{fig:error_analysis}{{6}{10}}
\@writefile{toc}{\contentsline {section}{References}{10}{}\protected@file@percent }
\bibcite{ke2021musiq}{10}
\bibcite{yang2022maniqa}{11}
\bibcite{golestaneh2022tres}{12}
\bibcite{zhang2023liqe}{13}
\bibcite{wang2023clipiqa}{14}
\bibcite{hosu2020koniq}{15}
\bibcite{fang2020perceptual}{16}
\bibcite{lin2019kadid}{17}
\bibcite{li2023agiqa}{18}
\bibcite{li2022sfa}{19}
\bibcite{zeng2021pqr}{20}
\bibcite{zhang2021unique}{21}
\bibcite{sun2024stairiqa}{22}
\citation{su2020hyperiq}
\@writefile{toc}{\contentsline {section}{Appendix}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A}HyperNetwork Architecture}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}TargetNetwork Details}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Model Architecture Specifications}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Design Rationale}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Complete Hyperparameter Configuration}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Learning Rate Sensitivity Analysis}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G}Training Dynamics and Convergence Analysis}{12}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Detailed Experimental Hyperparameters and Training Configuration}}{13}{}\protected@file@percent }
\newlabel{tab:hyperparameters}{{V}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H}Computational Complexity and Efficiency Analysis}{13}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Epoch-wise Training Log of Best Model (Swin-Base, LR=$5\times 10^{-7}$)}}{14}{}\protected@file@percent }
\newlabel{tab:training_log}{{VI}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Learning rate sensitivity analysis. SRCC (red, left y-axis) and PLCC (cyan, right y-axis) versus learning rate. The optimal learning rate of $5\times 10^{-7}$ (marked with gold star) achieves the best SRCC performance. Both metrics are unified to [0.930, 0.950] range for better comparison.}}{14}{}\protected@file@percent }
\newlabel{fig:lr_sensitivity}{{7}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I}Data Augmentation Strategy Analysis}{14}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Computational Complexity Comparison across Model Variants}}{15}{}\protected@file@percent }
\newlabel{tab:complexity}{{VII}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {J}Loss Function Comparison and Analysis}{15}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces Loss function comparison on KonIQ-10k}}{15}{}\protected@file@percent }
\newlabel{tab:loss_comparison}{{VIII}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {K}Visualization Methodology}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Loss function performance comparison. Left: SRCC comparison showing L1 (MAE) achieves the best performance. Right: SRCC vs PLCC scatter plot demonstrating the consistency of L1 loss across both metrics.}}{16}{}\protected@file@percent }
\newlabel{fig:loss_comparison}{{8}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {L}High-Quality Image Analysis (Figure\nobreakspace  {}9\hbox {})}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {M}Low-Quality Image Analysis (Figure\nobreakspace  {}10\hbox {})}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {N}Connection to Channel Attention}{16}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Feature map visualization for a high-quality image (MOS 81.4/100). The hierarchical feature extraction shows increasing activation strength from low to high stages, with Stage 3-4 dominating. This pattern indicates the model relies on semantic understanding for high-quality assessment.}}{17}{}\protected@file@percent }
\newlabel{fig:feature_high}{{9}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Feature map visualization for a low-quality image (MOS 20.1/100). Strong activations in lower stages (Stage 1-2) indicate the model focuses on local distortions (compression artifacts, noise, texture degradation), while higher stages show weaker activations due to semantic content being obscured by distortions.}}{17}{}\protected@file@percent }
\newlabel{fig:feature_low}{{10}{17}}
\gdef \@abspage@last{17}
