# SMART-IQA è®ºæ–‡è¡¨æ ¼ - æœ€ç»ˆç‰ˆæœ¬ ğŸ“Š

**æ—¥æœŸ**: 2024-12-24  
**è¯´æ˜**: è¿™äº›è¡¨æ ¼å¯ä»¥ç›´æ¥å¤åˆ¶åˆ°LaTeXè®ºæ–‡ä¸­

---

## è¡¨1: SOTAå¯¹æ¯” - KonIQ-10kæ€§èƒ½å¯¹æ¯”

**ç”¨é€”**: å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰SOTAæ–¹æ³•çš„å¯¹æ¯”

```latex
\begin{table*}[t]
\centering
\caption{Performance comparison with state-of-the-art methods on KonIQ-10k dataset}
\label{tab:sota_comparison}
\begin{tabular}{lccccc}
\hline
Method & Year & Backbone & SRCC & PLCC & Params \\
\hline
NIMA \cite{talebi2018nima} & 2018 & InceptionNet & 0.558 & 0.590 & - \\
DBCNN & 2020 & ResNet50 & 0.875 & 0.884 & - \\
PaQ-2-PiQ \cite{ying2020paq2piq} & 2020 & ResNet18 & 0.892 & 0.904 & - \\
HyperIQA \cite{su2020hyperiq} & 2020 & ResNet50 & 0.906 & 0.917 & 25M \\
MUSIQ \cite{ke2021musiq} & 2021 & Multi-scale ViT & 0.915 & 0.930 & 150M \\
TReS \cite{golestaneh2022tres} & 2022 & Transformer & 0.908 & 0.922 & - \\
MANIQA \cite{yang2022maniqa} & 2022 & ViT-Small & 0.920 & 0.930 & 46M \\
LIQE \cite{zhang2023liqe} & 2023 & CLIP & 0.919 & 0.927 & 120M \\
Q-Align \cite{wu2023qalign} & 2023 & mPLUG-Owl & 0.921 & 0.933 & - \\
\hline
\textbf{SMART-IQA (Ours)} & 2024 & Swin-Base & \textbf{0.9378} & \textbf{0.9485} & 88M \\
\hline
\textbf{Improvement over Best} & - & - & \textbf{+1.68\%} & \textbf{+1.55\%} & - \\
\textbf{Improvement over HyperIQA} & - & - & \textbf{+3.18\%} & \textbf{+3.15\%} & - \\
\hline
\end{tabular}
\end{table*}
```

**è¯´æ˜**: 
- è¡¨å¤´ `\begin{table*}` è¡¨ç¤ºåŒæ å®½åº¦ï¼Œé€‚åˆä¼šè®®è®ºæ–‡
- éœ€è¦å¼•ç”¨ç›¸åº”çš„æ–‡çŒ®ï¼ˆå·²åœ¨references.bibä¸­ï¼‰
- **åŠ ç²—**æ•°å­—è¡¨ç¤ºæœ€ä½³ç»“æœ

---

## è¡¨2: æ¶ˆèå®éªŒ - ç»„ä»¶è´¡çŒ®åˆ†æ

**ç”¨é€”**: å±•ç¤ºæ¯ä¸ªç»„ä»¶çš„è´¡çŒ®

```latex
\begin{table}[t]
\centering
\caption{Ablation study on KonIQ-10k: component contribution analysis}
\label{tab:ablation_study}
\begin{tabular}{lcccc}
\hline
Configuration & Backbone & Multi-Scale & Attention & SRCC & PLCC \\
\hline
\multicolumn{6}{l}{\textit{Baseline}} \\
C0: HyperIQA & ResNet50 & - & - & 0.9070 & 0.9180 \\
\hline
\multicolumn{6}{l}{\textit{Progressive Ablation (Swin-Base)}} \\
A2: Backbone only & Swin-Base & \xmark & \xmark & 0.9338 & 0.9438 \\
A1: + Multi-Scale & Swin-Base & \cmark & \xmark & 0.9353 & 0.9458 \\
E6: + Attention & Swin-Base & \cmark & \cmark & \textbf{0.9378} & \textbf{0.9485} \\
\hline
\multicolumn{6}{l}{\textit{Component Contributions}} \\
Swin Transformer & \multicolumn{4}{l}{+0.0268 SRCC (+2.68\%, 87\% of total gain)} \\
Multi-Scale Fusion & \multicolumn{4}{l}{+0.0015 SRCC (+0.15\%, 5\% of total gain)} \\
Attention Mechanism & \multicolumn{4}{l}{+0.0025 SRCC (+0.25\%, 8\% of total gain)} \\
\hline
\textbf{Total Improvement} & \multicolumn{4}{l}{\textbf{+0.0308 SRCC (+3.08\%)}} \\
\hline
\end{tabular}
\end{table}
```

**è¯´æ˜**:
- ä½¿ç”¨ `\cmark` å’Œ `\xmark` éœ€è¦åœ¨LaTeXåºè¨€ä¸­æ·»åŠ : `\usepackage{amssymb}`
- æˆ–è€…ç”¨ `\checkmark` å’Œ `-` ä»£æ›¿
- æ¸…æ¥šå±•ç¤ºäº†æ¸è¿›å¼æ¶ˆèçš„è¿‡ç¨‹

---

## è¡¨3: è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›

**ç”¨é€”**: å¯¹æ¯”HyperIQAå’ŒSMART-IQAçš„æ³›åŒ–èƒ½åŠ›

```latex
\begin{table}[t]
\centering
\caption{Cross-dataset generalization performance (trained on KonIQ-10k)}
\label{tab:cross_dataset}
\begin{tabular}{lccccc}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Type} & \multicolumn{2}{c}{HyperIQA} & \multicolumn{2}{c}{SMART-IQA (Ours)} \\
\cline{3-6}
& & SRCC & PLCC & SRCC & PLCC \\
\hline
KonIQ-10k & In-domain & 0.9060 & 0.9170 & \textbf{0.9378} & \textbf{0.9485} \\
\hline
\multicolumn{6}{l}{\textit{Cross-dataset Evaluation}} \\
SPAQ & Smartphone & 0.8490 & 0.8465 & \textbf{0.8698} & \textbf{0.8709} \\
KADID-10K & Synthetic & 0.4848 & 0.5160 & \textbf{0.5412} & \textbf{0.5591} \\
AGIQA-3K & AI-generated & 0.6627 & 0.7236 & 0.6484 & 0.6830 \\
\hline
\textbf{Avg (Cross)} & - & 0.6655 & 0.6954 & \textbf{0.6865} & \textbf{0.7044} \\
\hline
\multicolumn{6}{l}{\textit{Improvement}} \\
In-domain & - & \multicolumn{2}{c}{-} & \multicolumn{2}{c}{+3.18\% / +3.15\%} \\
Cross-domain & - & \multicolumn{2}{c}{-} & \multicolumn{2}{c}{+2.10\% / +0.90\%} \\
\hline
\end{tabular}
\end{table}
```

**è¯´æ˜**:
- ä½¿ç”¨ `\multirow` éœ€è¦æ·»åŠ åŒ…: `\usepackage{multirow}`
- æ¸…æ¥šå±•ç¤ºäº†åœ¨ä¸åŒç±»å‹æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›
- AGIQA-3Kæˆ‘ä»¬åè€Œç•¥å·®ï¼Œè¿™æ˜¯è¯šå®çš„å±•ç¤ºï¼Œå¯ä»¥åœ¨æ­£æ–‡ä¸­è®¨è®º

---

## è¡¨4: æ¨¡å‹å¤§å°å¯¹æ¯” - ç²¾åº¦ä¸æ•ˆç‡æƒè¡¡

**ç”¨é€”**: å±•ç¤ºä¸åŒæ¨¡å‹å¤§å°çš„æ€§èƒ½-æ•ˆç‡æƒè¡¡

```latex
\begin{table}[t]
\centering
\caption{Performance-efficiency trade-off across model sizes on KonIQ-10k}
\label{tab:model_size}
\begin{tabular}{lcccccc}
\hline
Model & Params & FLOPs & SRCC & PLCC & Time & FPS \\
\hline
\multicolumn{7}{l}{\textit{Baseline}} \\
HyperIQA & 25M & 4.0G & 0.9070 & 0.9180 & - & $\sim$100 \\
\hline
\multicolumn{7}{l}{\textit{SMART-IQA Variants}} \\
Tiny & 28M & $\sim$5G & 0.9249 & 0.9360 & 1.5h & $\sim$25 \\
Small & 50M & $\sim$11G & 0.9338 & 0.9455 & 1.7h & $\sim$23 \\
\textbf{Base} & 88M & 18.2G & \textbf{0.9378} & \textbf{0.9485} & 1.7h & $\sim$22 \\
\hline
\multicolumn{7}{l}{\textit{Analysis: Base vs Small}} \\
Params Reduction & \multicolumn{6}{l}{-43\% parameters, only -0.40\% SRCC loss} \\
\multicolumn{7}{l}{\textit{Analysis: Base vs Tiny}} \\
Params Reduction & \multicolumn{6}{l}{-68\% parameters, -1.29\% SRCC loss} \\
\hline
\multicolumn{7}{l}{\textit{Recommendation:}} \\
\multicolumn{7}{l}{â€¢ Base: Best accuracy for research and benchmarking} \\
\multicolumn{7}{l}{â€¢ Small: Best balance for deployment (43\% smaller, 0.4\% loss)} \\
\multicolumn{7}{l}{â€¢ Tiny: Resource-constrained scenarios (68\% smaller, 1.3\% loss)} \\
\hline
\end{tabular}
\end{table}
```

**è¯´æ˜**:
- FPS (Frames Per Second) = æ¯ç§’å¤„ç†å›¾åƒæ•°
- Time = è®­ç»ƒæ—¶é—´ï¼ˆ10 epochsï¼‰
- æ¸…æ¥šå±•ç¤ºäº†æ¨¡å‹å¤§å°ä¸æ€§èƒ½çš„æƒè¡¡

---

## è¡¨5 (å¯é€‰): æŸå¤±å‡½æ•°å¯¹æ¯”

**ç”¨é€”**: å¦‚æœè¦è¯¦ç»†å±•ç¤ºæŸå¤±å‡½æ•°å®éªŒ

```latex
\begin{table}[t]
\centering
\caption{Loss function comparison on KonIQ-10k (Swin-Base)}
\label{tab:loss_function}
\begin{tabular}{lcccc}
\hline
Loss Function & SRCC & PLCC & $\Delta$ SRCC & Ranking \\
\hline
\textbf{L1 (MAE)} & \textbf{0.9375} & \textbf{0.9488} & - & ğŸ¥‡ 1st \\
L2 (MSE) & 0.9373 & 0.9469 & -0.0002 & ğŸ¥ˆ 2nd \\
Pairwise Fidelity & 0.9315 & 0.9373 & -0.0060 & ğŸ¥‰ 3rd \\
SRCC Loss & 0.9313 & 0.9416 & -0.0062 & 4th \\
Pairwise Ranking & 0.9292 & 0.9249 & -0.0083 & 5th \\
\hline
\multicolumn{5}{l}{\textit{Note: Simple L1 loss outperforms complex ranking-based losses}} \\
\hline
\end{tabular}
\end{table}
```

---

## è¡¨6 (å¯é€‰): å­¦ä¹ ç‡æ•æ„Ÿåº¦

**ç”¨é€”**: å±•ç¤ºå­¦ä¹ ç‡å®éªŒç»“æœ

```latex
\begin{table}[t]
\centering
\caption{Learning rate sensitivity analysis (Swin-Base)}
\label{tab:lr_sensitivity}
\begin{tabular}{lcccc}
\hline
Learning Rate & SRCC & PLCC & $\Delta$ SRCC & Epochs \\
\hline
5e-6 & 0.9354 & 0.9448 & -0.24\% & 5 \\
3e-6 & 0.9364 & 0.9464 & -0.14\% & 5 \\
1e-6 & 0.9374 & 0.9485 & -0.04\% & 10 \\
\textbf{5e-7} & \textbf{0.9378} & \textbf{0.9485} & \textbf{0.0\%} & \textbf{10} \\
1e-7 & 0.9375 & 0.9488 & -0.03\% & 14 \\
\hline
\multicolumn{5}{l}{\textit{Note: Optimal LR is 200$\times$ lower than ResNet50 (1e-4)}} \\
\multicolumn{5}{l}{\textit{Swin Transformer requires much smaller learning rate}} \\
\hline
\end{tabular}
\end{table}
```

---

## åœ¨LaTeXä¸­ä½¿ç”¨è¿™äº›è¡¨æ ¼

### 1. æ·»åŠ å¿…è¦çš„åŒ…

åœ¨LaTeXæ–‡ä»¶çš„åºè¨€éƒ¨åˆ†ï¼ˆ`\documentclass` ä¹‹åï¼‰æ·»åŠ ï¼š

```latex
\usepackage{multirow}  % ç”¨äºè·¨è¡Œå•å…ƒæ ¼
\usepackage{amssymb}   % ç”¨äº \checkmark ç­‰ç¬¦å·
\usepackage{booktabs}  % ç”¨äºæ›´ç¾è§‚çš„è¡¨æ ¼çº¿ï¼ˆå¯é€‰ï¼‰
```

### 2. æ’å…¥è¡¨æ ¼

åœ¨æ­£æ–‡ä¸­ç›´æ¥å¤åˆ¶ä¸Šé¢çš„è¡¨æ ¼ä»£ç å³å¯ï¼š

```latex
\section{Experiments}

\subsection{Comparison with State-of-the-Art}

Table \ref{tab:sota_comparison} shows...

% æ’å…¥è¡¨1
\begin{table*}[t]
...
\end{table*}

\subsection{Ablation Study}

Table \ref{tab:ablation_study} demonstrates...

% æ’å…¥è¡¨2
\begin{table}[t]
...
\end{table}
```

### 3. è¡¨æ ¼ä½ç½®æ§åˆ¶

- `[t]` - é¡µé¢é¡¶éƒ¨ï¼ˆtopï¼‰
- `[b]` - é¡µé¢åº•éƒ¨ï¼ˆbottomï¼‰
- `[h]` - å½“å‰ä½ç½®ï¼ˆhereï¼‰
- `[!]` - å¼ºåˆ¶æ”¾ç½®
- `[t]` æœ€å¸¸ç”¨ï¼Œè®©LaTeXè‡ªåŠ¨ä¼˜åŒ–ä½ç½®

---

## è¡¨æ ¼è®¾è®¡è¯´æ˜

### è®¾è®¡åŸåˆ™

1. **æ¸…æ™°åº¦ä¼˜å…ˆ**: ä½¿ç”¨ç²—ä½“çªå‡ºæœ€ä½³ç»“æœ
2. **å¯¹æ¯”æ€§å¼º**: åˆ†ç»„æ˜¾ç¤ºï¼ˆbaseline / variantsï¼‰
3. **ä¿¡æ¯å®Œæ•´**: åŒ…å«æ–¹æ³•ã€å¹´ä»½ã€backboneã€æ€§èƒ½æŒ‡æ ‡
4. **ç»Ÿè®¡æ˜¾è‘—**: æ ‡æ³¨æå‡ç™¾åˆ†æ¯”å’Œæ’å

### è¡¨æ ¼å®½åº¦

- **å•æ è¡¨æ ¼**: ç”¨ `\begin{table}[t]`
  - é€‚åˆ4-6åˆ—çš„è¡¨æ ¼
  - è¡¨2ã€è¡¨3ã€è¡¨4ã€è¡¨5ã€è¡¨6

- **åŒæ è¡¨æ ¼**: ç”¨ `\begin{table*}[t]`
  - é€‚åˆè¾ƒå®½çš„è¡¨æ ¼ï¼ˆ>6åˆ—ï¼‰
  - è¡¨1 (SOTAå¯¹æ¯”)

### ç¾åŒ–æŠ€å·§

å¦‚æœæƒ³è¦æ›´ä¸“ä¸šçš„è¡¨æ ¼ï¼Œä½¿ç”¨ `booktabs` åŒ…ï¼š

```latex
\usepackage{booktabs}

\begin{table}[t]
\centering
\caption{...}
\begin{tabular}{lcccc}
\toprule  % ä»£æ›¿ \hlineï¼Œæ›´ç²—
Method & SRCC & PLCC \\
\midrule  % ä¸­é—´åˆ†éš”çº¿
Ours & 0.9378 & 0.9485 \\
\bottomrule  % åº•éƒ¨çº¿
\end{tabular}
\end{table}
```

---

## è¡¨æ ¼ä¸å›¾è¡¨çš„åˆ†å·¥

### è¡¨æ ¼é€‚åˆå±•ç¤º:
- âœ… ç²¾ç¡®æ•°å€¼å¯¹æ¯”
- âœ… å¤šæ–¹æ³•/å¤šæŒ‡æ ‡å¯¹æ¯”
- âœ… æ¶ˆèå®éªŒç»“æœ
- âœ… SOTAå¯¹æ¯”

### å›¾è¡¨é€‚åˆå±•ç¤º:
- âœ… è¶‹åŠ¿å˜åŒ–ï¼ˆè®­ç»ƒæ›²çº¿ï¼‰
- âœ… åˆ†å¸ƒæƒ…å†µï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰
- âœ… è§†è§‰å¯¹æ¯”ï¼ˆç‰¹å¾å›¾çƒ­åŠ›å›¾ï¼‰
- âœ… å…³ç³»å¯è§†åŒ–ï¼ˆæ•£ç‚¹å›¾ï¼‰

---

## å¿«é€Ÿå¼•ç”¨æ¸…å•

åœ¨æ­£æ–‡ä¸­å¼•ç”¨è¡¨æ ¼ï¼š

```latex
As shown in Table \ref{tab:sota_comparison}, our method achieves...

The ablation study (Table \ref{tab:ablation_study}) demonstrates...

Cross-dataset results (Table \ref{tab:cross_dataset}) indicate...

Table \ref{tab:model_size} presents the performance-efficiency trade-off...
```

---

**å‡†å¤‡å®Œæˆï¼å¯ä»¥ç›´æ¥å¤åˆ¶åˆ°LaTeXè®ºæ–‡ä¸­ï¼** ğŸ“âœ¨

**æ¥ä¸‹æ¥**: ç”Ÿæˆå›¾åƒå¯è§†åŒ–ï¼ˆæ³¨æ„åŠ›çƒ­åŠ›å›¾ã€è®­ç»ƒæ›²çº¿ç­‰ï¼‰


